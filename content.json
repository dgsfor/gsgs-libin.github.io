{"meta":{"title":"猿的野生香蕉","subtitle":"野生","description":"在运维道路上一路狂飙的老司机","author":"野生","url":"https://blog.itmonkey.icu","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2021-07-21T08:43:47.574Z","updated":"2021-07-21T08:10:42.017Z","comments":false,"path":"/404.html","permalink":"https://blog.itmonkey.icu/404.html","excerpt":"","text":""},{"title":"分类","date":"2021-07-21T08:43:07.917Z","updated":"2021-07-21T08:10:42.018Z","comments":false,"path":"categories/index.html","permalink":"https://blog.itmonkey.icu/categories/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2021-07-21T08:10:42.019Z","updated":"2021-07-21T08:10:42.019Z","comments":false,"path":"repository/index.html","permalink":"https://blog.itmonkey.icu/repository/index.html","excerpt":"","text":""},{"title":"归档","date":"2021-07-21T08:56:28.740Z","updated":"2021-07-21T08:56:28.735Z","comments":false,"path":"archives/index.html","permalink":"https://blog.itmonkey.icu/archives/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2021-07-21T08:43:13.116Z","updated":"2021-07-21T08:10:42.019Z","comments":true,"path":"links/index.html","permalink":"https://blog.itmonkey.icu/links/index.html","excerpt":"","text":""},{"title":"自我介绍","date":"2020-06-17T08:01:29.000Z","updated":"2021-03-26T02:35:08.131Z","comments":true,"path":"about/index.html","permalink":"https://blog.itmonkey.icu/about/index.html","excerpt":"","text":"名字: 李彬 网名: 野生 职业: SRE 座右铭: 在运维道路上一路狂飙的老司机 会啥：DevOps，SRE，K8s，阿里云，AWS，GCP，华为云 语言：shell，python, go，java，swift，vue，最近新学了flutter，还学了原生小程序开始 github: https://github.com/gsgs-libin 微信公众号： 程序猿的野生香蕉"},{"title":"标签","date":"2021-07-21T08:43:58.169Z","updated":"2021-07-21T08:10:42.019Z","comments":false,"path":"tags/index.html","permalink":"https://blog.itmonkey.icu/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"SRE稳定性运营平台之模板和权威消息","slug":"sre-stability-operation-platform-authority-message","date":"2021-11-09T05:12:21.000Z","updated":"2021-11-09T08:05:09.423Z","comments":true,"path":"2021/11/09/sre-stability-operation-platform-authority-message/","link":"","permalink":"https://blog.itmonkey.icu/2021/11/09/sre-stability-operation-platform-authority-message/","excerpt":"","text":"背景在我们逐步由运维转向SRE、SRE转向成SRE运营的过程中，我们的工作内容和工作重心都有着慢慢的变化。在这个转变的过程中，我们需要不断的把这些日常工作或者是定期工作逐步沉淀、不断的创新，那么就会带来两种东西： 第一：工具平台 第二：特色输出 工具能够协助我们快速、自动化、智能的完成工作；特色输出能够告诉外界你们做了什么，更重要的是它代表SRE独有，对于提高部门影响力起到一个关键作用以及也是重要途径之一。 备注：之前有写过一篇关于运营报告平台的介绍，可以简单看下，写的更具体一点：点击我跳转 介绍目前我基于在公司做的稳定性运营平台，抽离了部分功能以及脱离了公司的一些框架，输出了一版使用开源框架做的运营平台。这个平台会持续更新，当然可能会慢于公司的版本更新。 项目地址：点击我跳转 开源版本的信息： 1234前端：https://github.com/gsgs-libin/sre_cerebrum后端：https://github.com/gsgs-libin/sre_cerebrum_api前端框架：ant-design-vue-pro后端框架：gin 开源版本的优势： 更丰富的日志输出 更好的规范 更好看的界面 公司内部版本功能目前公司内部的项目已有的一些功能有：1.业务周报、SRE运营报告、巡检报告等一系列报告的输出2.业务巡检覆盖3.SRE权威消息发布(包含一切核心消息同步)4.通知渠道注册5.SRE事件编排(SRE轮值、节假日轮值、周期性事件)6.业务基础数据采集(qps，sla，带宽，db资源信息等)7.定时任务 开源版本这次先实现了权威消息模块以及渠道通知模块，平台有一个基础模块就是模板，所有的报告、消息都是基于模板来实现的。 功能介绍权威消息起初设计的目的是覆盖SRE内部日常对内对外的消息，包括： 各式各样的通知(该写周报了、该巡检了、该处理问题了) 对外轮值通知(告诉业务方今天是谁轮值) 故障消息(发生了什么故障、故障恢复了等等的通知，权威通知，防止群里七嘴八舌的) 定时发送的消息 如何创建权威消息1.创建权威消息模板 2.编辑模板内容模板内容可以是静态内容也可以是变量模式，比如可以用一个变量来替代你要发送的消息内容。 如果没有变量可以这样：如果有变量可以在内容增加写一个变量，然后在下面变量列表中写上： 3.变量注册如果在编辑模板的过程中增加了变量，那么我们需要注册下这个变量，意思就是我从哪里获取到你这个变量。目前变量分了三种：内置变量，自定义变量，图片变量。内置变量就是注册一次谁都可以用，自定义变量和图片变量都是跟具体的模板绑定的。 注册完成之后，可以看看这个变量的详情： 4.通知渠道注册顾名思义，就是创建一些可以通知消息的渠道，比如企业微信机器人，企业微信应用，钉钉机器人，邮件，短信等等。由于公司内部使用企业微信，所以我目前就实现了注册机器人和应用的功能，其他的有需求再加，或者可以提pr。 4.创建消息我们的每一条消息都是基于模板的，所以我们需要在模板位置去创建消息。创建消息会经历的步骤：创建动作–渲染动作(如果有变量)–合并动作(如果有变量)–审核动作(如果是需要审核的)–发送 5.查看消息渲染进度(如果有变量的话) 6.发送或者审核在这里你可以对消息进行一系列的操作，比如： 测试发送消息 看下发送历史，到底被发送了多少次 编辑，如果消息内容不理想，可以进行二次编辑 审核(当然需要审核人员才行，可以看后端代码定义) 废弃(创建消息的人可以选择主动废弃) 下期介绍下期会发布新功能：报告相关(运营报告、巡检报告等等)","categories":[{"name":"sre-report","slug":"sre-report","permalink":"https://blog.itmonkey.icu/categories/sre-report/"}],"tags":[{"name":"sre","slug":"sre","permalink":"https://blog.itmonkey.icu/tags/sre/"},{"name":"devops","slug":"devops","permalink":"https://blog.itmonkey.icu/tags/devops/"}]},{"title":"基于DCGM的ubuntu GPU监控方案","slug":"ubuntu-gpu-monitor-prometheus","date":"2021-10-26T08:50:15.000Z","updated":"2021-10-26T09:50:29.406Z","comments":true,"path":"2021/10/26/ubuntu-gpu-monitor-prometheus/","link":"","permalink":"https://blog.itmonkey.icu/2021/10/26/ubuntu-gpu-monitor-prometheus/","excerpt":"","text":"背景手上有几台gpu机器，目前没有单独加入到k8s集群，所以需要单独装一下监控来观察他的利用率情况。 技术目前使用NVIDIA/gpu-monitoring-tools来做监控，然后对接prometheus。 基础信息12机器型号：inux version 4.15.0-136-generic (buildd@lcy01-amd64-029) (gcc version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04)) #140-Ubuntu SMP Thu Jan 28 05:20:47 UTC 2021gpu版本：NVIDIA-SMI 450.80.02 Driver Version: 450.80.02 CUDA Version: 11.0 安装安装docker1apt install docker.io 部署exporter1docker run -d --gpus all --rm -p 9400:9400 nvcr.io/nvidia/k8s/dcgm-exporter:2.0.13-2.1.2-ubuntu18.04 测试1curl localhost:9400/metrics GPU指标说明可以参考下这个文章：点击 异常信息如果在部署exporter的时候报错如下： 1docker: Error response from daemon: could not select device driver \"\" with capabilities: [[gpu]]. 请这样处理：(记住一定要重启下docker) 1234561. distribution=$(. /etc/os-release;echo $ID$VERSION_ID)2. curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -3. curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list4. apt-get update5. apt-get install nvidia-container-toolkit6. systemctl restart docker 配置prometheus部署好之后，我们需要配置下prometheus，新增一个target： 123456789- job_name: 'GpuMonitor' scrape_interval: 300s scrape_timeout: 300s metrics_path: /metrics scheme: http static_configs: - targets: - 10.10.10.1:9400 - 10.10.10.2:9400 配置grafanagrafana模板到官方网站找一个，很多都可以，我用的这个模板：grafana模板 导入之后看看效果","categories":[{"name":"k8s","slug":"k8s","permalink":"https://blog.itmonkey.icu/categories/k8s/"}],"tags":[{"name":"prometheus","slug":"prometheus","permalink":"https://blog.itmonkey.icu/tags/prometheus/"},{"name":"gpu","slug":"gpu","permalink":"https://blog.itmonkey.icu/tags/gpu/"},{"name":"grafana","slug":"grafana","permalink":"https://blog.itmonkey.icu/tags/grafana/"}]},{"title":"实现华为云cce集群双GPU版本共存","slug":"hwcloud-cce-dual-gpu-version-coexist","date":"2021-08-31T06:31:08.000Z","updated":"2021-08-31T10:21:54.496Z","comments":true,"path":"2021/08/31/hwcloud-cce-dual-gpu-version-coexist/","link":"","permalink":"https://blog.itmonkey.icu/2021/08/31/hwcloud-cce-dual-gpu-version-coexist/","excerpt":"","text":"背景在使用华为云cce集群的时候，使用gpu插件来扩展我们集群的gpu能力。不过我们服务有更新，需要更高的gpu版本来支撑，但是老的又不能下。所以我们采用双gpu版本共存来实现需求。 问题点华为云目前一个集群只支持安装一个版本的gpu插件，所以我们需要知道他的gpu插件工作原理，单独实现一个。 方案我们发现华为云的gpu插件其实是两个ds来工作的，nvidia-driver-installer和nvidia-gpu-device-plugin。功能简单来说就是，在node节点上下载好gpu驱动，解压到特定目录。 123456➜ ~ k get ds -n kube-systemNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGEicagent 120 120 120 120 120 &lt;none&gt; 517dnvidia-driver-installer 60 60 60 9 60 &lt;none&gt; 467dnvidia-gpu-device-plugin 60 60 60 9 60 &lt;none&gt; 467dstorage-driver 120 120 120 120 120 &lt;none&gt; 517d 现在我们直接复制这两个ds的yaml文件，然后修改里面的name以及标签为nvidia-driver-installer-460和nvidia-gpu-device-plugin-460，你可以简单的认为做一下区分就好了。 创建两种节点池，然后分别打上标签为gpu_type=gpu-418和gpu_type=gpu-460 分别修改四个ds，然后加上nodeselector，如下：12nodeSelector: gpu_type=gpu-418 或者 gpu_type=gpu-460 最后apply，查看效果12345678➜ ~ k get ds -n kube-systemNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGEicagent 120 120 120 120 120 &lt;none&gt; 517dnvidia-driver-installer 60 60 60 9 60 gpu_type=gpu-418 467dnvidia-driver-installer-460 43 43 43 17 43 gpu_type=gpu-460 194dnvidia-gpu-device-plugin 60 60 60 9 60 gpu_type=gpu-418 467dnvidia-gpu-device-plugin-460 43 43 43 16 43 gpu_type=gpu-460 194dstorage-driver 120 120 120 120 120 &lt;none&gt; 517d","categories":[{"name":"云运维","slug":"云运维","permalink":"https://blog.itmonkey.icu/categories/%E4%BA%91%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://blog.itmonkey.icu/tags/k8s/"},{"name":"华为云","slug":"华为云","permalink":"https://blog.itmonkey.icu/tags/%E5%8D%8E%E4%B8%BA%E4%BA%91/"},{"name":"GPU","slug":"GPU","permalink":"https://blog.itmonkey.icu/tags/GPU/"}]},{"title":"gcp如何限制某台机器公网流量，放行内网流量","slug":"gcp-firewalls-restrict-release-of-public-networks-to-the-intranet","date":"2021-08-27T07:38:40.000Z","updated":"2021-08-27T07:46:29.721Z","comments":true,"path":"2021/08/27/gcp-firewalls-restrict-release-of-public-networks-to-the-intranet/","link":"","permalink":"https://blog.itmonkey.icu/2021/08/27/gcp-firewalls-restrict-release-of-public-networks-to-the-intranet/","excerpt":"","text":"背景当前有一台带公网ip的机器，网络标记是：datax，我们期望的状态是： 公网只放行8080、8793端口 内网只允许22端口 做法 新增入站规则，目标是网络标记为datax，来源0.0.0.0/0，允许8080和8974端口，优先级为1 新增入站规则，目标是网络标记为datax，来源是10.0.0.0/8(这个看你们内网网段是多少)，允许22端口，优先级为2 新增入站规则，目标是网络标记为datax，来源是0.0.0.0/0，拒绝所有流量，优先级为3 截图","categories":[{"name":"云运维","slug":"云运维","permalink":"https://blog.itmonkey.icu/categories/%E4%BA%91%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"gcp","slug":"gcp","permalink":"https://blog.itmonkey.icu/tags/gcp/"},{"name":"firewalls","slug":"firewalls","permalink":"https://blog.itmonkey.icu/tags/firewalls/"}]},{"title":"编程时间处理","slug":"ops-time","date":"2021-07-21T03:12:05.000Z","updated":"2021-07-21T05:56:21.014Z","comments":true,"path":"2021/07/21/ops-time/","link":"","permalink":"https://blog.itmonkey.icu/2021/07/21/ops-time/","excerpt":"golang","text":"golang 获取字符串时间 1start_time :=time.Now().Format(\"2006-01-02 15:04:05\") 时间差计算 123start_time :=time.Now()end_time :=time.Now()d := end_time.Sub(start_time) java123456789Calendar c = Calendar.getInstance();c.setTime(new Date());SimpleDateFormat hourPoint = new SimpleDateFormat(\"yyyy-MM-dd-HH\");SimpleDateFormat dayPoint = new SimpleDateFormat(\"yyyy-MM-dd\");String hourPointDate = hourPoint.format(c.getTime());String dayPointDate = dayPoint.format(c.getTime());long NowTime = c.getTimeInMillis();c.add(Calendar.HOUR, -1);long OneHourAgoTime = c.getTimeInMillis();","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://blog.itmonkey.icu/categories/DevOps/"}],"tags":[{"name":"time","slug":"time","permalink":"https://blog.itmonkey.icu/tags/time/"}]},{"title":"稳定性报告(report)平台 -- 模板篇","slug":"ops_report/report-02","date":"2021-07-15T05:37:17.000Z","updated":"2021-07-16T07:37:46.272Z","comments":true,"path":"2021/07/15/ops_report/report-02/","link":"","permalink":"https://blog.itmonkey.icu/2021/07/15/ops_report/report-02/","excerpt":"介绍后续我们称这个平台为srp平台。","text":"介绍后续我们称这个平台为srp平台。 后续的每一个模块，基本上会通过几个方面来介绍： 基础讲解 数据库设计 核心功能介绍 简单展示 基础讲解模板篇，我会把模板和变量注册放在一起讲解，因为两个关联性比较大，所以一起讲解会比较容易点。前面说到，我们的模板分成了三种类型： 模板类型 说明 基础模板 基础模板主要提供接入能力，比如我定义好了域名QPS统计的能力，那么你只需要提供你的域名，我就可以帮你完成统计动作 开放模板/周期模板 这类模板就相对自由，你可以自定义模板内容，变量，最终定义定时任务来定时的渲染该模板，然后选择发送等 基础模板首先对于基础模板，换个概念理解就是平台已经约定了这个模板只会采集某一类型的数据，然后对于用户来讲，你只需要关注要不要采集即可。 举个例子： 12我创建一个统计`QPS峰值`的模板，这个模板主要是统计每一个注册进来的域名过去1小时的峰值，而且是每小时统计一次。那么对于用户来讲，我只需要增加注册即可，比如我有一个a域名，那么新增注册，告诉平台我这个域名希望加入统计。 像这种模板还有很多，比如带宽、nat网关、负载均衡等等，都可以当成基础模板来使用，用户只需接入即可，不用配置更多。 开放模板/周期模板其实当初定义了两种类型，最后发现没有实际上的差别，所以就放到一起讲吧。这里我统称为自定义模板吧，字面意义上理解就是完全自定义，你可以定义你想要的报告的样子，比如下面： 我这变用了一个markdown编辑器来做，目前我们都是通过markdown格式的数据来展示报告，你可以跟日常写markdown文档一样就行。当你在某个位置想要用变量展示，那么定义一个变量名称，然后在下面添加就行，这个变量可以是图片变量、内置变量、模板变量。 变量注册我们定义好开放模板之后，以及也增加好了里面的变量。现在就需要去注册变量，本质就是告诉平台我这个变量是什么意思，怎么获取到这个值。 数据库设计首先对于模板表，我这边主要设计了以下几个字段：(唯一需要关注的就是Content字段，本身是markdown内容，所以设置成了TEXT类型) 123456789101112// 模板表type ReportTemplate struct &#123; BaseModel TemplateId string `json:\"template_id\"` // 模板id Name string `json:\"name\"` // 模板名称 Creater string `json:\"creater\"` // 创建人 Status string `json:\"status\"` // 模板状态 禁用，可用 enable disable Type string `json:\"type\"` // 模板类型，基础模板(base_temp)，开放模板(open_temp)，周期模板(cron_temp) Router string `json:\"router\" gorm:\"default:null\"` // 前端跳转路由 Content string `json:\"content\" gorm:\"type:TEXT;default:null\"` // markdown内容 VarList string `json:\"var_list\" gorm:\"type:TEXT;default:null\"` // 变量json串&#125; 其次是变量表：(变量参数是指请求url的时候带上什么参数，获取字段是指请求的结果中我用哪个字段的数据) 1234567891011// 变量注册表type ReportTemplateVar struct &#123; BaseModel VarTempId string `json:\"var_temp_id\"` // 模板id VarName string `json:\"var_name\"` // 变量名称 VarUrl string `json:\"var_url\" gorm:\"type:TEXT\"` // 变量获取地址 VarUrlField string `json:\"var_url_field\"` // 变量结果获取字段 VarType string `json:\"var_type\"` // 变量类型 ,内置变量、自定义变量、图片变量，inner_temp_var、custom_temp_var、img_temp_var VarParams string `json:\"var_params\" gorm:\"default:null\"` // 变量参数 VarDesc string `json:\"var_desc\" gorm:\"default:null\"` // 变量备注，变量说明&#125; 如果是基础模板的话，我们其实还会涉及到一个接入信息，比如我有一个qps的基础模板，那么接入表里面就会涉及到，哪个域名，什么path等： 123456789101112// qps峰值模板接入表，当前默认规定该模板下的接入，都是定时1小时获取数据一次，并存储数据。type QPSPeakBaseTemp struct &#123; model.BaseModel Aid string `json:\"aid\"` // 接入id Name string `json:\"name\"` // 接入名称 Domain string `json:\"domain\"` // 接入域名 LoadBalanceBiz string `json:\"load_balance_biz\"` // lb分组 Path string `json:\"path\"` // 域名path Group string `json:\"group\"` // 业务组 Creater string `json:\"creater\"` // 创建人 Status string `json:\"status\"` // 接入状态 禁用，可用 enable disable&#125; 核心功能介绍其实对于模板来讲，没啥核心功能，就是简单的crud。 前端的markdown表单 使用的是一个开源的markdown插件，git地址：https://github.com/hinesboy/mavonEditor 注册完成之后，可以很容易就创建一个markdown编辑框： 1&lt;v-md-editor v-model&#x3D;&quot;markdownContent&quot; height&#x3D;&quot;500px&quot;&gt;&lt;&#x2F;v-md-editor&gt; 变量注册 因为涉及到几种变量类型，所以需要分别做处理。 内置变量：其实就是平台提供好的变量，我这边主要是定义了一些基础的，比如时间的获取、某个域名的QPS峰值获取、某个nat网关的峰值获取等自定义变量：就是用户自己提供的url，返回值需要按照一定的格式来填写图片变量：目前主要是对接grafana，每一个grafana的panel都可以生成一个图片地址 比如：https://grafana.baidu.com/render/d-solo/BvV4IsPWk/nat?from=000000000&amp;orgId=63&amp;to=000000000&amp;panelId=2&amp;width=1000&amp;height=500&amp;tz=Asia%2FShanghai我在注册的时候就会替换里面的from和to两个时间，因为时间需要可传参，结果就是这样：https://grafana.baidu.com/render/d-solo/BvV4IsPWk/nat?from=SVAR_START_TIME&amp;orgId=63&amp;to=SVAR_END_TIME&amp;panelId=2&amp;width=1000&amp;height=500&amp;tz=Asia%2FShanghai 简单展示模板列表：(不同的模板类型，他的操作按钮也是不一样的) 变量列表：","categories":[{"name":"Report","slug":"Report","permalink":"https://blog.itmonkey.icu/categories/Report/"}],"tags":[{"name":"运营报告","slug":"运营报告","permalink":"https://blog.itmonkey.icu/tags/%E8%BF%90%E8%90%A5%E6%8A%A5%E5%91%8A/"},{"name":"巡检","slug":"巡检","permalink":"https://blog.itmonkey.icu/tags/%E5%B7%A1%E6%A3%80/"}]},{"title":"稳定性报告(report)平台思考","slug":"ops_report/report-01","date":"2021-07-13T05:44:01.000Z","updated":"2021-07-16T07:37:36.625Z","comments":true,"path":"2021/07/13/ops_report/report-01/","link":"","permalink":"https://blog.itmonkey.icu/2021/07/13/ops_report/report-01/","excerpt":"背景首先说一些日常工作场景： 你的群里是否经常会发送一些巡检报告，比如qps峰值统计，cpu利用率，机器使用数量统计等等 你的领导是否需要你每周发送一次公司业务层级的运行报告，包括上周有没有故障，上周所有业务的qps峰值是多少，周期内的一些业务变更或者运营活动等。","text":"背景首先说一些日常工作场景： 你的群里是否经常会发送一些巡检报告，比如qps峰值统计，cpu利用率，机器使用数量统计等等 你的领导是否需要你每周发送一次公司业务层级的运行报告，包括上周有没有故障，上周所有业务的qps峰值是多少，周期内的一些业务变更或者运营活动等。 所以你可能会看到类似这样的一些图： 初步思考基于上述的背景，我们常规的实现方式有以下几种： 日常的巡检报告，我们可能会用一个脚本定期的获取数据，然后发送到企业微信机器人、邮箱、钉钉等 周、月巡检报告运营报告这些，我们可能需要手动创建一个md文件，写上一些数据，截上一些监控图，加上一部分解读和批注，最终发到群里给大家看。 那么基于上述的实现方式，我们是否可以进一步的去考虑自动化或者说是平台化。比如是否可以设想一下： 把日常的报告、通知发送集中起来管理(目前可能是一个人一个脚本，不知道跑在哪个机器的定时任务上) 是否可以模板化，比如定制一些模板，然后拿着这个模板去渲染真实的数据，最终发送出来 还可以增加定时任务，比如针对这个模板，我想要定期发送，这样就更简单的托管起来我们分部在各个机器上的定时脚本了 功能期望针对上述的思考，我们团队做了一些建设，目前一期建设想要产出一个稳定性运营平台，这个平台目前的任务主要有以下几个： 接管我们当前的日常报告的功能(目前的报告都是自动化脚本实现，定期发送qps数据，带宽数据，网关数据等) 提供自定义模板功能，能快速定义想要的数据模板 可以实现监控图的发送(目前更多的是grafana的图片) 接管我们当前每周的稳定性运营报告的工作，可自动渲染数据，可自定义标注，最终发送终版报告给到用户 历史数据的存储，目前监控平台会存储全量数据，这个平台只是想要存储一些特定数据或者叫做过滤后有效的数据 实现定期发送的功能，可配置定时任务 平台设计首先这个平台分成了三个大的部分：前端、后端接口、定时任务端 这里主要讲解一下非前端部分，首先功能模块分了几个大块： 123456仪表盘模块模板模块变量注册模块报告模块数据存储模块定时任务模块 仪表盘模块这一块主要是总览展示的功能模块，包括整个平台有多少模板，有多少报告等。这一快的功能放到二期了，一起暂时没做，不过有一个初版的样子可以看看。 模板模块模板模块和变量注册模块是相辅相成的。模板目前主要划分了几种类型： 模板类型 说明 基础模板 基础模板主要提供接入能力，比如我定义好了域名QPS统计的能力，那么你只需要提供你的域名，我就可以帮你完成统计动作 开放模板/周期模板 这类模板就相对自由，你可以自定义模板内容，变量，最终定义定时任务来定时的渲染该模板，然后选择发送等 模板定义主要的原理就是：创建一个markdown主体内容，然后写上一些变量，最终这些变量都会被真实的值所替换。所以创建模板需要你重要的能力就是：会写markdown。 创建模板大致长这样： 变量注册模块变量注册主要是定义你在模板中设置的变量，比如这个变量的获取方式是什么样子的，获取的数据字段是哪个。对比变量的说明，主要划分了几种：内置变量、模板变量、图片变量针对这三种变量有一个简单的说明： 变量类型 变量说明 提供信息 内置变量 所有模板都可以使用的，可以理解为公共变量 提供获取地址 模板变量 属于模板特有的 提供获取地址 图片变量 专门下载grafana图片的 提供grafana地址 比如我举个例子： 1234我在模板中定义了一个变量`DATE`，那么我在变量注册的时候需要提供这些信息：变量获取地址：https://abc.com/var/date变量获取字段：data等到真正渲染模板动作执行的时候，就会去请求这个地址，拿到返回数据中data字段的值来具体的替换我的`DATE`数据，来达到渲染的目标。 1234我在模板中定义了一个变量`QPS__TREND_IMG`，这个主要是一个图片变量，获取的数据就是一个域名在某段时间的qps趋势图，那他提供的信息就是:grafana地址：http://grafana.c.com/****grafana的key：这个当然你可以放到服务端配置中去，也可以自定义，随意等到真正渲染模板动作执行的时候，就会去请求这个下载这个grafana图片，然后上传到你自己的存储，比如阿里云的oss，华为云的obs，最终提供一个图片地址然后替换这个变量，达到渲染的目标。 报告模块对于报告模块来讲，就是我们最终要发送和呈现的产物，那么我们可以从几个维度来讲解一下：报告如何产生、报告产生的来源、报告的状态 报告如何产生 报告产生的唯一入口就是通过模板，我们可以在模板处选择对应的模板，然后创建报告，创建报告需要几个信息需要填写： 1231. 模板名称(选哪个就用哪个)2. 报告名称3. 时间范围(所有数据都依赖于时间范围，有了时间范围我们才能产生数据，然后渲染报告) 报告产生的来源 关于报告的来源，平台的规划主要有两个，第一个是手动生成的报告,第二个是定时任务产生的报告 报告的状态 针对报告，我们需要有几种状态：未渲染、渲染中、待标注、已发布(NotRendered、RenderedIng、ToBeLabeled、Published) 数据存储模块对于数据存储模块，我们前面定义的是历史数据的存储，目前监控平台会存储全量数据，这个平台只是想要存储一些特定数据或者叫做过滤后有效的数据。 举个例子，当前我们平台存储的数据有哪些： 12341.qps峰值数据(我们监控存的是5s一个点的qps数据，不过该平台会每小时取一次时间段内的峰值qps作为存储)2.snat峰值数据(对于nat网关，我们也是采用类似的方式，存储过去一小时的峰值数据)3.带宽峰值数据4.弹性机器峰值数据(目前k8s集群的机器是弹性的，所以会存储每一小时的峰值数据) 当我们有了这些数据，我们能做的就比较多了，比如： 1231. 我们基础的监控数据在获取大范围时间内的数据，都是对峰值等数据做了聚合，所以会低于真实峰值，有了每小时的峰值数据，能准确的描绘业务峰值。2. 当我们需要周报、月报、年报的时候，我们都可以对这些峰值数据做一些处理和分析3. 从一定角度上讲，我们可以分析峰值的走势，来辅助业务做一些判断，比如业务的峰值时间越来越晚，是不是说明用户睡的越来越晚呢？ 定时任务模块定时任务，毋庸置疑，他承担着控制我们报告的发送时间和频率。比如我在平台可以针对模板创建一个定时任务：什么时间、哪个模板、发送到哪里。 目前我们使用的定时任务是对接开源的分布式定时任务框架xxl-job，二开了一些定时任务接口给到平台使用。 那么需要了解的主要可能有几点： 121.定时任务的发送地方：目前支持发送到企业微信应用、企业微信机器人2.定时任务的发送频率：主要取决你定时任务表达式的书写 最后说一句对于这个平台来说，其实整体功能不难，不过个人感觉能接管不少分散的服务和脚本，也使得趋于统一化。不过弊端也有，说实话当前的功能还比较单一，后续拓展有，但是还没有想好，所以对于单一的功能平台化，就是有点太卷了。","categories":[{"name":"Report","slug":"Report","permalink":"https://blog.itmonkey.icu/categories/Report/"}],"tags":[{"name":"运营报告","slug":"运营报告","permalink":"https://blog.itmonkey.icu/tags/%E8%BF%90%E8%90%A5%E6%8A%A5%E5%91%8A/"},{"name":"巡检","slug":"巡检","permalink":"https://blog.itmonkey.icu/tags/%E5%B7%A1%E6%A3%80/"}]},{"title":"上手GCP Anthos之多集群管理","slug":"gcp-anthos-multi-cluster-mgt","date":"2021-03-23T11:52:18.000Z","updated":"2021-03-23T15:51:56.784Z","comments":true,"path":"2021/03/23/gcp-anthos-multi-cluster-mgt/","link":"","permalink":"https://blog.itmonkey.icu/2021/03/23/gcp-anthos-multi-cluster-mgt/","excerpt":"提前说明1.上手anthos时日不多，有解释或者说明不对的地方，请指正！2.细节还是比较多的，有什么问题可以直接联系我，相互讨论！","text":"提前说明1.上手anthos时日不多，有解释或者说明不对的地方，请指正！2.细节还是比较多的，有什么问题可以直接联系我，相互讨论！ 了解Anthos 是一个现代应用管理平台，官方解释：官方文档 对于这几天上手了解后，发现非常多功能还是很有意思和借鉴意义的。目前来说，我们主要用到了多集群管理(Environ)、配置管理(ACM)、服务管理(ASM)，以及其他暂时还没有用到的。 多集群管理字面意思理解即可，不过特别说明的是，他可以纳管其他云商k8s集群以及自建k8s集群。 配置管理我比较喜欢称它为gitops，一切皆文件。简单描述下：你的服务部署所需要的所有yaml文件(默认在k8s里)，放在这个git里，任何改动的提交，都会同步到所有的集群(默认同步所有)，比如：证书更新管理，证书更新了，提交一下，发布到所有集群，不用一个一个去更新了(如果你有其他自动化，当我没有说) 注：这个是我比较喜欢的一个功能，很实用。 服务管理对于anthos管理的工作负载，asm会管理这些服务的网格环境，提供众多功能以及istio的所有功能，可以认为是一个增强改良版istio。 注：当前我还没有使用上，以后使用上了，再更新一篇文章。 额外提示初次使用anthos，gcp会提供一个900刀的试用金，可以尽情的享受了。 温馨提示：注意他的计费方式，不算太便宜。 开始吧试用anthos已经两三天了，基本的模型和框架都已经搭建完毕，以及简单的测试都已经ok。大致可以分为以下几个点： 注册集群 创建git中心 抽象配置文件 开启配置同步 测试批量管理 注册集群目前有两种途径来接入anthos ① 通过anthos直接创建集群(当然这个直接创建的是gke集群)② 纳管现有集群 – 纳管gke集群、纳管三方集群 纳管gke集群我当前都是已经有现成的gke集群，所以直接选择第二种途径纳管gke集群即可。 纳管三方集群可以选择添加外部集群，填写好你集群的标签，最终会生成一条注册命令，扔到你的集群中执行即可。 效果和验证当你纳管好集群后，可以看看是否纳管成功以及如何判断纳管成功。 ① 执行gcloud命令，查看当前纳管的集群和状态： 123456789101112131415161718192021222324[root@localhost ~]# gcloud container hub memberships listNAME EXTERNAL_IDcluster1 xxxxxx-xxxxxxxxxx-xxxxxxxxxxxxcluster2 xxxxxx-xxxxxxxxxx-xxxxxxxxxxxx[root@localhost ~]# gcloud container hub memberships describe cluster1 createTime: '2021-03-17T06:41:47.422321701Z'endpoint: gkeCluster: resourceLink: //container.googleapis.com/projects/****** kubernetesMetadata: kubernetesApiServerVersion: v1.17.15-gke.800 memoryMb: 8282 nodeCount: 2 nodeProviderId: gce updateTime: '2021-03-23T12:46:37.348171277Z' vcpuCount: 8externalId: xxxxxx-xxxxxxxxxx-xxxxxxxxxxxxlastConnectionTime: '2021-03-23T12:42:31.571643848Z'name: projects/***********state: code: READYuniqueId: xxxxxx-xxxxxxxxxx-xxxxxxxxxxxxupdateTime: '2021-03-23T12:46:37.689233201Z' ② 使用kubectl查看到底安装了什么(其实跟绝大多数集群管理平台一样，就安装了一个agent的服务，参考rancher) 123[root@localhost ~]# kubectl get pods -n gke-connectNAME READY STATUS RESTARTS AGEgke-connect-agent-20210305-01-00-54f7dcc455-l5x2f 1/1 Running 0 31h 注意事项 集群需要有足够的资源 节点需要有拉取git的权限 关于git的权限，你就简单认为他拿着你的配置文件然后做应用即可。在gke中，我们需要对节点池进行配置权限：(节点池权限Cloud Source Repositories开放read only即可) 如果你是命令行创建的nodepool，可以参考这个： 1gcloud container node-pools create my-nodepool --cluster=*** --zone *** --num-nodes=2 --disk-size=40 --machine-type=custom-4-4096 --project *** --scopes=cloud-source-repos-ro scopes这个是权限列表，你可以按需添加，官方文档：scopes说明 创建git中心顾名思义，把我们所有的配置和服务都抽象出yaml文件来，然后放到这个git里(正常anthos就是一个kubectl工具，你提交一个yaml文件，他apply一个)。这个git仓库可以有很多中类型：github、gitlab、gcp仓库，这里我直接使用gcp的仓库服务，其他的你可以自己研究，还是比较简单的。 抽象配置文件在一步，我们首先要了解两个概念，层级结构(hierarchy)和非结构化模式(unstructured)。 层级结构是gcp提供的一种具有约束力的文件目录结构，比如目录下有cluster和namespace两个目录，那么在这两个目录里就分别存放集群级别的配置和namespace级别的配置。非结构化模式是用户自己定义的一种文件模式，有点类似于helm、kustomize,不过值得注意的是，在这种模式下，我们每一个文件都需要标准的yaml文件。 nomos在整体操作之前，我们先了解一个工具nomos,这个工具能让我们快速创建Config Sync代码库，包括初始化以及语法校验。官方文档 安装一下： 1231.下载对应系统版本的二进制文件2.chmod +x nomos3.加入到环境变量中 层级结构1.初始化层级结构目录12$ cd my-repo/$ nomos init 这时候你就可以看到一个完整层级目录的结构： 123456789$ tree.├── cluster # 集群级别的配置文件，role、rolebinding等├── clusterregistry # 集群注册配置文件，比如我在这里定义了集群A的标签，在下面可以使用这个标签A表示只在A集群中安装，在其他集群不安装├── namespaces # namespace级别的一些配置文件，如：svc、ingress、configmap、secret等├── README.md└── system ├── README.md └── repo.yaml 2.编写集群注册的文件加入我们有两个集群，cluster1和cluster2，那么我们需要在clusterregistry这个目录加入一下文件： 1234567891011121314151617$ cat cluster1.yamlkind: ClusterapiVersion: clusterregistry.k8s.io/v1alpha1metadata: name: cluster1 labels: cluster: cluster1$ cat selector-cluster1.yamlkind: ClusterSelectorapiVersion: configmanagement.gke.io/v1metadata: name: selector-cluster1spec: selector: matchLabels: cluster: cluster1 上述文件表示：首先我们有一个cluster1.yaml表示我当前anthos管理的cluster1这个集群，selector-cluster1.yaml表示定义一个标签，当你满足我这个标签，就表示你选择了我这个集群。这个标签就是我们后续会用到的configmanagement.gke.io/cluster-selector: selector-cluster1,这个需要写在我们的annotations中。 3.编写namespace相关比如我们在namespace目录下创建一个test文件夹，那么表示我所管理的集群都会创建出一个名称为test的namespace(当然前提是你没有加cluster-selector)。然后你就可以在这个文件夹下创建一系列的yaml文件了，比如你的deployment、service、ingress、configmap、secret、hpa等，如下所示： 12345678$ ll -lh test/total 36K-rw-r--r-- 1 root root 2.9K Mar 22 15:52 deployment.yaml-rw-r--r-- 1 root root 359 Mar 22 15:52 image-pull-secret.yaml-rw-r--r-- 1 root root 276 Mar 22 15:52 hpa-cpu.yaml-rw-r--r-- 1 root root 461 Mar 22 15:52 ingress.yaml-rw-r--r-- 1 root root 9.4K Mar 22 15:52 ssl-tls.yaml-rw-r--r-- 1 root root 264 Mar 22 15:52 service.yaml 刚刚说的集群选择标签，当前我们有两个集群cluster1和cluster2，那么假如我只想让hpa创建在cluster1怎么做呢，可以看如下配置(重点关注annotations)： 123456789101112131415$ cat hpa-cpu.yamlapiVersion: autoscaling/v1kind: HorizontalPodAutoscalermetadata: name: test-hpa annotations: configmanagement.gke.io/cluster-selector: cluster1spec: scaleTargetRef: apiVersion: extensions/v1beta1 kind: Deployment name: test-deployment minReplicas: 1 maxReplicas: 5 targetCPUUtilizationPercentage: 30 4.验证语法1$ nomos vet 非结构化模式在非结构化模式中，我们采用的是helm的模式(需要对helm有一定基础，不熟悉的同学可以暂时跳过)。注：他有一个特殊之处，就是我前面提到anthos只认标准的k8s yaml文件，像helm chart中的values这种他就不认，那么我们怎么去把这种应用到anthos呢？ 带着问题，经过实践，我们得到如下结论：① 首先我们还是需要初始化一个标准的helm chart模板 – helm init② 我们需要打包出chart包 – helm package ChartName --debug③ 生成yaml模板 – helm template ChartName ChartName-Version.tgz④ 最终复制输出的结果到一个yaml文件中，上传到git仓库中 以下是我写的最简单的一个helm Chart，为了演示作用： 12345678910111213141516171819202122232425262728293031[root@localhost mychart]# tree.├── Chart.yaml├── templates│ └── namespace.yaml└── values.yaml1 directory, 3 files[root@localhost mychart]# cat values.yamlversion: 0.0.1namespace: \"mynamespace\"[root@localhost mychart]# cat templates/namespace.yamlapiVersion: v1kind: Namespacemetadata: name: &#123;&#123; .Values.namespace &#125;&#125;[root@localhost]# helm package mychart --debugSuccessfully packaged chart and saved it to: /www/anthos/repohelm/mychart-0.1.0.tgz[root@localhost]# helm template mychart mychart-0.1.0.tgz---# Source: mychart/templates/namespace.yamlapiVersion: v1kind: Namespacemetadata: name: mynamespace[root@localhost]# 然后把输出的内容另存到一个init.yaml中，丢到git仓库里 当然，采用非结构化模式，我们也需要校验一下语法是否正确： 1$ nomos vet --source-format=unstructured 开启配置同步当我们编写好以上的文件，我们就可以配置acm了。 选择合适的git仓库类型 填写git地址等信息这一步填写好git地址，分支，同步间隔时间(多久从git同步一次)，源格式(层级或非结构化) 开始安装在这里我们也可以直接看看他到底安装了什么东西，可以从下面看出他也是类似安装了一个服务，来定时同步我们的git文件到集群中。 1234[root@localhost]# kubectl get pods -n config-management-systemNAME READY STATUS RESTARTS AGEgit-importer-69699fd8b7-jzkhm 4/4 Running 1 31hmonitor-65b649fbd4-7wvps 1/1 Running 0 34h 查看状态当我们安装好之后，可以从控制台查看当前的一个状态，也可以使用nomos来查看当前同步的状态： 123456789101112[root@localhost]# nomos statusConnecting to clusters...*cluster1 -------------------- &lt;root&gt; https://source.developers.google.com/p/******@master SYNCED e160b119cluster2 -------------------- &lt;root&gt; https://source.developers.google.com/p/******@master SYNCED e160b119 测试批量管理当我们一切就绪之后，可以测试下同步功能，修改任意一个文件，等待一会(取决于你配置的同步时间)，然后直接去集群中验证即可。 注意事项① 层级结构中，namespace目录下每一个文件夹都代表着你对应集群下的namespace，你加了这个目录则表示我要通过acm管理这个namespace了；当然没加的，但是集群又存在的，不会影响。② 层级结构中，不用再写namespace.yaml这个创建namespace的文件了，默认他会读取目录名称，并创建相应的namespace。③ 如果集群存在一个deployment为A，那么我创建一个deployment-A.yaml，配置名称完全跟集群中的一致，那么他不会重新创建，而是直接纳管。④ 非结构化模式中也可以使用cluster-selector，你只需要在git仓库里新建一个clusterregistry目录即可，文件内容和用法跟层级结构一样。⑤ …","categories":[{"name":"anthos","slug":"anthos","permalink":"https://blog.itmonkey.icu/categories/anthos/"}],"tags":[{"name":"gcp","slug":"gcp","permalink":"https://blog.itmonkey.icu/tags/gcp/"},{"name":"gke","slug":"gke","permalink":"https://blog.itmonkey.icu/tags/gke/"},{"name":"anthos","slug":"anthos","permalink":"https://blog.itmonkey.icu/tags/anthos/"}]},{"title":"从零开始创建一个sre的门户网站","slug":"sre-portal-website-start","date":"2021-03-19T07:46:57.000Z","updated":"2021-03-23T11:50:17.333Z","comments":true,"path":"2021/03/19/sre-portal-website-start/","link":"","permalink":"https://blog.itmonkey.icu/2021/03/19/sre-portal-website-start/","excerpt":"背景运维工具挺多，不过貌似没有一个专门的门户网站去展示展示。","text":"背景运维工具挺多，不过貌似没有一个专门的门户网站去展示展示。 想法 展示工具 运维通知 其他 初始化初始化一个项目： 123@ vue-init webpack sreportal@ cd sreportal@ npm run dev 安装个依赖： 1npm install element-plus --save","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://blog.itmonkey.icu/categories/DevOps/"}],"tags":[{"name":"运维门户","slug":"运维门户","permalink":"https://blog.itmonkey.icu/tags/%E8%BF%90%E7%BB%B4%E9%97%A8%E6%88%B7/"},{"name":"vue","slug":"vue","permalink":"https://blog.itmonkey.icu/tags/vue/"}]},{"title":"如何梳理我负责的业务(SRE)","slug":"how-to-comb-my-business","date":"2021-03-19T06:18:35.000Z","updated":"2021-03-19T06:26:01.482Z","comments":true,"path":"2021/03/19/how-to-comb-my-business/","link":"","permalink":"https://blog.itmonkey.icu/2021/03/19/how-to-comb-my-business/","excerpt":"背景业务一多，我们就需要针对性的梳理负责的业务，不能一个业务一个样子。有效的梳理业务能带来以下好处：","text":"背景业务一多，我们就需要针对性的梳理负责的业务，不能一个业务一个样子。有效的梳理业务能带来以下好处： 1.更加深入了解业务2.能从不同的业务中抽象出相同规则，做一些标准化自动化工作3.业务之间运维工作可以有参考4.业务故障能帮助快速定位问题点5.让ab岗，轮值更进一步 注意当你梳理了业务文档，以下点是需要你注意的：1.定期/及时更新业务文档2.要覆盖全面，不仅包含业务文档，也要有运维所做的配套设置(告警、监控、cicd等)3.有一个总表记录业务变更 梳理条目那一个业务，我们需要梳理哪些基础的信息呢？ 基础平台信息 业务部署架构 后端资源信息 CI/CD 监控、日志、告警、自动化 灾备、预案信息 变更记录 日常问题排查记录 优化 总结 注意事项 具体说明 梳理条目 梳理细节 备注 基础平台信息 部署在哪里如果是新环境，我怎么进入环境平台的权限相关备注如果是新平台，基础环境要备注好，vpc、子网、nat等 业务部署架构 一个架构图还是要的服务有哪些，服务名称、服务作用、资源占用域名有哪些、以及解析地址cdn配置等 后端资源信息 实例列表实例规格资源同步相关文档 CI/CD cicd流程用什么是否有自建cicd工具 监控、日志、告警、自动化 监控地址(基础监控，业务监控)日志地址告警发送人、告警发送规则其他自动化配置 灾备、预案信息 灾备文档梳理预案文档梳理日常演练记录压测文档记录 变更记录 业务侧：业务变更、人员变更运维侧：配置变更、支撑变更 日常问题排查记录 故障记录日常问题记录 优化 性能优化支撑优化(支撑工具开发等)成本优化 总结 稳定性运营周报月报年报重要节日报 注意事项 证书更新特殊配置","categories":[{"name":"运维进阶","slug":"运维进阶","permalink":"https://blog.itmonkey.icu/categories/%E8%BF%90%E7%BB%B4%E8%BF%9B%E9%98%B6/"}],"tags":[{"name":"运维","slug":"运维","permalink":"https://blog.itmonkey.icu/tags/%E8%BF%90%E7%BB%B4/"},{"name":"梳理","slug":"梳理","permalink":"https://blog.itmonkey.icu/tags/%E6%A2%B3%E7%90%86/"}]},{"title":"关于前端自动刷新cdn相关想法","slug":"auto-refresh-cdn","date":"2021-03-08T07:43:37.000Z","updated":"2021-03-08T09:20:44.982Z","comments":true,"path":"2021/03/08/auto-refresh-cdn/","link":"","permalink":"https://blog.itmonkey.icu/2021/03/08/auto-refresh-cdn/","excerpt":"背景当前我司的前端构建+部署架构是：jenkins或者gitlabci—容器平台。 正常我们刷新cdn的流程是jenkins构建完代码后，在容器平台进行新版本发布，新版本发布成功后，手动刷新cdn。这样的操作会比较繁琐。","text":"背景当前我司的前端构建+部署架构是：jenkins或者gitlabci—容器平台。 正常我们刷新cdn的流程是jenkins构建完代码后，在容器平台进行新版本发布，新版本发布成功后，手动刷新cdn。这样的操作会比较繁琐。 想法针对上述背景，我们期望达到的目的就是版本发布完成后，自动刷新cdn，不需要手动介入。 当前现状有两个方面： jenkins构建完成之后，容器平台会自动发布该版本 jenkins构建完成之后，需要手动到容器平台发布该版本 所以针对上述所有的情况，我的想法就是：1.jenkins构建完成之后，创建一个刷新任务，这个任务包含哪个服务，要刷新什么，刷新状态等2.写一个定时任务，去实时监听是否有未刷新的任务3.如果获取到未刷新的任务，去监听这个任务对应服务的发布状态4.如果服务发布状态为已经发布，那么就可以触发刷新cdn动作了 开始1.创建一个任务平台这个平台需要实现如下接口：1.新增任务2.修改任务状态3.根据id查询任务详情4.获取所有任务列表 具体的就差不多这些吧： 12345[GIN-debug] POST /api/v1/cdnr/task --&gt; whyme-api/controller/cdn.AddRefreshTask (4 handlers)[GIN-debug] GET /api/v1/cdnr/alltask --&gt; whyme-api/controller/cdn.GetTaskList (4 handlers)[GIN-debug] GET /api/v1/cdnr/tasks --&gt; whyme-api/controller/cdn.GetUnCompletedTask (4 handlers)[GIN-debug] GET /api/v1/cdnr/task/:taskid --&gt; whyme-api/controller/cdn.GetRefreshTaskStatus (4 handlers)[GIN-debug] PUT /api/v1/cdnr/task/:taskid/:status --&gt; whyme-api/controller/cdn.UpdateRefreshTaskStatus (4 handlers) 关于任务字段的设计： 12345678taskid 任务idnamespace 项目名称service 服务名称version 本次发布的服务版本url 要刷新的地址createtime 创建时间updatetime 更新时间status 0代表未刷新，1代表刷新了 如果你的刷新量级比较大或者有更高的要求，你可以上kafka，像我们就很简单，直接接mysql。 2.写一个定时任务这个定时的复杂度取决于你上述任务字段设计的逻辑，像我上述涉及的，那这个定时任务会比较复杂点：1.实现未刷新任务查询2.实现服务发布状态获取3.实现修改刷新任务4.实现刷新cdn 如果你把这些状态status细分一下，就可以用多个定时任务分别去修改和获取，所以还是看你复杂度，如果复杂就分开，简单写在一起也没关系。 最终定时任务执行后差不多就是这个流程吧： 3.加一个友好的前端界面写一个页面去展示你的刷新任务状态 4.加一个友好的通知机制目前当有刷新任务完成刷新，会通知出来，不论你是用钉钉，企业微信还是邮件，自己写就完事了 开源想法代码都很简单，开源不开源没啥必要。。真的！ 想法和逻辑有了，那还不是很快就写完了","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://blog.itmonkey.icu/categories/DevOps/"}],"tags":[{"name":"cdn","slug":"cdn","permalink":"https://blog.itmonkey.icu/tags/cdn/"}]},{"title":"定义ingress 403页面","slug":"ingress-self-define-403","date":"2021-02-02T05:43:39.000Z","updated":"2021-02-02T07:16:08.196Z","comments":true,"path":"2021/02/02/ingress-self-define-403/","link":"","permalink":"https://blog.itmonkey.icu/2021/02/02/ingress-self-define-403/","excerpt":"背景默认的ingress controller提供的403 error页面太丑了，想要定制化一下","text":"背景默认的ingress controller提供的403 error页面太丑了，想要定制化一下 实现1.增加一个configmap，定义403页面2.挂载到ingress-controller的pod里3.对单个ingress对象增加annotation注解 开始吧增加一个configmap123456meitu403.html: | &lt;head&gt;&lt;meta charset=\"UTF-8\"&gt;&lt;/head&gt; &lt;html&gt; &lt;h1 align=\"center\"&gt;禁止外部访问&lt;/h1&gt; &lt;p align=\"center\"&gt;请连接美图wifi或者有线!&lt;/p&gt; &lt;/html&gt; 挂载到ingress-controller的pod里123volumeMounts: - mountPath: /etc/nginx/conf name: nginx-whitelist-volume 12345volumes: - configMap: defaultMode: 420 name: nginx-whitelist name: nginx-whitelist-volume 增加annotation1nginx.ingress.kubernetes.io/server-snippet: error_page 403 /meitu403.html; location /meitu403.html &#123; internal; root /etc/nginx/conf; allow all; &#125; 效果展示","categories":[{"name":"k8s","slug":"k8s","permalink":"https://blog.itmonkey.icu/categories/k8s/"}],"tags":[{"name":"ingress","slug":"ingress","permalink":"https://blog.itmonkey.icu/tags/ingress/"}]},{"title":"修改gcp默认的cloudsql网段","slug":"gcp-modify-default-cloudsql-iprange","date":"2021-01-18T07:40:38.000Z","updated":"2021-02-26T08:18:50.327Z","comments":true,"path":"2021/01/18/gcp-modify-default-cloudsql-iprange/","link":"","permalink":"https://blog.itmonkey.icu/2021/01/18/gcp-modify-default-cloudsql-iprange/","excerpt":"背景由于gcp和国内是有专线的，所以各个网络是互通，这样也就是需要保证每一个网段不能有冲突。但是gcp中，创建mysql的时候会默认自动分配一段网段，这个默认是没法指定的，所以这样就会有网段冲突的风险。","text":"背景由于gcp和国内是有专线的，所以各个网络是互通，这样也就是需要保证每一个网段不能有冲突。但是gcp中，创建mysql的时候会默认自动分配一段网段，这个默认是没法指定的，所以这样就会有网段冲突的风险。 概念当我们创建了mysql后，会默认生成两个private service connection(专有服务连接，可以在vpc下面看到)，这个的本质就是做vpc peering。如下是两个专有服务连接，一个是连接到cloud sql的project，一个是连接到其他公共服务的project。 12servicenetworking-googleapis-comcloudsql-mysql-googleapis-com 其实你在vpc peering中也可以看到，默认会有两个： 之后你就会看到默认会给你这两个专有服务连接分配了一段地址，这个名字叫google-managed-services-+vpc名字，这个地址段是随机分配的,并且也绑定上了两个专有连接。 思路思路很简单了，自定义创建一个ip range地址段，然后手动绑定到两个专有连接上。 1.创建ip range 地址段 2.使用gcloud更新翻看官网文档，这个需要使用gcloud命令行来更新，不然只能对专有连接的ip段进行叠加。我们只需要更新servicenetworking-googleapis-com这个专有连接即可，不用更新cloudsql。 12345gcloud services vpc-peerings update \\ --service=servicenetworking.googleapis.com \\ --ranges=test \\ --network=xxxx \\ --force 注意：range就是你刚自定义创建的地址段名称，network就是你vpc的名称。 3.验证最终你会发现你的自定义地址段就生效了 温馨提示1.自定义ip range必须是24位的2.mysql、redis需要一个单独的ip range3.mc和redis可以共用一个ip range4.cloudsql-ip-range是vpc级别的，不是region级别，多个ip-range，只有当一个用完了，才会用下一个5.private dns zone也是vpc级别的，不是region级别，一个vpc下不同region是走的一个dns zone 如果你只分配了一个ip range，那么创建云服务的时候会报这个错： 参考链接：https://cloud.google.com/vpc/docs/configure-private-services-access#allocating-range 温馨提示2如果你跟国内有bgp专线，那么有两种情况： custom 子网通告到对端，需要在哪里配置 自定义vpc peering子网通告到对端，需要在哪里配置 第一种情况，直接在vpc下面新增一个子网，然后直接在bgp router上配置自定义子网即可： 第二种情况，新增的vpc peering子网，比如mysql，redis等，需要在bpg会话配置自定义子网：","categories":[{"name":"云运维","slug":"云运维","permalink":"https://blog.itmonkey.icu/categories/%E4%BA%91%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"gcp","slug":"gcp","permalink":"https://blog.itmonkey.icu/tags/gcp/"}]},{"title":"端口被短连接耗尽了怎么办","slug":"tcp-short-port-exhausted","date":"2021-01-14T03:30:32.000Z","updated":"2021-01-14T03:59:17.384Z","comments":true,"path":"2021/01/14/tcp-short-port-exhausted/","link":"","permalink":"https://blog.itmonkey.icu/2021/01/14/tcp-short-port-exhausted/","excerpt":"背景最近发现业务POD使用短连接过程中，端口被耗尽，一系列操作。","text":"背景最近发现业务POD使用短连接过程中，端口被耗尽，一系列操作。 方案 扩大端口数量 降低单pod配置，横向扩容pod个数 改成长链接 调整内核参数 扩大端口数量pod新增内核参数 1net.ipv4.ip_local_port_range = 1024 65535 横向扩容如果上述修改端口限制无法解决你的问题，那么能快速解决问题的方法就是降低你单pod规格，使得pod个数增加，分担端口压力。 修改成长链接改这个，也需要你业务支持，编程语言有比较好的支持。 调整内核参数这两个参数配合使用，reuse能复用time_wait端口 12net.ipv4.tcp_timestamps=1 # 与tw_reuse一起用net.ipv4.tcp_tw_reuse=1 这个参数没必要开tcp_rw_recycle 额外说一句有同学说修改这个参数tcp_fin_timeout，字面意思是缩短time_wait的时间，加速端口回收，其实没啥*用，看看内核源码，就知道默认60s，除非修改内核代码，重新编译内核。 来一个生产环境机器的内核参数列表123456789101112131415161718192021net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_timestamps=1net.ipv4.tcp_tw_recycle = 0net.ipv4.tcp_syncookies = 1net.ipv4.tcp_synack_retries = 2net.ipv4.tcp_mem = 94500000 915000000 927000000net.ipv4.ip_local_reserved_ports=30000-32767net.ipv4.tcp_max_syn_backlog = 819200net.ipv4.ip_forward = 1net.ipv4.conf.default.rp_filter = 1net.ipv4.conf.default.accept_source_route = 0net.core.wmem_max = 16777216net.core.wmem_default = 8388608net.core.somaxconn = 32768net.core.rmem_max = 16777216net.core.rmem_default = 8388608net.core.netdev_max_backlog = 32768kernel.sysrq = 0kernel.core_uses_pid = 1net.ipv4.tcp_max_tw_buckets = 30000000vm.swappiness=0","categories":[{"name":"基础运维","slug":"基础运维","permalink":"https://blog.itmonkey.icu/categories/%E5%9F%BA%E7%A1%80%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"短连接","slug":"短连接","permalink":"https://blog.itmonkey.icu/tags/%E7%9F%AD%E8%BF%9E%E6%8E%A5/"},{"name":"内核参数","slug":"内核参数","permalink":"https://blog.itmonkey.icu/tags/%E5%86%85%E6%A0%B8%E5%8F%82%E6%95%B0/"}]},{"title":"《3》动手写一个operator","slug":"write-an-operator-3","date":"2020-12-25T08:55:10.000Z","updated":"2020-12-25T08:58:53.534Z","comments":true,"path":"2020/12/25/write-an-operator-3/","link":"","permalink":"https://blog.itmonkey.icu/2020/12/25/write-an-operator-3/","excerpt":"说明我们已经完成了operator基本cr创建和删除的流程，接下来我们就要去实现具体的逻辑代码。本章主要实现如何启动这个定时任务。","text":"说明我们已经完成了operator基本cr创建和删除的流程，接下来我们就要去实现具体的逻辑代码。本章主要实现如何启动这个定时任务。","categories":[{"name":"k8s","slug":"k8s","permalink":"https://blog.itmonkey.icu/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://blog.itmonkey.icu/tags/k8s/"},{"name":"operator","slug":"operator","permalink":"https://blog.itmonkey.icu/tags/operator/"}]},{"title":"《2》动手写一个operator","slug":"write-an-operator-2","date":"2020-12-22T03:05:26.000Z","updated":"2020-12-23T08:18:03.586Z","comments":true,"path":"2020/12/22/write-an-operator-2/","link":"","permalink":"https://blog.itmonkey.icu/2020/12/22/write-an-operator-2/","excerpt":"说明我们昨天已经明确项目以及创建好基础代码框架了并运行起来了。今天我们主要探讨下这个项目想要做成什么样，以及一些crd的设计。","text":"说明我们昨天已经明确项目以及创建好基础代码框架了并运行起来了。今天我们主要探讨下这个项目想要做成什么样，以及一些crd的设计。 介绍首先我们熟悉下代码目录，我这边只讲你会用到的，其他的我不一定能讲的明白。图中1、2两个文件，是我们需要修改和主要编写的文件。 第一个文件主要是定义你的crd文件的参数，包括spec和status参数. 第二个文件主要是写你controller的逻辑，比如你监听到了合适的事件消息，如何去相应，不论是调用三方接口或者去创建你的cr资源，都是响应的一种形式。 设计针对我们这个项目，我们是想要实现自定义cr资源能够管理我们的定时任务，并触发我们的deployment滚动更新。所以我们的cr资源字段应该这样设计： spec，我们涉及3个字段，一个是这个cr资源绑定的deployment，另一个是定时任务表达式，最后一个是是否启动这个定时任务 status，我们涉及到的字段包含状态和条件，而条件是一个array，每一条都记录着当前的状态信息。 开发 首先确定我们要监听的资源 修改pkg/controller/eru/eru_controller.go文件,在这里可以看到我们监听两个资源，一个你自定义的cr资源，一个是deployment资源 123456789101112131415161718func add(mgr manager.Manager, r reconcile.Reconciler) error &#123; c, err := controller.New(\"eru-controller\", mgr, controller.Options&#123;Reconciler: r&#125;) if err != nil &#123; return err &#125; err = c.Watch(&amp;source.Kind&#123;Type: &amp;mtsrev1.Eru&#123;&#125;&#125;, &amp;handler.EnqueueRequestForObject&#123;&#125;) if err != nil &#123; return err &#125; err = c.Watch(&amp;source.Kind&#123;Type: &amp;appsv1.Deployment&#123;&#125;&#125;, &amp;handler.EnqueueRequestForObject&#123;&#125;) if err != nil &#123; return err &#125; return nil&#125; 判断deployment和eru自定义资源是否存在，我们的逻辑是，如果deployment不存在了，那么我们就删除与其绑定的eru资源. 修改pkg/controller/eru/eru_controller.go文件，Reconcile这个作为主函数 1234567891011121314151617181920// deployment 和 eru 实例deployment_instance := &amp;appsv1.Deployment&#123;&#125;eru_instance := &amp;mtsrev1.Eru&#123;&#125;// deploymentExist 和 eruExist 实例deploymentExist := r.client.Get(context.TODO(), request.NamespacedName, deployment_instance)eruExist := r.client.Get(context.TODO(),request.NamespacedName,eru_instance)// *****判断deployment是否存在*****//如果不存在，那么就要一同删除绑定的eru cr资源if deploymentExist != nil &amp;&amp; errors.IsNotFound(deploymentExist) &#123; if eruExist == nil &#123; erulog.Warningf(\"this deployment : %v not found,maybe deleted,so eru cr need to deleted\",request.String()) if deletestatus ,err := r.DeleteEruCr(eru_instance); deletestatus &#123; return reconcile.Result&#123;&#125;,nil &#125; else &#123; return reconcile.Result&#123;&#125;, err &#125; &#125;&#125; 删除自定义cr资源代码： 12345678910111213141516// 删除eru cr资源func (r *ReconcileEru) DeleteEruCr(eru *mtsrev1.Eru) (bool, error) &#123; // 删除aom 策略 // 华为云目前观测到有定时去清理aom规则，如果deployment被删除的话。 retryError := retry.RetryOnConflict(retry.DefaultRetry, func() error &#123; return r.client.Delete(context.TODO(), eru) &#125;) // 判断删除cr资源是否成功 if retryError != nil &#123; erulog.Errorf(\"eru delete failure, %v/%v\", eru.Namespace, eru.Name) return false, retryError &#125; else &#123; erulog.Infof(\"eru delete success, %v/%v\", eru.Namespace, eru.Name) return true, nil &#125;&#125; 当我们的deployment存在，我们需要去判断他是否含有对应的annotations,约定的是mtsre.meitu.com/eru: &quot;true&quot; 12345678910//如果存在，请继续// 我们继续检查deployment是否有设置符合条件的annotations，我们约定是否含有`mtsre.meitu.com/eru: \"true\"`，就添加deploymentAnnotation := deployment_instance.Annotationsif _,ok := deploymentAnnotation[\"mtsre.meitu.com/eru\"]; ok &#123; fmt.Println(\"yes have\") // 判断eru资源是否存在，如果不存在，那么就创建&#125; else &#123; // 该deployment没有设置对应的annotations，所以我们不做处理，进行下一次循环 return reconcile.Result&#123;&#125;, nil&#125; 创建cr资源，我们主要做3个事情，第一创建cr资源，第二更新cr资源的annotation，第三更新cr资源的status 1234567891011121314151617181920212223242526272829303132333435363738// 创建eru cr资源func (r *ReconcileEru) CreateEruCr(deployment *appsv1.Deployment) (bool,error) &#123; // 如果创建失败，那么就更新status，说失败了 eru := eru_res.NewEru(deployment) if err := r.client.Create(context.TODO(),eru); err != nil &#123; erulog.Errorf(\"create eru cr failure,reason: %v\",err) failureCondition := mtsrev1.EruCondition&#123; Ready: false, Reason: \"create eru cr failure\", Message: \"create eru cr failure\", LastedTranslationTime: v1.Now(), &#125; if updateerror := r.updateStatus(eru,failureCondition,mtsrev1.EruFailure); updateerror != nil &#123; return false,updateerror &#125; return false,err &#125; // 如果创建cr成功，我们需要更新annotation、更新status，默认cr的定时任务是停止的，我们需要用户手动去触发是否启动，并修改对应的定时任务表达式 // 这里是更新annotation deploymentData,enabledData,scheduleData := toString(eru) eru.Annotations = map[string]string&#123; \"mtsre.meitu.com/deploymentName\": deploymentData, \"mtsre.meitu.com/enabled\": enabledData, \"mtsre.meitu.com/schedule\": scheduleData, &#125; r.UpdateAnnotations(eru) // 这里是更新status stopCondition := mtsrev1.EruCondition&#123; Ready: true, Reason: \"create eru cr success,wait to start\", Message: \"create eru cr success,wait to start\", LastedTranslationTime: v1.Time&#123;&#125;, &#125; if updateerror := r.updateStatus(eru,stopCondition,mtsrev1.EruStoping); updateerror != nil &#123; return false,updateerror &#125; return true,nil&#125; eru_res.NewEru这个方法很简单，初始化了一个cr对象,并给这个cr对象设置一些初始值: 1234567891011121314151617func NewEru(deployment *appsv1.Deployment) *mtsrev1.Eru &#123; return &amp;mtsrev1.Eru&#123; TypeMeta: v1.TypeMeta&#123; Kind: \"Eru\", APIVersion: \"v1\", &#125;, ObjectMeta: v1.ObjectMeta&#123; Name: deployment.Name, Namespace: deployment.Namespace, &#125;, Spec: mtsrev1.EruSpec&#123; DeploymentName: deployment.Name, Enabled: false, Schedule: \"0 0 * * *\", // 默认每天凌晨执行一次 &#125;, &#125;&#125; 更新status和更新annotation都单独抽离了方法，如下： 123456789101112131415//更新cr的status函数func (r *ReconcileEru) updateStatus(eru *mtsrev1.Eru, condition mtsrev1.EruCondition, phase mtsrev1.EruPhase) error &#123; eru.Status.Phase = phase eru.Status.Conditions = append(eru.Status.Conditions, condition) return retry.RetryOnConflict(retry.DefaultRetry, func() error &#123; return r.client.Status().Update(context.TODO(), eru) &#125;)&#125;// 更新cr的annotation函数，更新一般不会失败，所以不做判断处理func (r *ReconcileEru) UpdateAnnotations(eru *mtsrev1.Eru) &#123; _ = retry.RetryOnConflict(retry.DefaultRetry, func() error &#123; return r.client.Update(context.TODO(), eru) &#125;)&#125; 测试运行因为我们修改了types.go这个文件，所以需要执行如下代码去生成代码： 1$ operator-sdk generate k8s 最后我们就可以运行试下了： 1operator-sdk run --local --namespace your-namespace 可以看到输出如下： 我们创建一个deployment试试，然后我们先不带annotation，看看有什么效果测试发现，只会监听到你这个deployment的信息，由于没有任何配置，所以不会创建cr资源等。 我们删除deployment，创建一个带正确annotation的deployment测试，看看效果可以看到，我们首先是检测到了annotation，然后就开始创建cr资源 接下来我们看下创建的cr资源是否正常 1234567891011121314151617181920212223242526272829➜ kubectl get eru -n lb6NAME AGEopsnatmonitor-v1 5m➜ kubectl get eru opsnatmonitor-v1 -n lb6 -o yamlapiVersion: mtsre.meitu.com/v1kind: Erumetadata: annotations: mtsre.meitu.com/deploymentName: '\"opsnatmonitor-v1\"' mtsre.meitu.com/enabled: \"false\" mtsre.meitu.com/schedule: '\"0 0 * * *\"' creationTimestamp: \"2020-12-23T07:54:43Z\" generation: 1 name: opsnatmonitor-v1 namespace: lb6 resourceVersion: \"147596357\" selfLink: /apis/mtsre.meitu.com/v1/namespaces/lb6/erus/opsnatmonitor-v1 uid: 1e4a28d9-44f4-11eb-89e4-fa163ebf9460spec: deploymentname: opsnatmonitor-v1 enabled: false schedule: 0 0 * * *status: conditions: - lastedTranslationTime: null message: create eru cr success,wait to start ready: true reason: create eru cr success,wait to start phase: Stoping 最后我们可以测试下删除deployment，会不会一起删除我们的cr资源 1➜ kubectl delete deployment *** -n *** controller完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194package eruimport ( \"context\" \"encoding/json\" \"eru-operator/resource/eru_res\" mtsrev1 \"eru-operator/pkg/apis/mtsre/v1\" erulog \"eru-operator/tools/log\" appsv1 \"k8s.io/api/apps/v1\" v1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" \"k8s.io/client-go/util/retry\" \"k8s.io/apimachinery/pkg/api/errors\" \"k8s.io/apimachinery/pkg/runtime\" \"sigs.k8s.io/controller-runtime/pkg/client\" \"sigs.k8s.io/controller-runtime/pkg/controller\" \"sigs.k8s.io/controller-runtime/pkg/handler\" logf \"sigs.k8s.io/controller-runtime/pkg/log\" \"sigs.k8s.io/controller-runtime/pkg/manager\" \"sigs.k8s.io/controller-runtime/pkg/reconcile\" \"sigs.k8s.io/controller-runtime/pkg/source\")var log = logf.Log.WithName(\"controller_eru\")func Add(mgr manager.Manager) error &#123; return add(mgr, newReconciler(mgr))&#125;func newReconciler(mgr manager.Manager) reconcile.Reconciler &#123; return &amp;ReconcileEru&#123;client: mgr.GetClient(), scheme: mgr.GetScheme()&#125;&#125;func add(mgr manager.Manager, r reconcile.Reconciler) error &#123; c, err := controller.New(\"eru-controller\", mgr, controller.Options&#123;Reconciler: r&#125;) if err != nil &#123; return err &#125; err = c.Watch(&amp;source.Kind&#123;Type: &amp;mtsrev1.Eru&#123;&#125;&#125;, &amp;handler.EnqueueRequestForObject&#123;&#125;) if err != nil &#123; return err &#125; err = c.Watch(&amp;source.Kind&#123;Type: &amp;appsv1.Deployment&#123;&#125;&#125;, &amp;handler.EnqueueRequestForObject&#123;&#125;) if err != nil &#123; return err &#125; return nil&#125;var _ reconcile.Reconciler = &amp;ReconcileEru&#123;&#125;type ReconcileEru struct &#123; client client.Client scheme *runtime.Scheme&#125;func (r *ReconcileEru) Reconcile(request reconcile.Request) (reconcile.Result, error) &#123; erulog.Infof(\"Reconciling Eru,%v\",request.String()) // deployment 和 eru 实例 deployment_instance := &amp;appsv1.Deployment&#123;&#125; eru_instance := &amp;mtsrev1.Eru&#123;&#125; // deploymentExist 和 eruExist 实例 deploymentExist := r.client.Get(context.TODO(), request.NamespacedName, deployment_instance) eruExist := r.client.Get(context.TODO(),request.NamespacedName,eru_instance) // *****判断deployment是否存在***** //如果不存在，那么就要一同删除绑定的eru cr资源 if deploymentExist != nil &amp;&amp; errors.IsNotFound(deploymentExist) &#123; if eruExist == nil &#123; erulog.Warningf(\"this deployment : %v not found,maybe deleted,so eru cr need to deleted\",request.String()) if deletestatus ,err := r.DeleteEruCr(eru_instance); deletestatus &#123; return reconcile.Result&#123;&#125;,nil &#125; else &#123; return reconcile.Result&#123;&#125;, err &#125; &#125; &#125; //如果存在，请继续 // 我们继续检查deployment是否有设置符合条件的annotations，我们约定是否含有`mtsre.meitu.com/eru: \"true\"`，就添加 deploymentAnnotation := deployment_instance.Annotations if _,ok := deploymentAnnotation[\"mtsre.meitu.com/eru\"]; ok &#123; // 判断eru资源是否存在，如果不存在，那么就创建 if deploymentAnnotation[\"mtsre.meitu.com/eru\"] == \"true\" &amp;&amp; eruExist != nil &#123; erulog.Infof(\"check annotation success,now to create cr resource with deployment : %v\",request.String()) if _, createcrerror := r.CreateEruCr(deployment_instance); createcrerror != nil &#123; erulog.Errorf(\"create cr resource with deployment : %v , failure : %v\",request.String(),createcrerror) return reconcile.Result&#123;&#125;, createcrerror &#125; erulog.Infof(\"create cr resource with deployment : %v success\",request.String()) return reconcile.Result&#123;&#125;,nil &#125; else if deploymentAnnotation[\"mtsre.meitu.com/eru\"] == \"true\" &amp;&amp; eruExist == nil &#123; erulog.Infof(\"Start checking eru for updates...\") &#125; else &#123; erulog.Infof(\"annotations not exist or config error\") return reconcile.Result&#123;&#125;,nil &#125; &#125; else &#123; // 该deployment没有设置对应的annotations，所以我们不做处理，进行下一次循环 return reconcile.Result&#123;&#125;, nil &#125; // 这边就判断我们的cr资源是否需要更新了 return reconcile.Result&#123;&#125;, nil&#125;// 创建eru cr资源func (r *ReconcileEru) CreateEruCr(deployment *appsv1.Deployment) (bool,error) &#123; // 如果创建失败，那么就更新status，说失败了 eru := eru_res.NewEru(deployment) if err := r.client.Create(context.TODO(),eru); err != nil &#123; erulog.Errorf(\"create eru cr failure,reason: %v\",err) failureCondition := mtsrev1.EruCondition&#123; Ready: false, Reason: \"create eru cr failure\", Message: \"create eru cr failure\", LastedTranslationTime: v1.Now(), &#125; if updateerror := r.updateStatus(eru,failureCondition,mtsrev1.EruFailure); updateerror != nil &#123; return false,updateerror &#125; return false,err &#125; // 如果创建cr成功，我们需要更新annotation、更新status，默认cr的定时任务是停止的，我们需要用户手动去触发是否启动，并修改对应的定时任务表达式 // 这里是更新annotation deploymentData,enabledData,scheduleData := toString(eru) eru.Annotations = map[string]string&#123; \"mtsre.meitu.com/deploymentName\": deploymentData, \"mtsre.meitu.com/enabled\": enabledData, \"mtsre.meitu.com/schedule\": scheduleData, &#125; r.UpdateAnnotations(eru) // 这里是更新status stopCondition := mtsrev1.EruCondition&#123; Ready: true, Reason: \"create eru cr success,wait to start\", Message: \"create eru cr success,wait to start\", LastedTranslationTime: v1.Time&#123;&#125;, &#125; if updateerror := r.updateStatus(eru,stopCondition,mtsrev1.EruStoping); updateerror != nil &#123; return false,updateerror &#125; return true,nil&#125;// 删除eru cr资源func (r *ReconcileEru) DeleteEruCr(eru *mtsrev1.Eru) (bool, error) &#123; // 删除aom 策略 // 华为云目前观测到有定时去清理aom规则，如果deployment被删除的话。 retryError := retry.RetryOnConflict(retry.DefaultRetry, func() error &#123; return r.client.Delete(context.TODO(), eru) &#125;) // 判断删除cr资源是否成功 if retryError != nil &#123; erulog.Errorf(\"eru delete failure, %v/%v\", eru.Namespace, eru.Name) return false, retryError &#125; else &#123; erulog.Infof(\"eru delete success, %v/%v\", eru.Namespace, eru.Name) return true, nil &#125;&#125;//更新cr的status函数func (r *ReconcileEru) updateStatus(eru *mtsrev1.Eru, condition mtsrev1.EruCondition, phase mtsrev1.EruPhase) error &#123; eru.Status.Phase = phase eru.Status.Conditions = append(eru.Status.Conditions, condition) return retry.RetryOnConflict(retry.DefaultRetry, func() error &#123; return r.client.Status().Update(context.TODO(), eru) &#125;)&#125;// 更新cr的annotation函数，更新一般不会失败，所以不做判断处理func (r *ReconcileEru) UpdateAnnotations(eru *mtsrev1.Eru) &#123; _ = retry.RetryOnConflict(retry.DefaultRetry, func() error &#123; return r.client.Update(context.TODO(), eru) &#125;)&#125;// 把spec转化成stringfunc toString(eru *mtsrev1.Eru) (string,string,string) &#123; deploymentData,_ := json.Marshal(eru.Spec.DeploymentName) enabledData,_ := json.Marshal(eru.Spec.Enabled) scheduleData,_ := json.Marshal(eru.Spec.Schedule) return string(deploymentData),string(enabledData),string(scheduleData)&#125;// 把string转化成spec 计划下一篇我们主要实现更新cr资源的逻辑，如何让这个定时任务run起来以及修改定时任务表达式之后，怎么生效。","categories":[{"name":"k8s","slug":"k8s","permalink":"https://blog.itmonkey.icu/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://blog.itmonkey.icu/tags/k8s/"},{"name":"operator","slug":"operator","permalink":"https://blog.itmonkey.icu/tags/operator/"}]},{"title":"《1》动手写一个operator","slug":"write-an-operator-1","date":"2020-12-22T03:04:09.000Z","updated":"2020-12-22T08:37:52.082Z","comments":true,"path":"2020/12/22/write-an-operator-1/","link":"","permalink":"https://blog.itmonkey.icu/2020/12/22/write-an-operator-1/","excerpt":"背景前几天学了一下go，准备拿一个项目练练手，刚好了解到operator，想写一个管理定时任务的operator，这个定时任务主要是定时滚动更新deployment。举个例子：我创建一个cr资源，绑定了某个deployment A，当匹配到cron表达式，那么就定时滚动更新A deployment下面所有的pod。 (优雅的更新)","text":"背景前几天学了一下go，准备拿一个项目练练手，刚好了解到operator，想写一个管理定时任务的operator，这个定时任务主要是定时滚动更新deployment。举个例子：我创建一个cr资源，绑定了某个deployment A，当匹配到cron表达式，那么就定时滚动更新A deployment下面所有的pod。 (优雅的更新) 项目前提要求 会go 懂k8s 听过operator 见过operator-sdk 啥是crd学习operator，首先你要知道什么是crd(CustomResourceDefinition)，可以去官网看看。 想知道最简单的crd例子，可以看之前的博客：CRD 我个人理解，operator就是通过自定义的cr资源去管理符合条件的pod，类似deployment. 可以看看当前你集群中有哪些crd资源： 1234567891011121314$ kubectl get crdNAME CREATED ATalertmanagers.monitoring.coreos.com 2019-10-12T10:07:13Zapplications.app.k8s.io 2020-05-21T15:22:07Zcsidrivers.csi.storage.k8s.io 2019-10-11T08:25:46Zcsinodeinfos.csi.storage.k8s.io 2019-10-11T08:25:46Zhorizontalnodeautoscalers.autoscaling.cce.io 2020-02-13T04:16:57Znetwork-attachment-definitions.k8s.cni.cncf.io 2019-10-11T08:25:07Zpermissions.rbac.cce.io 2019-10-11T08:25:10Zpodmonitors.monitoring.coreos.com 2019-10-12T10:07:18Zprometheuses.monitoring.coreos.com 2019-10-12T10:07:24Zprometheusrules.monitoring.coreos.com 2019-10-12T10:07:29Zservicemonitors.monitoring.coreos.com 2019-10-12T10:07:35Zsyncvolumes.csms.storage 2020-03-10T04:46:34Z operator工作原理Operator 实际上作为kubernetes自定义扩展资源注册到controller-manager,通过list and watch的方式监听对应资源的变化，然后在周期内的各个环节做相应的协调处理。所谓的处理就是operator实现由状态的应用的核心部分，当然不同应用其处理的方式也不同 代码生成器介绍operator中核心controller的代码都已经被封装好了，我们只需要写具体的逻辑即可。 即：你不用关心controller如何监听这个资源，不用关心去读取队列消息等等，只需要写当你获取到这个资源变化的事件信息，你应该做什么，或者应该响应什么。 那么接下来就是介绍operator-sdk了，他就很好的帮你实现了上述的功能，自动生成一套代码，你往里面插入数据就行了。 项目介绍打算：目前我们集群中部分业务会出现内存或者显存溢出的问题，所以想针对这些异常pod做一些定时重启(滚动更新一下)的动作。 想法：deployment配置上特定的annotation，比如mtsre.meitu.com/eru: &quot;true&quot;，自定义controller监听到这个key，我就为这个deployment创建一个cr资源，这个cr资源可以配置定时任务表达式，最终会按照这个定时任务表达式去定时的更新deployment。更新deployment这个操作可以通过修改spec.template pod的annotation来实现。 创建项目项目名称，我定为优雅的滚动更新(Elegant rolling updates)，简称ERU. 通过operator-sdk生成代码模板： 1$ operator-sdk new eru-operator operator-sdk的安装可以直接看官方文档：安装operator-sdk我这里安装的版本是0.15.2. 创建api1$ operator-sdk add api --api-version=mtsre.meitu.com/v1 --kind=Eru 创建controller1$ operator-sdk add controller --api-version=mtsre.meitu.com/v1 --kind=Eru 看看默认的代码结构 运行试试要想运行项目，我们首先要在我们的集群中部署一下我们的role,rolebinding,crd等资源。在deploy目录里有这几个文件，我们分别依次部署一下： 1234$ kubectl apply -f role.yaml $ kubectl apply -f role_binding.yaml $ kubectl apply -f service_account.yaml$ kubectl apply -f crds/mtsre.meitu.com_erus_crd.yaml 部署好之后，我们在项目的跟目录下直接运行： 1$ operator-sdk run --local --namespace your-namespace-name 这时你就可以看到输出，默认的代码是监听这个namespace下pod的变化。","categories":[{"name":"k8s","slug":"k8s","permalink":"https://blog.itmonkey.icu/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://blog.itmonkey.icu/tags/k8s/"},{"name":"operator","slug":"operator","permalink":"https://blog.itmonkey.icu/tags/operator/"}]},{"title":"pod异常无法启动的排查过程","slug":"get-some-mesg-in-k8s-event","date":"2020-12-22T02:27:17.000Z","updated":"2020-12-22T02:52:43.444Z","comments":true,"path":"2020/12/22/get-some-mesg-in-k8s-event/","link":"","permalink":"https://blog.itmonkey.icu/2020/12/22/get-some-mesg-in-k8s-event/","excerpt":"背景一大早收到业务方说自己的服务挂了，一直起不来，然后就开始了排查之路。","text":"背景一大早收到业务方说自己的服务挂了，一直起不来，然后就开始了排查之路。 现象pod无法启动，然后查看pod的event也正常，没看到什么异常信息，就卡在create pod. 操作 查看当前这个pod调度到的机器，发现基础监控异常，怀疑机器初始化有问题。 我们登录到机器上查看docker初始化日志： 1journalctl -n 100查看最新的100条日志 最终看到机器的磁盘挂载有问题。 然后临时把pod调度到其他正常的机器上，发现问题还是存在。 只能接着看event日志了， 1kubectl get events -n my-namespace 最终看到如下信息： 从日志中看到，失败是在FailedPostStartHook这一步，也就是服务的poststart脚本启动有问题。其实也可以在日志里看到更详细的报错： 1Redis::connect(): php_network_getaddresses: getaddrinfo failed: Name or service not known 字面上错误是跟redis相关，我们看了配置文件，发现redis其实正常，只不过读取了错误的redis配置文件，最终是在环境变量上发现了突破口，我们是根据环境变量来读取不同环境的配置文件，环境变量丢失了，最终读取到了dev环境的配置文件，导致服务一直无法启动。","categories":[{"name":"云运维","slug":"云运维","permalink":"https://blog.itmonkey.icu/categories/%E4%BA%91%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://blog.itmonkey.icu/tags/k8s/"}]},{"title":"go get国内加速","slug":"go-module-speedup","date":"2020-12-01T12:42:21.000Z","updated":"2020-12-01T12:48:15.698Z","comments":true,"path":"2020/12/01/go-module-speedup/","link":"","permalink":"https://blog.itmonkey.icu/2020/12/01/go-module-speedup/","excerpt":"背景go开发过程中，国内下载依赖包太慢了，怎么办？","text":"背景go开发过程中，国内下载依赖包太慢了，怎么办？ 配置 开启go module 1export GO111MODULE=on 配置代理 1export GOPROXY=https://goproxy.io 下载，可以试用go get 体验极速 额外说一句有很多都是配置阿里云的export GOPROXY=https://mirrors.aliyun.com/goproxy/，不过我发现很多包都找不到，所以干脆不用了。 全局配置你可以配置到/etc/profile或者个人用户下~/.bash_profile 12export GO111MODULE=onexport GOPROXY=https://goproxy.io","categories":[{"name":"go","slug":"go","permalink":"https://blog.itmonkey.icu/categories/go/"}],"tags":[{"name":"go","slug":"go","permalink":"https://blog.itmonkey.icu/tags/go/"}]},{"title":"例子：自定义crd","slug":"def-custom-crd","date":"2020-12-01T10:12:27.000Z","updated":"2020-12-01T12:35:02.986Z","comments":true,"path":"2020/12/01/def-custom-crd/","link":"","permalink":"https://blog.itmonkey.icu/2020/12/01/def-custom-crd/","excerpt":"写一个crd资源","text":"写一个crd资源 123456789101112131415161718192021# cat custom-res-def.yamlapiVersion: apiextensions.k8s.io/v1beta1kind: CustomResourceDefinitionmetadata: name: libin.meitu.k8s.iospec: group: meitu.k8s.io versions: - name: v1 served: true storage: true scope: Namespaced names: plural: libin singular: libin kind: Libin shortNames: - lib# kubectl apply -f custom-res-def.yamlcustomresourcedefinition.apiextensions.k8s.io/libin.meitu.k8s.io created 查看crd定义123# kubectl get crd | grep libinNAME CREATED ATlibin.meitu.k8s.io 2020-12-01T09:11:32Z 创建crd资源对象123456789101112# cat custom-res-object.yamlapiVersion: meitu.k8s.io/v1kind: Libinmetadata: name: object-libin namespace: opsspec: name: \"李彬\" func: \"SRE\"# kubectl apply -f custom-res-object.yamllibin.meitu.k8s.io/object-libin created 查看创建的对象1234# kubectl get libin -n ops# kubectl get lib -n opsNAME AGEobject-libin 1h","categories":[{"name":"k8s","slug":"k8s","permalink":"https://blog.itmonkey.icu/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://blog.itmonkey.icu/tags/k8s/"}]},{"title":"helm安装ingress-nginx开启自定义header以及关闭强制https跳转","slug":"ingress-nginx-enable-customheader-and-ssl","date":"2020-11-23T02:51:17.000Z","updated":"2020-11-23T03:11:58.267Z","comments":true,"path":"2020/11/23/ingress-nginx-enable-customheader-and-ssl/","link":"","permalink":"https://blog.itmonkey.icu/2020/11/23/ingress-nginx-enable-customheader-and-ssl/","excerpt":"介绍默认安装的ingress-nginx，是没有开启用户自定义header的，以及强制跳转https是开启的，所以我们可以选择关闭。","text":"介绍默认安装的ingress-nginx，是没有开启用户自定义header的，以及强制跳转https是开启的，所以我们可以选择关闭。 配置修改values.yaml文件，配置如下： 12345config: allow-backend-server-header: \"true\" enable-underscores-in-headers: \"true\" hsts-include-subdomains: \"false\" ssl-redirect: \"false\" 修改完成后升级ingress-nginx，查看默认ingress-controller启动配置的configmap，配置中多了一些自定义内容。 1234[root@locahost]# kubectl get configmap -n ingressNAME DATA AGEingress-controller-leader-ingress-public-eve 0 9dingress-nginx-controller 1 9d 查看ingress-nginx-controller可以获取到自定义的配置： 123456789apiVersion: v1data: allow-backend-server-header: \"true\" enable-underscores-in-headers: \"true\" hsts-include-subdomains: \"false\" ssl-redirect: \"false\"kind: ConfigMap...... 另外说一句因为上述我们关闭了全局的默认强制跳转https，如果有需要强制跳转，我们可以单独配置在ingress资源上，如下： 1234567891011121314151617181920apiVersion: extensions/v1beta1kind: Ingressmetadata: annotations: kubernetes.io/ingress.class: ingress-public nginx.ingress.kubernetes.io/force-ssl-redirect: \"true\" name: ingress-prexx-xx-com namespace: evespec: rules: - host: prexx.xx.com http: paths: - backend: serviceName: php-global-svc-pre servicePort: 80 tls: - hosts: - prexx.xx.com secretName: x-xx-com-20200728","categories":[{"name":"k8s","slug":"k8s","permalink":"https://blog.itmonkey.icu/categories/k8s/"}],"tags":[{"name":"ingress-nginx","slug":"ingress-nginx","permalink":"https://blog.itmonkey.icu/tags/ingress-nginx/"}]},{"title":"https证书cer转pem或者crt","slug":"ssl-cert-cer-to-pem-or-crt","date":"2020-11-16T05:46:14.000Z","updated":"2020-11-16T06:02:42.340Z","comments":true,"path":"2020/11/16/ssl-cert-cer-to-pem-or-crt/","link":"","permalink":"https://blog.itmonkey.icu/2020/11/16/ssl-cert-cer-to-pem-or-crt/","excerpt":"背景在对接各个cdn厂商的过程中，会遇到上传https证书的需求，不过不同的厂商导入https的姿势或者格式是不同的，证书签发商一般给的都是cer+key，所以需要我们做一下格式转换等操作。","text":"背景在对接各个cdn厂商的过程中，会遇到上传https证书的需求，不过不同的厂商导入https的姿势或者格式是不同的，证书签发商一般给的都是cer+key，所以需要我们做一下格式转换等操作。 cer转pem1$ openssl x509 -inform PEM -in xx-xx-com.cer -out xx-xx-com.pem cer转crt1$ openssl x509 -inform PEM -in xx-xx-com.cer -out xx-xx-com.crt","categories":[{"name":"基础运维","slug":"基础运维","permalink":"https://blog.itmonkey.icu/categories/%E5%9F%BA%E7%A1%80%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"运维","slug":"运维","permalink":"https://blog.itmonkey.icu/tags/%E8%BF%90%E7%BB%B4/"},{"name":"https","slug":"https","permalink":"https://blog.itmonkey.icu/tags/https/"}]},{"title":"(官方版)使用helm3在gke集群中安装ingress-nginx","slug":"helm-install-ingress-controller-in-gke","date":"2020-11-12T08:45:05.000Z","updated":"2020-11-13T10:14:12.502Z","comments":true,"path":"2020/11/12/helm-install-ingress-controller-in-gke/","link":"","permalink":"https://blog.itmonkey.icu/2020/11/12/helm-install-ingress-controller-in-gke/","excerpt":"说明前两篇文章已经讲解过nginx-ingress，nginx-ingress是由nginx官方推出的，今天讲解一下k8s社区推出的ingress-nginx。另外gke上基础的东西我就不讲了，类似节点池的创建，在之前的文章都已经写过了，不太懂的可以翻看一下。 前置注意k8s集群版本要求&gt;=1.16.0-0，所以如果没达到要求的，建议升级下~","text":"说明前两篇文章已经讲解过nginx-ingress，nginx-ingress是由nginx官方推出的，今天讲解一下k8s社区推出的ingress-nginx。另外gke上基础的东西我就不讲了，类似节点池的创建，在之前的文章都已经写过了，不太懂的可以翻看一下。 前置注意k8s集群版本要求&gt;=1.16.0-0，所以如果没达到要求的，建议升级下~ 步骤 下载最新的charts 修改配置 安装验证 下载最新的charts12$ helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx$ helm pull ingress-nginx/ingress-nginx 修改配置1234567891011121. image.repository,镜像需要梯子，注意2. ingressClass这个改一改，设置你的ingress-controller组名3. hostNetwork如果你用daemonset，请设置为true4. kind，看你用什么方式了，deployment还是daemonset5. nodeSelector，最好指定单独的节点池，这样业务和代理不混用6. healthCheckPath，设置成你想要的健康检查uri7. autoscaling，增加了hpa设置，可以看下8. service.enabled默认是true，会生成一个loadbalancer类型的svc9. internal，是否开启内部负载均衡器，对于云厂商需要设置不同的annotations，才行。对于gke来讲：`cloud.google.com/load-balancer-type: Internal`即可开启内部负载均衡器(注意：这个默认是用gke官方的ingress-controller，而不是我们本次安装的)10. admissionWebhooks还没研究过是啥，暂时关闭11. metrics开启，如果在gke上，设置type为NodePort12. publishService.enable 修改成false，不然需要你为controller创建一个默认的svc 安装验证1helm install my-ingress-nginx -n ingress-nginx . 完成上述安装之后，你可以看到会生成三个svc:一个loadbalancer外网，一个loadbalancer内网，一个metrics的svc 接下来我们创建一个ingress资源，测试下 1234567891011121314151617181920apiVersion: extensions/v1beta1kind: Ingressmetadata: annotations: nginx.ingress.kubernetes.io/enable-cors: \"true\" kubernetes.io/ingress.class: mt-ingress-eve-release #注意修改成你自己的ingressclass name: test-ingress namespace: monitoringspec: rules: - host: test.xx.com http: paths: - backend: serviceName: mt-prometheus-operator-prometheus servicePort: 9090 tls: - hosts: - test.xx.com secretName: x-xx-com-20201112 最佳实践最佳配置：(适用于生产环境) 1234567891.ingressClass，内网和公网分开，分别叫：internal-ingree-group,public-ingress-group2.hostNetwork=true3.kind=DaemonSet4.service创建设为false5.nodeSelector: nodetype=ingress6.healthCheckPath自定义7.metrics开启，port默认10254，注意防火墙规则是否打开(curl ip:10254/metrics)8.defaultBackend最好打开下9. publishService.enable 修改成false，不然需要你为controller创建一个默认的svc 安装外网的ingress-controller: 1helm install public-ingress-nginx -n ingress-nginx . --set controller.ingressClass=public-ingress-nginx 安装内网的ingress-controller: 1helm install inner-ingress-nginx -n ingress-nginx . --set controller.ingressClass=inner-ingress-nginx 安装完之后，创建lb负载均衡器，代理到ingress实例组里，跟这篇文章的最后部分一样了，不讲解了。跳转 最后你可以创建一个ingress资源测试下，看看效果。","categories":[{"name":"k8s","slug":"k8s","permalink":"https://blog.itmonkey.icu/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://blog.itmonkey.icu/tags/k8s/"},{"name":"helm","slug":"helm","permalink":"https://blog.itmonkey.icu/tags/helm/"},{"name":"ingress-nginx","slug":"ingress-nginx","permalink":"https://blog.itmonkey.icu/tags/ingress-nginx/"}]},{"title":"(deployment篇)使用helm3在gke中安装ingress-controller","slug":"helm-install-ingress-in-gke-with-deployment","date":"2020-11-12T06:27:01.000Z","updated":"2020-11-12T06:36:05.799Z","comments":true,"path":"2020/11/12/helm-install-ingress-in-gke-with-deployment/","link":"","permalink":"https://blog.itmonkey.icu/2020/11/12/helm-install-ingress-in-gke-with-deployment/","excerpt":"背景上一篇文章已经通过daemonset的方式安装部署了ingress-controller，这篇文章我们主要通过deployment方式来部署，其实基本上差不多，就一些小点需要修改下罢了。如果你没有看过上一篇文章，请先看上一篇的内容。","text":"背景上一篇文章已经通过daemonset的方式安装部署了ingress-controller，这篇文章我们主要通过deployment方式来部署，其实基本上差不多，就一些小点需要修改下罢了。如果你没有看过上一篇文章，请先看上一篇的内容。 步骤 创建gke ingress 节点池 下载最新的helm charts 修改配置文件 安装验证 验证流量 创建节点池这里就直接通过控制台创建一个ingress的节点池就好了，或者你也可以不用创建，也可以和业务混合调度，不过建议还是分开，毕竟代理更重要点。 下载charts12$ helm repo add nginx-stable https://helm.nginx.com/stable$ helm repo update 12$ helm pull nginx-stable/nginx-ingress$ tar xvf nginx-ingress-0.7.0.tgz 修改配置文件既然我们选择了deployment+loadbalancer的方式来部署，那么我们主要修改以下内容： 12345678$ vim values.yaml1. nodeSelector: (如果你有单独的节点池请修改,没有就算了) nodetype: ingress2. 如果你有自定义镜像，也可以把image.repository和image.tag修改掉，注意修改下imagePullSecrets3. 如果你有日志落盘的需求，那么可以设置volumeMounts来挂载宿主机目录到容器4. 打开健康检查healthStatus=true,healthStatusURL=\"/devops/status\"，另外自定义健康检查路径5. 修改ingressClass为自定义组：mt-ingress-eve-release(后面会说有啥用) 安装验证123456789$ helm install nginx-ingress -n ingress-controller .NAME: nginx-ingressLAST DEPLOYED: Wed Nov 11 16:26:25 2020NAMESPACE: ingress-controllerSTATUS: deployedREVISION: 1TEST SUITE: NoneNOTES:The NGINX Ingress Controller has been installed. 123456$ kubectl get deployment -n ingress-controllerNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGEnginx-ingress-nginx-ingress 1 1 1 1 1 nodetype=ingress 44m$ kubectl get pods -n ingress-controllerNAME READY STATUS RESTARTS AGEnginx-ingress-nginx-ingress-xxx8z 1/1 Running 0 40m 由于是通过deployment+svc的模式创建的，他会默认创建一个loadbalancer类型的svc，接管你现在的ingress-controller的pod，然后端口转发，80到80，443到443 验证流量拿到loadbalancer类型svc的公网ip之后，你就可以直接创建ingress资源来验证，然后绑定host测试即可。 (注意：secret要在这个ns下创建好) 1234567891011121314151617181920apiVersion: extensions/v1beta1kind: Ingressmetadata: annotations: nginx.ingress.kubernetes.io/enable-cors: \"true\" kubernetes.io/ingress.class: mt-eve-ingress-release name: ingress-xx-com namespace: monitoringspec: rules: - host: test.xx.com http: paths: - backend: serviceName: mt-prometheus-operator-prometheus servicePort: 9090 tls: - hosts: - test.xx.com secretName: x-xx-com-20201112 还是额外讲一句由于gke的机器是不允许的登录的，所以通过这种方式创建的ingress-controller，你是没有办法日志落盘的，所以搞不定。","categories":[{"name":"k8s","slug":"k8s","permalink":"https://blog.itmonkey.icu/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://blog.itmonkey.icu/tags/k8s/"},{"name":"helm","slug":"helm","permalink":"https://blog.itmonkey.icu/tags/helm/"},{"name":"ingress","slug":"ingress","permalink":"https://blog.itmonkey.icu/tags/ingress/"}]},{"title":"(daemonset篇)使用helm3在gke中安装ingress-controller","slug":"helm-install-ingress-in-gke-with-daemonset","date":"2020-11-11T08:32:38.000Z","updated":"2020-11-13T06:42:19.652Z","comments":true,"path":"2020/11/11/helm-install-ingress-in-gke-with-daemonset/","link":"","permalink":"https://blog.itmonkey.icu/2020/11/11/helm-install-ingress-in-gke-with-daemonset/","excerpt":"背景我们的业务目前在gke上是使用的自带的ingress服务，直接对接他们的loadbalancer服务，上几篇文章写的是安装prometheus，发现获取不到代理层的一些数据，所以想直接自己撸ingress-controller来实现。备注：gke上的stackdriver虽然可以看到比较多的数据，但是关于loadbalancer，他没有具体的域名数据，所以比较不直观。","text":"背景我们的业务目前在gke上是使用的自带的ingress服务，直接对接他们的loadbalancer服务，上几篇文章写的是安装prometheus，发现获取不到代理层的一些数据，所以想直接自己撸ingress-controller来实现。备注：gke上的stackdriver虽然可以看到比较多的数据，但是关于loadbalancer，他没有具体的域名数据，所以比较不直观。 描述我们使用ingress-controller的姿势其实有很多，一般有： deployment + loadbalancer模式的service daemonset + hostnetwork + nodeselector 具体可以参考这篇博客，我一个朋友写的：跳转 我分成两篇文章写吧！ 这里着重讲一句，ingress-nginx和nginx-ingress是两个不同的，一个是k8s社区推出的，一个是nginx推出的，本篇文章是按照nginx-ingress讲的，接下来会讲ingress-nginx，这个也会作为以后默认的官方使用版本 步骤 创建gke ingress 节点池 下载最新的helm charts 修改配置文件 安装验证 增加lb代理到ingress-controller机器 验证流量 创建节点池目前测试，在控制台创建ingress节点池，默认是不会打开防火墙的http和https规则，所以会影响你后续的流量访问。那么通过查看文档发现，可以直接使用gcloud来创建节点池，并指定打开http和https防火墙规则。gcloud命令参考：重要的就是--tags=http-server,https-server这个参数(其中–cluster，–zone，–node-labels，–machine-type这些你都可以自定义，具体的参数请参考：官方文档) 1gcloud container node-pools create nodepool-ingress --cluster=gke-cluster-name --zone us-central1-c --tags=http-server,https-server --num-nodes=1 --node-labels=nodetype=ingress --enable-autoscaling --max-nodes=3 --min-nodes=1 --disk-size=50 --machine-type=n2-custom-2-4096 --project projectName 下载charts12$ helm repo add nginx-stable https://helm.nginx.com/stable$ helm repo update 12$ helm pull nginx-stable/nginx-ingress$ tar xvf nginx-ingress-0.7.0.tgz 修改配置文件既然我们选择了deamonset+hostnetwork+nodeselector的方式来部署，那么我们主要修改以下内容： 1234567891011$ vim values.yaml1. kind: daemonset 默认是deployment，修改成daemonset2. hostNetwork: true 默认是false3. nodeSelector: nodetype: ingress4. 如果你有自定义镜像，也可以把image.repository和image.tag修改掉，注意修改下imagePullSecrets5. 如果你有日志落盘的需求，那么可以设置volumeMounts来挂载宿主机目录到容器6. service.create 设置为false，不然会默认创建一个类型为loadbalancer的svc7. 打开健康检查healthStatus=true,healthStatusURL=\"/devops/status\"，另外自定义健康检查路径8. 修改ingressClass为自定义组：mt-ingress-eve-release(后面会说有啥用) 安装验证123456789$ helm install nginx-ingress -n ingress-controller .NAME: nginx-ingressLAST DEPLOYED: Wed Nov 11 16:26:25 2020NAMESPACE: ingress-controllerSTATUS: deployedREVISION: 1TEST SUITE: NoneNOTES:The NGINX Ingress Controller has been installed. 由于是daemonset方式部署，所以当你扩容ingress节点池，他pod也会自动伸缩的 123456$ kubectl get daemonset -n ingress-controllerNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGEnginx-ingress-nginx-ingress 1 1 1 1 1 nodetype=ingress 44m$ kubectl get pods -n ingress-controllerNAME READY STATUS RESTARTS AGEnginx-ingress-nginx-ingress-xxx8z 1/1 Running 0 40m 通过hostnetwork的方式，就是把容器暴露的端口映射到宿主机上，容器目前是暴露80和443端口，所以你在宿主机也能看到这两个端口，由于gke的节点是不允许登录的，所以我们直接telnet验证下就行了。 另外也可以看看健康检查是否正常： 增加lb代理到ingress-controller1.创建tcp负载均衡器，然后选择后端实例组，也就是你创建nodepool的时候自动创建的实例组2.创建健康检查，就是你上面在values.yaml中定义的3.创建前端转发配置，分别是80和4434.最后查看下glb创建的结果以及健康检查的状态 验证流量拿到glb的ip之后，添加一个ingress资源，添加的ingress资源需要设置kubernetes.io/ingress.class:这个annotations，在前面已经定义了ingressClass为mt-ingress-eve-release，这个能解决你一个集群中有多个ingress-controller，可以通过这个标签来判断你的ingress资源应该被哪个ingress-controller接管。(注意：secret要在这个ns下创建好) 1234567891011121314151617181920apiVersion: extensions/v1beta1kind: Ingressmetadata: annotations: nginx.ingress.kubernetes.io/enable-cors: \"true\" kubernetes.io/ingress.class: mt-eve-ingress-release name: ingress-xx-com namespace: monitoringspec: rules: - host: test.xx.com http: paths: - backend: serviceName: mt-prometheus-operator-prometheus servicePort: 9090 tls: - hosts: - test.xx.com secretName: x-xx-com-20201112 然后本地绑定一个host先测试下是否能够访问到。通过ingress-controller的日志可以看到是没问题的： 额外讲一句由于gke的机器是不允许的登录的，所以通过这种方式创建的ingress-controller，你是没有办法日志落盘的，所以搞不定。 在额外讲一句安装好之后默认是打开了metrics的，所以你就可以创建一个servicemonitor，把数据收集到你prometheus中，如果你不懂这个是啥，请欣赏之前的文章，在gke中安装prometheus-operaotor。或者也可以配置prometheus自动收集集群中的metrics数据 可以看看他默认提供了什么信息，默认的端口是9113。(注意：需要打开防火墙规则，放开9113端口，默认是不行的) 1curl 10.10.182.28:9113/metrics","categories":[{"name":"k8s","slug":"k8s","permalink":"https://blog.itmonkey.icu/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://blog.itmonkey.icu/tags/k8s/"},{"name":"helm","slug":"helm","permalink":"https://blog.itmonkey.icu/tags/helm/"},{"name":"ingress","slug":"ingress","permalink":"https://blog.itmonkey.icu/tags/ingress/"}]},{"title":"使用prometheusrules自定义创建告警规则","slug":"prometheus-operator-prometheusrules","date":"2020-11-09T09:21:27.000Z","updated":"2020-11-11T05:06:49.586Z","comments":true,"path":"2020/11/09/prometheus-operator-prometheusrules/","link":"","permalink":"https://blog.itmonkey.icu/2020/11/09/prometheus-operator-prometheusrules/","excerpt":"介绍首先这篇文章是跟着上一篇helm 部署prometheus-operator来的，部署完成之后，我们就需要自定义一些配置。 这篇文章主要讲解如何自定义告警规则，如何让prometheus发现他。","text":"介绍首先这篇文章是跟着上一篇helm 部署prometheus-operator来的，部署完成之后，我们就需要自定义一些配置。 这篇文章主要讲解如何自定义告警规则，如何让prometheus发现他。 步骤 添加prometheusrules规则 验证 名词解释prometheusrules，也是安装好prometheus-operator后创建的一种自定义资源，我们可以看下默认自带了哪些规则： 12345678910111213141516171819202122232425262728[root@localhost]# kubectl get prometheusrules -n monitoringNAME AGEprometheus-operator-me-alertmanager.rules 2d23hprometheus-operator-me-etcd 2d23hprometheus-operator-me-general.rules 2d23hprometheus-operator-me-k8s.rules 2d23hprometheus-operator-me-kube-apiserver-availability.rules 2d23hprometheus-operator-me-kube-apiserver-slos 2d23hprometheus-operator-me-kube-apiserver.rules 2d23hprometheus-operator-me-kube-prometheus-general.rules 2d23hprometheus-operator-me-kube-prometheus-node-recording.rules 2d23hprometheus-operator-me-kube-scheduler.rules 2d23hprometheus-operator-me-kube-state-metrics 2d23hprometheus-operator-me-kubelet.rules 2d23hprometheus-operator-me-kubernetes-apps 2d23hprometheus-operator-me-kubernetes-resources 2d23hprometheus-operator-me-kubernetes-storage 2d23hprometheus-operator-me-kubernetes-system 2d23hprometheus-operator-me-kubernetes-system-apiserver 2d23hprometheus-operator-me-kubernetes-system-controller-manager 2d23hprometheus-operator-me-kubernetes-system-kubelet 2d23hprometheus-operator-me-kubernetes-system-scheduler 2d23hprometheus-operator-me-node-exporter 2d23hprometheus-operator-me-node-exporter.rules 2d23hprometheus-operator-me-node-network 2d23hprometheus-operator-me-node.rules 2d23hprometheus-operator-me-prometheus 2d23hprometheus-operator-me-prometheus-operator 2d23h 当然这些规则，你也可以在prometheus的界面上看到，具体也就是对应一个一个的rules 开始①添加prometheusrules规则创建自定义rules文件 123456789101112131415161718192021[root@localhost]# cat demo1.yamlapiVersion: monitoring.coreos.com/v1kind: PrometheusRulemetadata: labels: app: prometheus-operator release: eve-prometheus-operator name: testtalus-rules-1 namespace: lb6spec: groups: - name: testtalus.rules rules: - alert: processorNatGatewayMonitor_snat_to_hight_100 expr: processorNatGatewayMonitor_snat &gt; 100 for: 1m labels: severity: warning annotations: summary: \"nat gateway &#123;&#123; $labels.natgatewayid &#125;&#125; snat连接数过高\" description: \"nat gateway &#123;&#123; $labels.natgatewayid &#125;&#125; snat连接数大于100 (当前值：&#123;&#123; $value &#125;&#125;)\" 具体的指标不解释了，这个文档一大堆，简单说下groups.name这个，就是一个组名，然后下面有很多很多的规则，比如当前processorNatGatewayMonitor_snat_to_hight_100就是testtalus.rules这个组里面的一个指标而已。 开始创建: 12[root@localhost]# kubectl delete prometheusrules testtalus-rules-1 -n lb6prometheusrule.monitoring.coreos.com \"testtalus-rules-1\" deleted 如果你这里报错，并且报错信息如下： 12[root@localhost]# kubectl apply -f demo1.yamlError from server (InternalError): error when creating \"demo1.yaml\": Internal error occurred: failed calling webhook \"prometheusrulemutate.monitoring.coreos.com\": Post https://prometheus-operator-me-operator.meitu-monitoring.svc:443/admission-prometheusrules/mutate?timeout=30s: context deadline exceeded (Client.Timeout exceeded while awaiting headers) 那么在这里找答案：跳转 我的解决方案是：删除资源validatingwebhookconfigurations.admissionregistration.k8s.io和MutatingWebhookConfiguration，并且重新创建你的rules 1234567891011[root@localhost]# kubectl get validatingwebhookconfigurations.admissionregistration.k8s.ioNAME CREATED ATprometheus-operator-me-admission 2020-11-06T10:47:12Z[root@localhost]# kubectl get MutatingWebhookConfigurationNAME CREATED ATprometheus-operator-me-admission 2020-11-06T10:47:12Zpod-ready.config.common-webhooks.networking.gke.io 2020-02-25T13:52:06Z[root@localhost]# kubectl delete validatingwebhookconfigurations.admissionregistration.k8s.io eve-prometheus-operator-me-admissionvalidatingwebhookconfiguration.admissionregistration.k8s.io \"eve-prometheus-operator-me-admission\" deleted[root@localhost]# kubectl delete MutatingWebhookConfiguration eve-prometheus-operator-me-admissionmutatingwebhookconfiguration.admissionregistration.k8s.io \"eve-prometheus-operator-me-admission\" deleted ②验证到prometheus的rules界面，你就可以看到你自定义的规则了","categories":[{"name":"k8s","slug":"k8s","permalink":"https://blog.itmonkey.icu/categories/k8s/"}],"tags":[{"name":"prometheus","slug":"prometheus","permalink":"https://blog.itmonkey.icu/tags/prometheus/"},{"name":"prometheus-operator","slug":"prometheus-operator","permalink":"https://blog.itmonkey.icu/tags/prometheus-operator/"},{"name":"prometheusrules","slug":"prometheusrules","permalink":"https://blog.itmonkey.icu/tags/prometheusrules/"}]},{"title":"使用ServiceMonitor自定义暴露指标","slug":"prometheus-operator-servicemanager","date":"2020-11-09T06:50:18.000Z","updated":"2020-11-11T05:04:40.981Z","comments":true,"path":"2020/11/09/prometheus-operator-servicemanager/","link":"","permalink":"https://blog.itmonkey.icu/2020/11/09/prometheus-operator-servicemanager/","excerpt":"介绍首先这篇文章是跟着上一篇helm 部署prometheus-operator来的，部署完成之后，我们就需要自定义一些配置。 这篇文章主要讲解如何自定义服务发现，当我们有一个服务想要暴露数据给prometheus，我怎么操作。","text":"介绍首先这篇文章是跟着上一篇helm 部署prometheus-operator来的，部署完成之后，我们就需要自定义一些配置。 这篇文章主要讲解如何自定义服务发现，当我们有一个服务想要暴露数据给prometheus，我怎么操作。 步骤 部署你的服务，并暴露数据 添加svc 添加servicemonitor规则 验证 拓展 名词解释servicemonitor，也是安装好prometheus-operator后创建的一种自定义资源，我们可以看下默认自带了哪些规则： 123456789101112131415[root@localhost]# kubectl get servicemonitor -n monitoringNAME AGEprometheus-operator-me-alertmanager 2d22hprometheus-operator-me-apiserver 2d22hprometheus-operator-me-coredns 2d22hprometheus-operator-me-grafana 2d22hprometheus-operator-me-kube-controller-manager 2d22hprometheus-operator-me-kube-etcd 2d22hprometheus-operator-me-kube-proxy 2d22hprometheus-operator-me-kube-scheduler 2d22hprometheus-operator-me-kube-state-metrics 2d22hprometheus-operator-me-kubelet 2d22hprometheus-operator-me-node-exporter 2d22hprometheus-operator-me-operator 2d22hprometheus-operator-me-prometheus 2d22h 当然这些规则，你也可以在prometheus的界面上看到，具体也就是对应一个一个的target 开始①创建服务首先创建一个服务，并暴露metric接口，这个我直接用java写了一个demo，运行后可以访问http://localhost:9000/talus/metrics/prometheus就能看到一些数据。 服务有了，我们就部署到k8s集群中吧，写个deployment，这里不贴了，不会写的可以google。下面是我部署的服务： 123[root@svcmonitor-demo]# kubectl get pod -n lb6NAME READY STATUS RESTARTS AGEtesttalus-67c754cbcb-bcrkb 1/1 Running 0 51m ②创建svc针对你的服务，创建一个svc，内容如下： 12345678910111213apiVersion: v1kind: Servicemetadata: name: testtaulus-svc namespace: lb6 labels: smsvc: testtalus # 这个定义好你svc的标签spec: ports: - name: testtalus-port # 定义好你svc的port名称和端口 port: 9000 selector: release: testtalus # 选择合适的pod ③创建servicemonitor我想把这个servicemonitor的规则放到相应服务的namespace空间下，或者你也可以统一管理，放到monitoring里面，这个取决于你后期的维护。 123456789101112131415161718apiVersion: monitoring.coreos.com/v1kind: ServiceMonitormetadata: name: monitor-testtalus namespace: lb6 labels: release: testtalus #Prometheus所选择的标签 release: eve-prometheus-operator # 这个必须，是prometheus发现你这个规则的标签，我怎么知道是这个规则呢？ 查看最后的拓展spec: namespaceSelector: #监控的pod所在名称空间 matchNames: - lb6 selector: #选择监控endpoint的标签 matchLabels: smsvc: testtalus # 这个是刚刚svc定义的标签 endpoints: - port: testtalus-port #service中对应的端口名称 path: /talus/metrics/prometheus # service对应的路径 ④验证部署好之后，你可以看看prometheus中target是不是多了一个刚刚创建的： 另外也可以看看你服务暴露的数据是否能查询到，我自定义暴露了一个指标叫做processorNatGatewayMonitor_snat，查看可以看到对应的数据： ⑤拓展(这个还是不要改了，会出现rules无法挂载到prometheus实例中，具体问题还在排查)关于servicemonitor中，我怎么知道prometheus能识别到我这个servicemonitor资源呢，我这边定义了一个release: eve-prometheus-operator label，是从哪里来的，其实可以看prometheus的配置文件即可。 123[root@localhost]# kubectl get prometheus -n monitoringNAME VERSION REPLICAS AGEeve-prometheus-operator-me-prometheus v2.18.2 1 2d22h 关注我写上去的部分，他会告诉你prometheus发现servicemonitor的标准。 1234567[root@localhost]# kubectl get prometheus prometheus-operator-me-prometheus -n monitoring -o yaml......serviceMonitorNamespaceSelector: &#123;&#125; serviceMonitorSelector: matchLabels: release: eve-prometheus-operator 当然下一节讲解的prometheusrules也会涉及到这个，prometheus是怎么发现你自定义的告警规则呢，也是通过这个标签来的 12345678[root@localhost]# kubectl get prometheus prometheus-operator-me-prometheus -n monitoring -o yaml......ruleNamespaceSelector: &#123;&#125; ruleSelector: matchLabels: app: prometheus-operator release: eve-prometheus-operator 最后，如果你真的想修改这些标签，你觉得这个标签不够明显，想要自定义一些标签，那么针对helm安装的prometheus-operator，我们可以修改values.yaml文件: 1234567891. 修改servicemonitor选择labelserviceMonitorSelector: matchLabels: serviceMonitorLabelKey: serviceMonitorLabelValue 2. 修改prometheusmonitor选择labelruleSelector: matchLabels: RuleLabelKey: RuleLabelValue","categories":[{"name":"k8s","slug":"k8s","permalink":"https://blog.itmonkey.icu/categories/k8s/"}],"tags":[{"name":"prometheus","slug":"prometheus","permalink":"https://blog.itmonkey.icu/tags/prometheus/"},{"name":"prometheus-operator","slug":"prometheus-operator","permalink":"https://blog.itmonkey.icu/tags/prometheus-operator/"},{"name":"serviceMonitor","slug":"serviceMonitor","permalink":"https://blog.itmonkey.icu/tags/serviceMonitor/"}]},{"title":"使用helm3在gke上安装prometheus-operator","slug":"install-prometheus-operator-in-gcp","date":"2020-11-06T06:28:00.000Z","updated":"2021-03-19T08:27:48.756Z","comments":true,"path":"2020/11/06/install-prometheus-operator-in-gcp/","link":"","permalink":"https://blog.itmonkey.icu/2020/11/06/install-prometheus-operator-in-gcp/","excerpt":"简介本文主要讲解如何使用helm3在gke上安装prometheus-operator，包含持久化存储的使用。 今天会先最简单安装一下prometheus-operator，然后再一步一步优化，最终我们使用自定义Chart文件保存为私有的安装包。","text":"简介本文主要讲解如何使用helm3在gke上安装prometheus-operator，包含持久化存储的使用。 今天会先最简单安装一下prometheus-operator，然后再一步一步优化，最终我们使用自定义Chart文件保存为私有的安装包。 前提 安装并会使用helm3 步骤 安装helm3 最简安装prometheus-operator 使用ingress来暴露服务 创建自定义chart文件 ① 设置自定义ns ② 设置svc暴露方式为NodePort ③ 自定义ingress ④ 设置持久化存储 ⑤ 自定义镜像 ⑥ 修改默认grafana的用户密码 ⑦ 打包 开始1. 安装helm3非常简单，官方文档给出了几种安装方式，随便一个都行。文档 这里我采用最简单的二进制文件安装即可 12341.到这里下载适合你系统的二进制文件包https://github.com/helm/helm/releases2.tar -zxvf helm-v3.0.0-linux-amd64.tar.gz3.mv linux-amd64/helm /usr/local/bin/helm4.helm help 验证下是否安装成功 安装好之后，我们配置下helm的仓库，既然安装prometheus-operator，那么就先add一个他的仓库。 123451.helm repo add stable https://charts.helm.sh/stable2.验证仓库，helm repo list[root@localhost]# helm repo listNAME URLstable https://charts.helm.sh/stable 最简安装prometheus-operator 如果你不知道prometheus-operator是什么，以及他包含了啥，请移步这里：文档 1.查找一下我们仓库中的prometheus-operator的包 123[root@localhost]# helm search repo prometheus-operatorNAME CHART VERSION APP VERSION DESCRIPTIONstable/prometheus-operator 9.3.2 0.38.1 DEPRECATED Provides easy monitoring definitions... 2.直接安装，报错暂时忽略 123456789101112131415161718192021222324252627[root@localhost]# helm install &lt;release-name&gt; --namespace &lt;your-namespace&gt; stable/prometheus-operatorWARNING: This chart is deprecatedmanifest_sorter.go:192: info: skipping unknown hook: \"crd-install\"manifest_sorter.go:192: info: skipping unknown hook: \"crd-install\"manifest_sorter.go:192: info: skipping unknown hook: \"crd-install\"manifest_sorter.go:192: info: skipping unknown hook: \"crd-install\"manifest_sorter.go:192: info: skipping unknown hook: \"crd-install\"manifest_sorter.go:192: info: skipping unknown hook: \"crd-install\"NAME: eve-prometheus-operatorLAST DEPLOYED: Fri Nov 6 13:50:46 2020NAMESPACE: meitu-monitoringSTATUS: deployedREVISION: 1NOTES:********************** DEPRECATED ************************ stable/prometheus-operator chart is deprecated.* Further development has moved to https://github.com/prometheus-community/helm-charts* The chart has been renamed kube-prometheus-stack to more clearly reflect* that it installs the `kube-prometheus` project stack, within which Prometheus* Operator is only one component.The Prometheus Operator has been installed. Check its status by running: kubectl --namespace meitu-monitoring get pods -l \"release=eve-prometheus-operator\"Visit https://github.com/coreos/prometheus-operator for instructions on how 3.安装完成之后，你就可以在你的ns下看到资源了 123456789[root@beautyplus-bigdata-gcp pre]# kubectl get pods -n monitoringNAME READY STATUS RESTARTS AGEalertmanager-prometheus-operator-alertmanager-0 2/2 Running 0 53mprometheus-operator-grafana-b4494db55-f87mt 2/2 Running 0 53mprometheus-operator-kube-state-metrics-5fd49d67d-xhpjs 1/1 Running 0 53mprometheus-operator-operator-75d86956bb-4rg99 2/2 Running 0 53mprometheus-operator-prometheus-node-exporter-hvccz 1/1 Running 0 53mprometheus-operator-prometheus-node-exporter-l4kdx 1/1 Running 0 53mprometheus-prometheus-operator-prometheus-0 3/3 Running 0 53m 3.使用ingress来暴露服务注意：我们首先看下安装好之后的svc信息 12345678910[root@localhost]# kubectl get svc -n monitoringNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEalertmanager-operated ClusterIP None &lt;none&gt; 9093/TCP,9094/TCP,9094/UDP 56mprometheus-operator-alertmanager ClusterIP 10.220.0.220 &lt;none&gt; 9093:31261/TCP 57mprometheus-operator-grafana ClusterIP 10.220.0.54 &lt;none&gt; 80:30830/TCP 57mprometheus-operator-kube-state-metrics ClusterIP 10.220.0.249 &lt;none&gt; 8080/TCP 57mprometheus-operator-operator ClusterIP 10.220.0.178 &lt;none&gt; 8080/TCP,443/TCP 57mprometheus-operator-prometheus ClusterIP 10.220.0.55 &lt;none&gt; 9090:32218/TCP 57mprometheus-operator-prometheus-node-exporter ClusterIP 10.220.0.253 &lt;none&gt; 9100/TCP 57mprometheus-operated ClusterIP None &lt;none&gt; 9090/TCP 56m 正常来讲都是ClusterIP的方式，如果我们没有公网代理等，请直接修改ClusterIP为NodePort方式。如果你有公网代理等，直接不用修改，创建ingress资源即可。 不过，gke特殊，如果你想暴露你的服务，必须要求你的svc是NodePort方式或者是LoadBalancer方式，所以在gke上使用ingress方式来暴露服务，那么必须要先修改svc为NodePort方式 123[root@localhost]# kubectl edit svc prometheus-operator-alertmanager -n monitoring[root@localhost]# kubectl edit svc prometheus-operator-grafana -n monitoring[root@localhost]# kubectl edit svc prometheus-operator-prometheus -n monitoring 修改完上述内容，那么我们直接创建ingress资源：（注意需要提前创建下secret证书） 123456789101112131415161718192021222324252627282930[root@localhost]# cat ingress.yamlapiVersion: extensions/v1beta1kind: Ingressmetadata: annotations: nginx.ingress.kubernetes.io/enable-cors: \"true\" name: prometheus-ingress namespace: monitoringspec: rules: - host: grafana.xx.com http: paths: - backend: serviceName: prometheus-operator-grafana servicePort: 80 - host: prometheus.xx.com http: paths: - backend: serviceName: prometheus-operator-prometheus servicePort: 9090 - host: alertmanager.xx.com http: paths: - backend: serviceName: prometheus-operator-alertmanager servicePort: 9093 tls: - secretName: x-xx-com-20201106 创建完直接apply一下，应用即可: 1[root@localhost]# kubectl apply -f ingress.yaml 最后到gcp控制台观察下该ingress创建的状态，拿到公网ip，本地绑定host测试下就行。 注意：grafana默认的用户密码是admin/prom-operator 4. 创建自定义chart文件首先我们下载chart文件 123[root@localhost]# helm pull stable/prometheus-operator[root@localhost]# tar xzvf prometheus-operator-9.3.2.tgz[root@localhost]# cd prometheus-operator/ ① 设置自定义ns1修改values.yaml中的namespaceOverride字段，设置为自己的ns ② 设置svc暴露方式为NodePort12345671.修改alertmanager.service.type为NodePort2.修改prometheus.service.type为NodePort3.grafana比较特殊，需要在charts/grafana/values.yaml文件中的service下添加以下内容：service: type: NodePort port: 3000 targetPort: 3000 ③ 自定义ingress这个不建议了，因为这样默认会创建出三个公网ip，比较麻烦，最好的方式是安装好之后，单独定义一个ingress资源，去代理这三个服务：alertmanager、grafana、prometheus。 具体方法看上面说的ingress服务暴露。(注意：你前面已经吧grafana的暴露方式修改成NodePort，并且把端口也改了，注意修改ingress文件) ④ 设置持久化存储(prometheus+grafana)首先你需要了解pv，pvc，StorageClass三个概念，不然没法进行下去。 注意：其实正常来讲你不用手动创建一个sc的，因为而且在gke提供了一个默认的standard (default) ，而且在gke上，我们是直接创建pvc，不用创建pv的，因为gke的csi已经帮我们管理pv了，即云厂商的磁盘服务而已。 【prometheus持久化存储】 当我们使用helm来设置自定义存储的时候，我们只需要: 1.创建一个sc 2.并且修改values文件的配置，不用再创建pvc了。 123456789101112[root@localhost]# cat sc.yamlkind: StorageClassapiVersion: storage.k8s.io/v1metadata: name: gke-standard-sc namespace: monitoringprovisioner: kubernetes.io/gce-pdparameters: type: pd-standardvolumeBindingMode: WaitForFirstConsumer # pvc创建后不会创建pv，只有pod正常后才会创建pv，也就是具体存储reclaimPolicy: Retain # 回收策略，pvc删除后，数据不会删除[root@localhost]# kubectl apply -f sc.yaml 创建好sc就可以在gcp看到如下内容： 123456789[root@localhost]# vim values.yaml(注意是prometheus.prometheusSpec)storageSpec: volumeClaimTemplate: spec: storageClassName: gke-standard-sc accessModes: [\"ReadWriteOnce\"] resources: requests: storage: 50Gi 修改完上述内容，然后安装好prometheus后，就能看到新的pvc创建出来了。 【grafana持久化存储】 首先也是要使用上面创建的sc，并自已创建pvc 1234567891011121314[root@localhost]# cat grafana-pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata: name: gke-grafana-pvc namespace: meitu-monitoringspec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: gke-standard-sc[root@localhost]# kubectl apply -f grafana-pvc.yaml 然后修改charts/grafana/values.yaml，使用上面的pvc 123456789101112persistence: type: pvc enabled: true storageClassName: gke-standard-sc accessModes: - ReadWriteOnce size: 10Gi # annotations: &#123;&#125; finalizers: - kubernetes.io/pvc-protection # subPath: \"\" existingClaim: gke-grafana-pvc 持久化存储搞定之后，我们可以看看我们的grafana和prometheus运行在哪台机器上，然后登录到机器，就可以看到这台机器挂载了你的存储磁盘。(注意：gke的node节点是不允许登录的，所以你就在控制台看看就行) ⑤ 自定义镜像这里只需要注意，你的镜像如果是私有的，那么请设置全局的global.imagePullSecrets ⑥ 修改grafana默认用户密码注意要注释掉默认values中的grafana.adminPassword配置 123[root@localhost]# vim charts/grafana/values.yamladminUser: abcadminPassword: def ⑦ 打包121. 修改Chart.yaml中的name字段，设置为自己的name2. helm package self-define-name --debug 我们的helm仓库一般使用chartmuseum，如果想看怎么安装或者推送私有镜像到chartmuseum，请看我之前的博客：chartmuseum","categories":[{"name":"k8s","slug":"k8s","permalink":"https://blog.itmonkey.icu/categories/k8s/"}],"tags":[{"name":"gke","slug":"gke","permalink":"https://blog.itmonkey.icu/tags/gke/"},{"name":"k8s","slug":"k8s","permalink":"https://blog.itmonkey.icu/tags/k8s/"},{"name":"prometheus","slug":"prometheus","permalink":"https://blog.itmonkey.icu/tags/prometheus/"},{"name":"prometheus-operator","slug":"prometheus-operator","permalink":"https://blog.itmonkey.icu/tags/prometheus-operator/"}]},{"title":"helm仓库chartmuseum的部署","slug":"helm-repo-chartmuseum-deploy","date":"2020-10-20T10:37:41.000Z","updated":"2020-11-11T05:05:43.135Z","comments":true,"path":"2020/10/20/helm-repo-chartmuseum-deploy/","link":"","permalink":"https://blog.itmonkey.icu/2020/10/20/helm-repo-chartmuseum-deploy/","excerpt":"了解chartmuseum用过helm的都知道，chartmuseum作为helm重要的仓库，掌握他的私有部署是必不可少的。 官方文档：跳转","text":"了解chartmuseum用过helm的都知道，chartmuseum作为helm重要的仓库，掌握他的私有部署是必不可少的。 官方文档：跳转 部署方式chartmuseum的部署方式有两种，从官方首页就能看出来：一种是二进制，一种是docker。 二进制部署方式1.下载二进制文件 123curl -LO https://s3.amazonaws.com/chartmuseum/release/latest/bin/linux/amd64/chartmuseumchmod 777 chartmuseummv chartmuseum /usr/local/bin 2.创建启动文件 123456789101112131415[root@meitu]# cat /etc/systemd/system/chartmuseum.service[Unit]Description=chartmuseumRequires=network-online.targetAfter=network-online.target[Service]EnvironmentFile=/etc/chartmuseum/chartmuseum.configUser=rootRestart=allwaysExecStart=/usr/local/bin/chartmuseum $ARGSExecStop=/usr/local/bin/chartmuseum step-down[Install]WantedBy=multi-user.target 3.创建配置文件 这里修改对应的配置： 端口 local，表示本地存储 dir，就是本地存储的路径 以及一些基本的认证信息 注：存储可以有多种方式，比如aws的s3，阿里云的oss等，具体可以查看文档：github12[root@meitu]# cat /etc/chartmuseum/chartmuseum.configARGS=--port=88 --storage=\"local\" --storage-local-rootdir=\"/www/chartmuseum/\" --log-json --basic-auth-user=meitu --basic-auth-pass=\"********\" 4.启动 123systemctl start chartmuseumsystemctl status chartmuseumsystemctl enable chartmuseum 5.然后验证是否可以使用该仓库 12➜ helm repo add my-chartmuseum-repo http://my-chartmuseum-repo-url --username meitu --password ********\"my-chartmuseum-repo\" has been added to your repositories docker部署方式1.下载最近的docker镜像 1➜ docker pull chartmuseum/chartmuseum:latest 2.创建本地目录，当做存储位置 1➜ mkdir ./chartmuseum 3.启动服务 1➜ docker run --rm -it -p 8081:8080 -v $(pwd)/chartmuseum:/charts -e DEBUG=true -e STORAGE=local -e STORAGE_LOCAL_ROOTDIR=/charts chartmuseum/chartmuseum:latest 测试使用1.首先你本地helm要装好helm-push插件 2.加入repo 12➜ helm repo add my-chartmuseum-repo http://my-chartmuseum-repo-url --username meitu --password ********\"my-chartmuseum-repo\" has been added to your repositories 3.push包到仓库 123➜ helm push natmonitor-0.1.1.tgz my-chartmuseum-repoPushing natmonitor-0.1.1.tgz to my-chartmuseum-repo...Done.","categories":[{"name":"k8s","slug":"k8s","permalink":"https://blog.itmonkey.icu/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://blog.itmonkey.icu/tags/k8s/"},{"name":"helm","slug":"helm","permalink":"https://blog.itmonkey.icu/tags/helm/"},{"name":"chartmuseum","slug":"chartmuseum","permalink":"https://blog.itmonkey.icu/tags/chartmuseum/"}]},{"title":"使用vpn隧道模式打通gcp和华为云内网","slug":"create-vpn-tunnel-in-gcp-hwcloud","date":"2020-10-10T10:16:45.000Z","updated":"2020-11-11T05:04:48.791Z","comments":true,"path":"2020/10/10/create-vpn-tunnel-in-gcp-hwcloud/","link":"","permalink":"https://blog.itmonkey.icu/2020/10/10/create-vpn-tunnel-in-gcp-hwcloud/","excerpt":"背景公司某业务目前有海外和国内服务，需求是国内数据需要实时同步到海外，DBA同事使用mysql主从同步的方式进行，所以建议打通海外和国内网络。","text":"背景公司某业务目前有海外和国内服务，需求是国内数据需要实时同步到海外，DBA同事使用mysql主从同步的方式进行，所以建议打通海外和国内网络。 分析目前实现这个背景有两种方式： 专线 vpn隧道 由于目前业务量级非常小，数据量不大，所以使用专线方式难免有点贵，所以我们采用vpn隧道的方式打通两方内网。 技术实现想要实现vpn隧道，目前我们使用云服务商的话，可以直接使用云服务商提供的vpn服务。 123GCP：混合连接-vpn华为云：云连接-虚拟专用网络 12vpn隧道其实比较简单，你就认为是A和B都有一台公网机器，通过一定协议实现了互联，那么这台机器最终充当的一个角色就是网关路由，当你A地的一台机器访问B地的一台机器，都是需要先通过网关的。注意：流量都是走公网的，不过传输过程是加密的。 画了一个简单的架构图，大致是这个样子吧： 具体步骤 创建gcp网关 创建华为云网关 创建gcp隧道 创建华为云隧道 检查防火墙、安全组、acl等限制 测试网络联通性 1.创建gcp网关首先学习下红框中的3个概念： 1231. Cloud vpn隧道---真实隧道，连接AB两地网关2. Cloud vpn网关---GCP端网关3. 对等vpn网关---对方侧网关 注：由于华为云没有提供高可用类型网关(就是多个ip，防止被封)，所以不必创建对等vpn网关。 创建传统的vpn网关，可以先不用创建隧道： 2.创建华为云网关几个地方需要注意和填写下： 12341. 本端子网---就是你要跟gcp网络互通的网段2. 远端网关---就是你刚刚创建的gcp的网关ip地址3. 远端子网---gcp的网段4. 秘钥---填写一个密码，到时候创建隧道的时候会用到，通过这个来认证建立连接 3.创建gcp隧道 4.创建华为云隧道其实第二步的时候已经顺便创建隧道了，可以直接去看看状态 5.检查状态创建好gcp的隧道之后，他们就会默认去尝试建立连接，可以观察gcp和华为云上隧道的状态 6.检查防火墙、安全组、acl等限制检查两边网段的防火墙规则和安全组是否放行了，不论你用ping还是telnet。 默认创建完隧道之后，都会生成一条新的路由，就是到对端网段的数据包都走隧道过，保证数据能够正常流通。 注：gcp上mysql是专用的一个网段，这个网段是自动生成的，当你创建隧道新生成了一条路由，他是不会被自动发现的，需要手动去更新mysql的网段路由发现，勾选一下下图即可。 7.测试网络联通性在gcp和华为云上各找一台机器测试(注意：找的机器一定是要在你创建隧道的网段内)","categories":[{"name":"云运维","slug":"云运维","permalink":"https://blog.itmonkey.icu/categories/%E4%BA%91%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"gcp","slug":"gcp","permalink":"https://blog.itmonkey.icu/tags/gcp/"},{"name":"vpn-tunnel","slug":"vpn-tunnel","permalink":"https://blog.itmonkey.icu/tags/vpn-tunnel/"}]},{"title":"用java简单实现一个prometheus exporter","slug":"java-prometheus-exporter-demo","date":"2020-09-23T07:05:53.000Z","updated":"2020-11-11T05:04:32.637Z","comments":true,"path":"2020/09/23/java-prometheus-exporter-demo/","link":"","permalink":"https://blog.itmonkey.icu/2020/09/23/java-prometheus-exporter-demo/","excerpt":"介绍塔罗斯(Talus) 塔罗斯（希腊语：Ταλως；英语：Talos / Talus）是希腊神话中的机械巨人（Automaton）。该名义为“砍伐”或“太阳”。塔罗斯的身世有三种版本。按赫西奥德（Hesiod）的说法，宙斯曾创造了四代人类：黄金、白银、青铜、黑铁。","text":"介绍塔罗斯(Talus) 塔罗斯（希腊语：Ταλως；英语：Talos / Talus）是希腊神话中的机械巨人（Automaton）。该名义为“砍伐”或“太阳”。塔罗斯的身世有三种版本。按赫西奥德（Hesiod）的说法，宙斯曾创造了四代人类：黄金、白银、青铜、黑铁。 功能后期主要提供给thanos运维数据，提供prometheus接口 git地址1https://github.com/gsgs-libin/talus 接口输出接口：/talus/metrics/prometheus 访问地址script1http://localhost:9000/talus/metrics/prometheus 构建script12mvn clean package -Dmaven.test.skip=truedocker build -t harbor.xxx.com/talus/talus:release-1.1.1 . 例子我这边写了几个例子，分别对应的指标类型为：Counter、Guage、Histogram、Summary，对应的指标定义在：MetricConfig.java中可以看到 指标说明： 指标 指标说明 Counter Counter类型代表一种样本数据单调递增的指标，即只增不减，除非监控系统发生了重置 Guage Guage类型代表一种样本数据可以任意变化的指标，即可增可减 Histogram Histogram 由bucket{le=””}，bucket{le=”+Inf”},sum，count 组成，主要用于表示一段时间范围内对数据进行采样（通常是请求持续时间或响应大小），并能够对其指定区间以及总数进行统计，通常它采集的数据展示为直方图 Summary Summary 和 Histogram 类似，由{quantile=”&lt;φ&gt;”}，sum，count 组成，主要用于表示一段时间内数据采样结果（通常是请求持续时间或响应大小），它直接存储了 quantile 数据，而不是根据统计区间计算出来的。 我简单写了两个例子去实现counter和guage指标： 指标名称 类名称 Counter NginxIngressRequest Guage NatGatewayMonitorSnat 查看exportor的结果可以看到如下内容： 123# HELP processorNatGatewayMonitor_snat processorNatGatewayMonitorSnat：record by gatewayid and nodeip# TYPE processorNatGatewayMonitor_snat gaugeprocessorNatGatewayMonitor_snat&#123;natgatewayid=\"id-123145\",nodeip=\"192.168.1.1\",&#125; 123.0 123# HELP processorNginxIngress_request processorNginxIngressRequest : domain# TYPE processorNginxIngress_request counterprocessorNginxIngress_request&#123;ingressname=\"ingress1\",domain=\"ingress.dgsfor.com\",&#125; 7.0","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://blog.itmonkey.icu/categories/DevOps/"}],"tags":[{"name":"prometheus","slug":"prometheus","permalink":"https://blog.itmonkey.icu/tags/prometheus/"},{"name":"java","slug":"java","permalink":"https://blog.itmonkey.icu/tags/java/"},{"name":"cloudnative","slug":"cloudnative","permalink":"https://blog.itmonkey.icu/tags/cloudnative/"}]},{"title":"如何使用prometheus监控nginx","slug":"nginx-with-prometheus","date":"2020-09-18T02:54:45.000Z","updated":"2020-11-11T05:05:50.862Z","comments":true,"path":"2020/09/18/nginx-with-prometheus/","link":"","permalink":"https://blog.itmonkey.icu/2020/09/18/nginx-with-prometheus/","excerpt":"[toc] 需求场景使用prometheus监控nginx，图表展示使用grafana。","text":"[toc] 需求场景使用prometheus监控nginx，图表展示使用grafana。 分析nginx我们使用tengine(https://github.com/alibaba/tengine) nginx prometheus模块使用nginx-module-vts(https://github.com/vozlt/nginx-module-vts) exportor使用nginx-vts-exporter(https://github.com/hnlq715/nginx-vts-exporter) (其实这个exportor也可以不用，因为默认的vts已经提供了prometheus的metrics地址，这个exportor只是为了生成这个地址) nginx以及插件部署下载文件1234567git clone https://github.com/alibaba/tenginegit clone https://github.com/vozlt/nginx-module-vtsyum -y install zlib* library*yum -y install openssl openssl-devel---------[root@10-17-41-57 software]# lsnginx-module-vts tengine 构建tengine，带上vts12345[root@10-17-41-57 software]# cd tengine[root@10-17-41-57 tengine]# ./configure --add-module=/root/software/nginx-module-vts --prefix=/usr/local/nginx --user=nginx --group=nginx --with-stream --with-http_ssl_module --with-http_stub_status_module --with-http_realip_module --with-http_v2_module --with-http_ssl_module[root@10-17-41-57 tengine]# make[root@10-17-41-57 tengine]# make install[root@10-17-41-57 tengine]# useradd www 配置nginxnginx.conf中的http块中加入下面代码： 123456# 开启基础监控vhost_traffic_status_zone;# 开启状态码过滤vhost_traffic_status_filter on;# 开启此功能，在Nginx配置有多个server_name的情况下，会根据不同的server_name进行流量的统计，否则默认会把流量全部计算到第一个server_name上vhost_traffic_status_filter_by_host on; 新增nginx-vts-status.conf文件： 123456789server &#123; listen 3389; location /mt_status &#123; vhost_traffic_status_display; vhost_traffic_status_display_format html; &#125;&#125; 启动nginx： 12[root@10-17-41-57 software]# /usr/local/nginx/sbin/nginx -t[root@10-17-41-57 software]# /usr/local/nginx/sbin/nginx 查看原始nginx输出数据1234567891011121314[root@10-17-41-57 software]# curl https://localhost:3389/mt_status/format/prometheus# HELP nginx_vts_start_time_seconds Nginx start time# TYPE nginx_vts_start_time_seconds gaugenginx_vts_start_time_seconds 1600401217.372# HELP nginx_vts_main_connections Nginx connections# TYPE nginx_vts_main_connections gaugenginx_vts_main_connections&#123;status=\"accepted\"&#125; 201nginx_vts_main_connections&#123;status=\"active\"&#125; 98nginx_vts_main_connections&#123;status=\"handled\"&#125; 201nginx_vts_main_connections&#123;status=\"reading\"&#125; 0nginx_vts_main_connections&#123;status=\"requests\"&#125; 10nginx_vts_main_connections&#123;status=\"waiting\"&#125; 97nginx_vts_main_connections&#123;status=\"writing\"&#125; 1 加一个测试的nginx配置文件123456789101112131415server &#123; listen 80; server_name mtlab-nginx-test.meitu.com; proxy_ignore_client_abort on; # 开启详细状态码统计 vhost_traffic_status_filter_by_set_key $status $server_name; # 开启uri统计 vhost_traffic_status_filter_by_set_key $uri uris::$server_name; location / &#123; return 200; &#125;&#125; 配置好上述的参数，我们可以在输出看到一些更加详细的信息 备注(nginx.conf文件)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104user www www;worker_processes auto;error_log /usr/local/nginx/logs/nginx_error.log error;pid /usr/local/nginx/nginx.pid;worker_rlimit_nofile 65535;events&#123; use epoll; worker_connections 65535;&#125;http &#123; include mime.types; default_type application/octet-stream; vhost_traffic_status_zone; log_format api '$time_iso8601 $remote_addr - $host \"$request\" $status - $request_time $http_x_real_ip \"$http_x_forwarded_for\" $content_length $request_length $sent_http_content_length $body_bytes_sent $http_cdn \"$http_referer\" \"$http_user_agent\" $upstream_addr $upstream_status $upstream_response_time $sent_http_request_id'; log_format json '$time_iso8601 $remote_addr - $host \"$request\" $status - $request_time $http_x_real_ip \"$http_x_forwarded_for\" $content_length $request_length $sent_http_content_length $body_bytes_sent $http_cdn \"$http_referer\" \"$http_user_agent\" $upstream_addr $upstream_status $upstream_response_time $sent_http_request_id'; log_format mtlog '$time_iso8601 $remote_addr $server_addr $host \"$request\" $status - $request_time $http_x_real_ip \"$http_x_forwarded_for\" $content_length $request_length $sent_http_content_length $body_bytes_sent $http_cdn \"$http_referer\" \"$http_user_agent\" $upstream_addr $upstream_status $upstream_response_time $sent_http_request_id $http_request_id'; log_format toamtlog '$time_iso8601 $server_addr $host \"$request\" $status - $request_time $http_x_real_ip \"$http_x_forwarded_for\" $content_length $request_length $sent_http_content_length $body_bytes_sent $http_cdn \"$http_referer\" \"$http_user_agent\" $upstream_addr $upstream_status $upstream_response_time $sent_http_request_id $http_request_id'; log_format mtlog_noarg '$time_iso8601 $remote_addr $server_addr $host \"$request_method $server_protocol\" $status - $request_time $http_x_real_ip \"$http_x_forwarded_for\" $content_length $request_length $sent_http_content_length $body_bytes_sent $http_cdn \"$http_referer\" \"$http_user_agent\" $upstream_addr $upstream_status $upstream_response_time $sent_http_request_id $http_request_id'; log_format post '$time_iso8601 $remote_addr $server_addr $host \"$request\" $status - $request_time $http_x_real_ip \"$http_x_forwarded_for\" $content_length $request_length $sent_http_content_length $body_bytes_sent $http_cdn \"$http_referer\" \"$http_user_agent\" $upstream_addr $upstream_status $upstream_response_time $sent_http_request_id $request_body'; server_tokens off;# charset gb2312; server_names_hash_bucket_size 256; client_header_buffer_size 32k; large_client_header_buffers 4 32k; client_max_body_size 100m; client_body_buffer_size 10m; sendfile on; tcp_nopush off; keepalive_timeout 600; keepalive_requests 10000; tcp_nodelay on; #proxy_connect_timeout 20s; #proxy_send_timeout 90s; #proxy_read_timeout 90s; proxy_connect_timeout 600s; proxy_send_timeout 600s; proxy_read_timeout 600s; proxy_buffer_size 64k; proxy_buffers 4 64k; proxy_busy_buffers_size 128k; proxy_temp_file_write_size 128k;# proxy_buffering off;# proxy_request_buffering off; gzip on; gzip_min_length 1k; gzip_buffers 4 16k; gzip_http_version 1.0; gzip_comp_level 4; gzip_types text/plain application/x-javascript text/css application/xml text/xml application/json application/javascript; gzip_vary on; ssi on; ssi_silent_errors on; ssi_types text/shtml; ssi_last_modified on; underscores_in_headers on; # The following is a sneaky way to do \"set $the_real_ip $remote_addr\" # Needed because using set is not allowed outside server blocks. # We can't use $proxy_add_x_forwarded_for because the realip module # replaces the remote_addr too soon map $http_x_forwarded_for $full_x_forwarded_for &#123; default \"$http_x_forwarded_for, $remote_addr\"; &#125; # ws.live.meitu.com map $http_upgrade $connection_upgrade &#123; default upgrade; '' close; &#125; include upstream/*.conf; include vhosts/*.conf; limit_req_status 403; limit_req_zone $remote_addr zone=mpstatplayvideo:100m rate=20r/s;&#125; 额外补充如果你的nginx机器是多台，或者是动态的，那么这个是不能使用域名负载均衡来代理的，因为prometheus每次只能抓一条，即每次都只会获取一台机器的数据。 所以针对这个情况，两种方式： 1.如果你是容器化部署，那么他本身就有服务发现的机制，无所谓。 2.如果你是物理部署的，要嘛你写死prometheus的target配置，要嘛研究一下prometheus动态发现的原理。","categories":[{"name":"基础运维","slug":"基础运维","permalink":"https://blog.itmonkey.icu/categories/%E5%9F%BA%E7%A1%80%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"prometheus","slug":"prometheus","permalink":"https://blog.itmonkey.icu/tags/prometheus/"},{"name":"nginx","slug":"nginx","permalink":"https://blog.itmonkey.icu/tags/nginx/"}]},{"title":"k8s亲和和反亲和相关","slug":"k8s-AntiAffinity-and-Affinity","date":"2020-09-17T02:40:04.000Z","updated":"2020-11-11T05:04:55.676Z","comments":true,"path":"2020/09/17/k8s-AntiAffinity-and-Affinity/","link":"","permalink":"https://blog.itmonkey.icu/2020/09/17/k8s-AntiAffinity-and-Affinity/","excerpt":"背景最近在测试亲和和反亲和，发现很多知识都遗忘了，准备重新捡起来看看。","text":"背景最近在测试亲和和反亲和，发现很多知识都遗忘了，准备重新捡起来看看。 Pod和Node从pod出发，可以分成亲和性和反亲和性，分别对应podAffinity和podAntiAffinity。 从node出发，也可以分成亲和性和反亲和性，分别对应nodeAffinity和nodeAntiAffinity。 从操作指令来讲，可以有In、NotIn、Exists、DoesNotExist等等。 那么针对亲和性来讲，in代表我要调度到有这个标签的位置。 那么针对反亲和性来讲，in代表我不要调度到有这个标签的位置。 结论：反亲和和亲和的in和notin操作是完全相反的，注意一下就行了。 硬亲和和软亲和preferredDuringSchedulingIgnoredDuringExecution 软亲和 requiredDuringSchedulingIgnoredDuringExecution 硬亲和 不必过多讲解，反正就是必须和尽量的关系。 代码样例下面代表我的pod不调度到pod拥有release=opsnatmonitor-v1标签的node上. 12345678910111213spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: release operator: In values: - opsnatmonitor-v1 topologyKey: kubernetes.io/hostname 下面代表我的pod要调度到pod拥有release=opsnatmonitor-v1标签的node上. 12345678910111213spec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: release operator: In values: - opsnatmonitor-v1 topologyKey: kubernetes.io/hostname 最后补充场景： pod有3个，node有2个，分别测试硬亲和和软亲和结果。 结果： 硬亲和会分别调度到两个node上，并有一个pod是pending的。 软亲和会按照pod个数为1:2分配到两个node上。","categories":[{"name":"运维进阶","slug":"运维进阶","permalink":"https://blog.itmonkey.icu/categories/%E8%BF%90%E7%BB%B4%E8%BF%9B%E9%98%B6/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://blog.itmonkey.icu/tags/k8s/"}]},{"title":"不使用gcloud认证gke集群","slug":"get-kubeconfig-without-gcloud-in-gke","date":"2020-07-23T06:41:02.000Z","updated":"2020-11-11T05:05:12.143Z","comments":true,"path":"2020/07/23/get-kubeconfig-without-gcloud-in-gke/","link":"","permalink":"https://blog.itmonkey.icu/2020/07/23/get-kubeconfig-without-gcloud-in-gke/","excerpt":"背景正常我们都是通过gcloud切换集群，然后才能正常使用kubectl，这篇文章主要教你如何”不使用”gcloud来进行gke集群的认证。这里的不使用，意思是只要使用一次即可。","text":"背景正常我们都是通过gcloud切换集群，然后才能正常使用kubectl，这篇文章主要教你如何”不使用”gcloud来进行gke集群的认证。这里的不使用，意思是只要使用一次即可。 准备首先需要一台机器，已经装好gcloud并认证，初始化可参考官方文档：点击我 开始1.创建环境变量替换下面命令中的GKE_CLUSTER_NAME、ZONE、PROJECT_NAME，运行结果如下图 1GET_CMD=\"gcloud container clusters describe GKE_CLUSTER_NAME --zone=ZONE --project PROJECT_NAME\" 2.创建kubeconfig文件命令中的my-cluster自行替换 123456789101112cat &gt; kubeconfig.yaml &lt;&lt;EOFapiVersion: v1kind: Configcurrent-context: my-clustercontexts: [&#123;name: my-cluster, context: &#123;cluster: cluster-1, user: user-1&#125;&#125;]users: [&#123;name: user-1, user: &#123;auth-provider: &#123;name: gcp&#125;&#125;&#125;]clusters:- name: cluster-1 cluster: server: \"https://$(eval \"$GET_CMD --format='value(endpoint)'\")\" certificate-authority-data: \"$(eval \"$GET_CMD --format='value(masterAuth.clusterCaCertificate)'\")\"EOF 3.得到文件最终你会得到文件kubeconfig.yaml 使用我们在上述得到了kubeconfig.yaml文件，那么我们还需要一个文件，就是gcp的iam文件，你需要创建一个serviceaccount，然后得到他的json认证文件。 最终我们需要有两个文件： 12kubeconfig.yaml k8s的认证文件serviceaccount.json gcp的iam文件 接下来就是需要创建环境变量： 12export GOOGLE_APPLICATION_CREDENTIALS=serviceaccount.jsonexport KUBECONFIG=kubeconfig.yaml 最后，直接执行kubectl命令就可以了： 1234[root@localhost]# kubectl get nodesNAME STATUS ROLES AGE VERSIONgke-eve-gke-nodepool-111 Ready &lt;none&gt; 110d v1.14.10-gke.27gke-eve-gke-nodepool-222 Ready &lt;none&gt; 110d v1.14.10-gke.27 备注 后续你可以为你的每一个集群都创建一个kubeconfig文件和serviceaccount文件，那么越来越多的集群将如何管理呢，我之前简单写了一个shell脚本来管理，地址：kubeSwitch 注意gke集群apiserver是有白名单的，所以需要加上出口地址，不然获取不到数据。","categories":[{"name":"多云专辑","slug":"多云专辑","permalink":"https://blog.itmonkey.icu/categories/%E5%A4%9A%E4%BA%91%E4%B8%93%E8%BE%91/"}],"tags":[{"name":"gcp","slug":"gcp","permalink":"https://blog.itmonkey.icu/tags/gcp/"},{"name":"gke","slug":"gke","permalink":"https://blog.itmonkey.icu/tags/gke/"},{"name":"k8s","slug":"k8s","permalink":"https://blog.itmonkey.icu/tags/k8s/"}]},{"title":"云运维-简单认识各个云常用云服务","slug":"sre-cloud-base-1","date":"2020-06-23T09:57:42.000Z","updated":"2020-11-11T05:05:32.814Z","comments":true,"path":"2020/06/23/sre-cloud-base-1/","link":"","permalink":"https://blog.itmonkey.icu/2020/06/23/sre-cloud-base-1/","excerpt":"说明 再多的讲解不如后面的实战课程，概念性的东西本身有点枯燥无味，这篇文章旨在让大家对我们使用的云服务有一个大概的了解，后续补个ppt 在大家了解华为云之后，其实这篇文章相对来说就没这么重要的，我就简单列举下常用的，另外说几个其他云独有的服务。","text":"说明 再多的讲解不如后面的实战课程，概念性的东西本身有点枯燥无味，这篇文章旨在让大家对我们使用的云服务有一个大概的了解，后续补个ppt 在大家了解华为云之后，其实这篇文章相对来说就没这么重要的，我就简单列举下常用的，另外说几个其他云独有的服务。 华为云/阿里云下面这些都是最基本的服务，搭建一套完整的可用生产环境就需要这些，其他可以按需或者按云使用。 云服务名称 作用 备注 ECS 弹性云服务器 云基本服务，常用操作：按需初始化、镜像定制、统一登录 RDS Mysql redis redis DSN 云解析 常用内网dns域，其他一般没用 ELB/SLB 负载均衡器 公网，内网，tcp，http VPC 虚拟网络 注意子网划分，nat网关是作用在子网上的 NAT nat网关 云服务访问外部必经之路，建议最早部署 安全组 安全组 AS/ESS 弹性伸缩 虚机部署的弹性伸缩服务，云服务最大特点之一 OBS/OSS 对象存储 CCE/CS 容器服务 创建k8s集群 SWR 容器镜像服务 类似harbor 【案例讲解】阿里云经验分享可以查看之前的博客，有分享过：https://blog.itmonkey.icu/2020/06/17/meitu-aliyun-use-experience/ AWS上面华为云的服务，他该有的都有，下面列举之前用的的特殊的 云服务名称 作用 备注 ECS 容器服务 aws自研的容器服务 EKS 容器服务 aws自研的容器服务(k8s版) ES ES集群 集日志存储、日志展示一体的，ES+Kibana Route53 dns解析 对应华为云上的dns服务 S3 对象存储 对应华为云的OBS，对应阿里云的oss Lambda 无服务计算 简单解释就是：你写好代码，aws帮你运行，常用作触发服务 Cloudwatch 云监控 在这里，你可以看到所有的监控信息，监控指标，自定义告警，自定义监控报表 CloudFormation 云服务编排 他是一个编排工具帮你组合使用各个云产品，最终形态为一个yaml文件，定义了几个云组件之间的关系 Kinesis 数据流处理 集数据采集，处理，分析一体的服务 IAM 权限控制 用户、组、角色都是在这里定义 SNS 消息收发服务 可以收发相应的告警数据等 AWS Cost Explorer 成本分析 成本分析工具 【案例讲解】EC2 + ECS/EKS + ES + kinesis + lambda从下图中可以简单认识几个组件： 组件名称 组件作用 EC2 就是云服务器 ECS/EKS 容器服务 Kinesis 数据传输流中转站，接收云服务提发来的日志数据，同时push到es集群中 cloudwatch ecs容器服务的日志默认会投放到cloudwath中，从这里就可以看出cloudwatch其实是aws上集日志、告警未一体的服务 lambda 当cloudwatch新增一条日志，就会触发一次自定义的lambda函数，这个函数可以自定义，比如我判断这条日志是200的accesslog，那么我就不投放到es，这样lambda就实现了投放和过滤的功能 es集群 就是es集群，不过是托管于aws的，顺便提供了kibana可视化界面给你 后面会有详细教程，请关注：云运维–aws如何处理日志(物理日志+容器日志)kinesis+lambda 【案例讲解】CloudFormation使用从下图中可以简单认识几个组件： 组件名称 组件作用 AutoScaling 弹性伸缩组，类似华为云的AS，阿里云的ESS Load Balancer 负载均衡器，类似华为云ELB，阿里云的slb，GCP的GLB CloudFormation 简单认为他是一个编排工具，他把ec2、autoscaling、lb组合起来，我创建一个CloudFormation服务，就相当于创建了三个组合服务 那么CloudFormation有啥作用呢？ 快速编排服务，有一个做图界面，你可以任意组合你的云服务(下图)，最终会生成一个yaml文件，后续拿着yaml文件就可以快速创建一个新的cf服务 快速更新，比如我服务代码更新了，我创建一个新的镜像，我只需要去cloudformation中填写上新的镜像id，然后他就会自动帮我更新所有的ec2实例 【案例讲解】SNS消息服务几个概念需要先熟悉下：主题：类似消息模板，比如我定义一个cpu报警主题订阅：创建一个订阅，他里面定义了是采用什么方式发送这个主题消息，比如邮件，短信等。比如当创建好邮件订阅之后，你就会收到一条消息，点击之后你以后就能收到这个主题的消息了。 用途 sns常常配合cloudwatch进行监控告警信息的发送。 当然也有其他用途，类似下图： GCP使用的也就下面这些，其他的我这边暂时没接触到，其实google真正厉害的感觉并不是下面的服务，也没用到，所以也说不出个所以然。 云服务名称 作用 备注 Compute engine 虚拟机实例 云服务器 kubernetes engine 容器集群 gke Storage 对象存储 logging 日志中心 stackdriver 监控中心 可以把logging中的日志在这里做成图表 网络服务 网络服务 包含负载均衡器，nat，dns，cdn等，起初我是没找到负载均衡器也在这里面 bigquery 云数据仓库 【案例讲解】最简单的生产环境实现方案 创建gke集群 底层compute engine作为node承载 创建服务、ingress等 在logging中可以查看到具体的k8s原生日志，业务日志等 stackdriver中根据loggging的指标作图 访问外网需求，增加nat","categories":[{"name":"多云专辑","slug":"多云专辑","permalink":"https://blog.itmonkey.icu/categories/%E5%A4%9A%E4%BA%91%E4%B8%93%E8%BE%91/"}],"tags":[{"name":"云运维","slug":"云运维","permalink":"https://blog.itmonkey.icu/tags/%E4%BA%91%E8%BF%90%E7%BB%B4/"}]},{"title":"云运维-开篇随便聊聊自己","slug":"sre-cloud-start","date":"2020-06-22T08:48:27.000Z","updated":"2020-11-11T05:06:01.007Z","comments":true,"path":"2020/06/22/sre-cloud-start/","link":"","permalink":"https://blog.itmonkey.icu/2020/06/22/sre-cloud-start/","excerpt":"背景写这个系列的目的： 内部建设，sre组内之间分工明确，不过知识互通和共享目前暂时比较欠缺，通过这个把自己的沉淀分享出去。 个人总结，其实早就想要总结一下工作的这几年，不过一直没有动手，趁着这个机会，搞一把。 查漏补缺，每次写文档和总结的时候都是一个非常好的学习机会。","text":"背景写这个系列的目的： 内部建设，sre组内之间分工明确，不过知识互通和共享目前暂时比较欠缺，通过这个把自己的沉淀分享出去。 个人总结，其实早就想要总结一下工作的这几年，不过一直没有动手，趁着这个机会，搞一把。 查漏补缺，每次写文档和总结的时候都是一个非常好的学习机会。 随便说点回忆过去17年7月毕业，然后进入了第一家公司，做酒类电商平台的。凭着在大三跟室友一起学了一点linux，会个rm -rf命令，就萌萌东东来到了公司，其实当时算是啥都不会，对运维没有概念，也不懂什么是nginx，不懂什么是jenkins，不会写python…就这样过了在公司慢慢学了半年多，才逐步上道了~ 想想当时跟着部门同学学了非常多， zk、dubbo、codis、jenkins、docker、nginx等等 看着别人的项目学了Django以及Django REST framework，自己写了几个小项目用着，记得写了一个入职流程以及账号开通的项目 知道啥是发版，每次要等到凌晨才能上线 接触阿里云，天天在钉钉群里跟他们打交道 搞容器化，起初用rancher，最后离职了也没搞完 接触it、弱电等，买网线、交换机、投影仪、电脑、打印机墨粉，自己做网线弄水晶头，修理电脑、打印机、投影仪，教行政财务怎么修复excel、word，跟着搞新办公区装修、拿着产品出去参展 …… 想想现在 认识到运维里还分了这么多，业务运维、运维开发、dba、安全、it、大数据、基础运维等等 认识到了一群大佬，果然强的人还是强 大的平台自然能接触到更多，美图是一个非常好的平台 在美图起初压力比较大，因为感觉自己扛不起来这些服务，现在慢慢上手了 初探sre 语言到是学了不少，shell、python、java、vue、swift，但是没一个稍微精通的 公司给了非常好的机会接触到了各种云，阿里云、华为云、aws、gcp 深入接触到了容器化，k8s牛逼 上云的忙碌到现在的心里空空，让我开始思考怎么去更好的开展运维工作 脾气收敛不少，没这么急躁了 …… 总结下 知识杂乱，没有章法 可能都知道点，但是稍微一问就完蛋 没能好好展示自己，让自己太平庸了 需要深入学习了，太多的皮毛容易被吹散","categories":[{"name":"多云专辑","slug":"多云专辑","permalink":"https://blog.itmonkey.icu/categories/%E5%A4%9A%E4%BA%91%E4%B8%93%E8%BE%91/"}],"tags":[{"name":"云运维","slug":"云运维","permalink":"https://blog.itmonkey.icu/tags/%E4%BA%91%E8%BF%90%E7%BB%B4/"}]},{"title":"SRE在多云环境下的生存之道","slug":"sre-cloud","date":"2020-06-22T06:56:26.000Z","updated":"2020-11-11T05:06:30.191Z","comments":true,"path":"2020/06/22/sre-cloud/","link":"","permalink":"https://blog.itmonkey.icu/2020/06/22/sre-cloud/","excerpt":"持续更新中……简介名称：SRE在多云环境下的生存之道简介：主要总结一下自己在云运维道路上的点点滴滴、踩的坑等。第一让新人快速认识各种云，第二让坑不再重演。地址：微信公众号程序猿的野生香蕉","text":"持续更新中……简介名称：SRE在多云环境下的生存之道简介：主要总结一下自己在云运维道路上的点点滴滴、踩的坑等。第一让新人快速认识各种云，第二让坑不再重演。地址：微信公众号程序猿的野生香蕉 开篇 云运维–开篇 基础篇： 云运维–通俗讲解各个云常用云服务 进阶篇： 云运维–如何在华为云快速部署一套可用的生产环境 云运维–如何在阿里云快速部署一套可用的生产环境 云运维–如何在aws快速部署一套可用的生产环境 云运维–如何在gcp快速部署一套可用的生产环境 深入篇： 云运维–如何在各个云上优雅的使用弹性伸缩 云运维–aws上容器服务ecs的实战教学 云运维–aws容器服务ecs进阶–引入弹性伸缩 云运维–aws如何使用原生的监控告警服务cloudwatch+sns 云运维–aws快速服务编排CloudFormation的使用 云运维–aws如何处理日志(物理日志+容器日志)kinesis+lambda 开发篇： 云运维–我在华为云上做的自动化工作 云运维–我在阿里云上做的自动化工作 云运维–我在aws上做的自动化工作 云运维–我在gcp上做的自动化工作","categories":[{"name":"多云专辑","slug":"多云专辑","permalink":"https://blog.itmonkey.icu/categories/%E5%A4%9A%E4%BA%91%E4%B8%93%E8%BE%91/"}],"tags":[{"name":"云运维","slug":"云运维","permalink":"https://blog.itmonkey.icu/tags/%E4%BA%91%E8%BF%90%E7%BB%B4/"}]},{"title":"inotify-tools如何做文件数据实时同步","slug":"rsync-inotify","date":"2020-06-17T11:41:32.000Z","updated":"2020-11-11T05:05:03.780Z","comments":true,"path":"2020/06/17/rsync-inotify/","link":"","permalink":"https://blog.itmonkey.icu/2020/06/17/rsync-inotify/","excerpt":"知识点落成文才是硬道理，欢迎大家读他人博客，写自己博客。 前言在日常的运维工作中，发现经常会处理一些迁移工作，在迁移过程中，难免会涉及到一些文件同步工作，今天这篇chat就来聊聊我经常是怎么去处理这些同步工作的。当然有大佬有其他姿势的都可以留言，欢迎交流。","text":"知识点落成文才是硬道理，欢迎大家读他人博客，写自己博客。 前言在日常的运维工作中，发现经常会处理一些迁移工作，在迁移过程中，难免会涉及到一些文件同步工作，今天这篇chat就来聊聊我经常是怎么去处理这些同步工作的。当然有大佬有其他姿势的都可以留言，欢迎交流。 知识点补充 你了解过哪些可以来操作同步的工具或者命令呢？ scp，没错scp算是文件拷贝命令，但是这个过程你也可以简单的认为他是同步的过程。 rsync，大家一定不会陌生，非常好用的一款同步工具，操作简单而且功能强大，至于具体的命令参数我就不做详细介绍了，这种网上一抓一大把，一会做场景说明的时候会举例子。 inotify-tools，他是linux下监控文件系统事件的工具，尝尝跟rsync来做文件的实时同步。 同步场景 单文件同步，点对点。 多文件同步 准实时同步 点对点实时同步 弹性同步 增量同步 单文件同步，点对点1234从本地传输到远端：scp -P 22 a.txt root@10.13.43.126:~从远端拉取到本地：scp -P 22 root@10.13.43.126:~/a.txt ./ 多文件同步1234567891011确保：1.安装好rsync2.配置好相应的module远端配置，我举个简单的例子：[rsync_test]path = /www/gitbook/read only = nohosts allow = *在正常的生产使用，hosts allow需要控制到具体的ip或者段，另外加上hosts deny = *来防止rsync类攻击以及漏洞。3.保证网络以及权限正常4.rsync进程是否运行(修改配置文件不用重启) 从本地push文件到远端测试： 准实时同步我先解释一下这个准实时，其实很简单，新增的文件并不会直接同步到远端，而是周期性的同步，有时间差。这种适用于报表导出等，我规定一个时间去同步文件。 实现逻辑：rsync做好文件同步，crontab配合在做定时同步，常用* * * * *来做准实时同步，每分钟同步一次。 举一个简单的例子来看： 123456rsync脚本：#!/bin/bash/usr/bin/rsync -rt 192.168.1.1::test_moduel/ /www/crontab定时任务可以这样写：* * * * * root /bin/sh rsync.sh &gt; /dev/null 2&gt;&amp;1 时间可以自己随意调整，主要是看具体的业务场景。 点对点实时同步这个同步方式简单解释下，其实就是利用inotify+rsync做文件数据的实时同步，废话不多说，直接举例说明： 首先需要安装inotify。检测你的机器是否支持inotify，检测方法 12345ll /proc/sys/fs/inotify，是否有一下三条输出，没有表示不支持total 0-rw-r--r-- 1 root root 0 May 7 17:01 max_queued_events-rw-r--r-- 1 root root 0 May 7 17:01 max_user_instances-rw-r--r-- 1 root root 0 May 7 17:01 max_user_watches 安装 1yum install inotify-tools -y 验证本地监听测试 例子：验证我本地/www/abc/的一个目录文件变化，然后做出对应的日志输出。 12命令：/usr/bin/inotifywait -mrq --timefmt '%Y/%m/%d-%H:%M:%S' --format '%T %w %f %e' -e modify,delete,create,move,attrib /www/abc/ 参数说明：|参数|说明|:–|:–|-m|持续监听|-r|使用递归形式监视目录|-q|减少冗余信息，只打印出需要的信息|-e|指定要监视的事件，多个时间使用逗号隔开|–timefmt|时间格式|–format|监听到的文件变化的信息对于的参数-e，他可以随意组合事件，可以看下表：|参数|说明|:–|:–|access|访问，读取文件。|modify|修改，文件内容被修改。|attrib|属性，文件元数据被修改。|move|移动，对文件进行移动操作。|create|创建，生成新文件|open|打开，对文件进行打开操作。|close|关闭，对文件进行关闭操作。|delete|删除，文件被删除。下面是我做实验的结果，具体可以看下日志和对应的操作： 做了上述验证之后，我们就可以直接进行点对点实时同步验证了。 描述下场景：A机器：10.17.36.122B机器：10.17.41.120A机器的/www/abc/需要跟B机器的/www/abc/进行实时同步准备：① B机器上已经配置好rsync module了，module叫：rsync_test② rsync同步命令： 1/usr/bin/rsync -rt /www/abc/ 10.17.41.120::rsync_test/ ③ 准备好rsync_inotify.sh脚本 12345678910#!/bin/bashfunction inotify_fun()&#123; /usr/bin/inotifywait -mrq --timefmt '%Y/%m/%d-%H:%M:%S' --format '%T %w %f %e' -e modify,delete,create,move,attrib /www/abc/ | while read file do /usr/bin/rsync -rt --progress -v --delete /www/abc/ 10.17.41.120::rsync_test/ done&#125;inotify_fun &gt;&gt; /dev/null 2&gt;&amp;1 &amp; ④ 运行测试A机器上运行脚本：sh rsync_inotify.sh，此脚本后台运行。为了能看到传输效果，我把&gt;&gt; /dev/null 2&gt;&amp;1 &amp;去掉了，可以看到具体的日志。 注意点 ① 生产环境如果要使用的话，直接把脚本后台运行。② rsync的参数可以自己定义，我写了一个–delete的参数，这个在生产环境要谨慎使用。 弹性同步这个场景比较特殊，是我在运维过程中遇到的，不一定对其他人都适用，有兴趣可以简单了解下。 背景 目前我们用弹性伸缩这个服务比较多，类似阿里云的ess，华为云的as，aws的autoscaling等。熟悉这个服务的同学都知道弹性出来的机器是按照某个镜像产生的，当产生镜像的这个机器(灰度机)中有一个目录需要实时更新，我们不可能每次更新就去创建一个镜像并且更新现有已经弹性出来的机器，这样不仅费劲还费钱。 处理方法 针对这个问题，其实最简单的做法就是在灰度机上做一个定时向某一台机器同步的任务，而这某一台机器就是灰度机自己。这样弹性出来的机器就会从灰度机拉取最新的文件。 举个例子 灰度机ip：192.168.1.1灰度机上定时任务： 1* * * * * /usr/bin/rsync -rt /www/abc/ 192.168.1.1::rsync_test/ 这样一来，弹性出来的机器都会带上这个定时任务，当192.168.1.1更新了/www/abc/目录下的文件，弹性机器就会每分钟同步一次。 如果想要实时同步，那么可以参考实时同步，在灰度机上做配置即可。 增量同步关于增量同步，有两个说法： 文件的增量同步，比如我有1.txt 2.txt，那么我本地新增了一个3.txt，那么传输的时候只会传输3.txt，而不会重新1.txt和2.txt都传输一遍。 文件内容的增量同步，比如我在1.txt中追加或者删除内容，那么远端会不会同步删除或者更新。 针对上述两个问题，我通常采用的方式就是上rsync_inotify.sh的内容，inotify来检测文件变更，其中有一个参数就是modify和attrib，这两个就可以满足第二条说法。对于第一条大家可以简单了解下rsync的–append参数。 最后关于同步其实方式很多，选择自己合适的，并且简单的，能满足业务需求的，就够了。","categories":[{"name":"基础运维","slug":"基础运维","permalink":"https://blog.itmonkey.icu/categories/%E5%9F%BA%E7%A1%80%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"rsync","slug":"rsync","permalink":"https://blog.itmonkey.icu/tags/rsync/"},{"name":"inotify-tools","slug":"inotify-tools","permalink":"https://blog.itmonkey.icu/tags/inotify-tools/"},{"name":"文件同步","slug":"文件同步","permalink":"https://blog.itmonkey.icu/tags/%E6%96%87%E4%BB%B6%E5%90%8C%E6%AD%A5/"}]},{"title":"适用于中小型企业自研的监控告警通知系统(附源码)","slug":"pandora-alert","date":"2020-06-17T11:33:52.000Z","updated":"2020-11-11T05:06:21.008Z","comments":true,"path":"2020/06/17/pandora-alert/","link":"","permalink":"https://blog.itmonkey.icu/2020/06/17/pandora-alert/","excerpt":"背景监控告警是运维包括开发的杀手锏，目前存在太多的告警通知方式，微信，邮件，短信，甚至机器人等等。那么我们如何把想要的通知快速而准确的通过这些渠道发送呢？今天就分享一个整合这些渠道的通知系统。(注：本人不是开发出身，所以这个系统是我闲暇时刻自学编写的，可能代码质量和规范上不如真正的开发，但是这个系统也是我平时运维中用的比较多的告警通知手段。希望能帮助到大家，有任何问题的同学可以联系我，欢迎批评指正！)","text":"背景监控告警是运维包括开发的杀手锏，目前存在太多的告警通知方式，微信，邮件，短信，甚至机器人等等。那么我们如何把想要的通知快速而准确的通过这些渠道发送呢？今天就分享一个整合这些渠道的通知系统。(注：本人不是开发出身，所以这个系统是我闲暇时刻自学编写的，可能代码质量和规范上不如真正的开发，但是这个系统也是我平时运维中用的比较多的告警通知手段。希望能帮助到大家，有任何问题的同学可以联系我，欢迎批评指正！) 文章导读监控告警是运维包括开发的杀手锏，目前存在太多的告警通知方式，微信，邮件，短信，甚至机器人等等。那么我们如何把想要的通知快速而准确的通过这些渠道发送呢？今天就分享一个整合这些渠道的通知系统。 交流： 如果对该篇文章或者代码有任何疑问的同学，可以添加本人微信号：sicau_libin，欢迎随时交流！ 常用的告警方式就目前来说，我已经接触到大部分告警通知方式了，那么这些告警方式都有哪些优缺点呢？让我们来简单谈谈吧。 邮件：他最基础的告警，简单，但是实时性并不是很高(比如我就很少看邮件) 短信：短信算是一个比较常用的告警方式，实时性高。不过不免费，虽然也不是特别贵，另外一点当告警收敛做的不够好的时候可能会造成打扰休息。 电话：这种是一个最高等级的通知方式，一般非常紧急的告警会采用这个，能够让人第一时间去介入问题的处理。 企业微信：这个针对性比较强，适合用企业微信的公司，当然个人账号也是ok的。定制性和实时性都比较好，推荐使用。 企业微信机器人：比较灵活，相对企业微信应用来说，我认为最大的优点就是比较好玩，我经常用机器人做一些定制化需求和定时任务通知等。 钉钉机器人：针对性较强，跟企业微信机器人类似。 大家还有使用到哪些告警方式，欢迎交流！ 如何设计一个告警系统我们公司有专门一个监控告警团队在做这一块，我作为业务运维，平时可能就是使用上会多点，所以我的观点不一定准确，仅仅是我个人的一些看法和建议。 告警实时性 告警准确性 告警可读性 告警统一性 告警追溯性 告警定制型 告警收敛(尤为重要) 告警实时性 告警必备条件之一，实时性。我们不可能等故障已经发生了十分钟后，才收到告警，这样是毫无意义的，告警一定要在用户发现问题之前发到我们手上，并且能够让我们快速定位问题，解决问题。 目前做的好的告警，已经实现了预警，就是还未发生告警，但是已经有告警的趋势，他会提前通知出来。举个简单的例子：我的磁盘使用率在持续增长，不过在前一个小时，这个增长速率变快，方差变大，那么这时候就要考虑是否是业务量级上涨，导致我的磁盘已经不满足需求，需要扩容。告警准确性这个比较好理解，出现告警一般是需要我们及时去处理的，那么就要求这个告警真实有效。举个例子：凌晨两点出现宕机告警，那么我们可能需要紧急来处理，如果这个告警是误报，那么这种就严重影响休息了。告警可读性简单来说，就是你发出来的告警能让人看懂，并且能够快速定位到问题点。举个例子，一条5xx告警，它需要体现哪些信息呢？答：告警标题，谁出错了，错误信息是什么，错误占比是多少，是在哪里出现错误了(那台机器上)，什么时候告警，哪个接口告警了，更甚至是给出一些告警解决的建议等等。告警统一性这个统一性有几个方面，第一告警平台的统一性；第二告警方式的统一性。 关于告警平台的统一性。一家公司一定要有一个统一告警平台。说简单点就是所有的告警必须由一个地方发出来。为什么说这点呢？因为就现状而言，三个人中，可能有三种不同的告警发送出路，你写一个脚本监控这个，他撸一个平台监控这个，告警满天飞，不仅湮灭了有效告警，还带来不小的成本开销。 关于告警方式的统一性。一般来说，我们会给告警划分等级，不同等级的告警采用不同的方式，一般告警用微信邮件方式，紧急告警用短信方式，最高等级告警直接电话(比如我们公司的机器宕机就是通过电话方式告警的)。告警追溯性这个比较好理解，简单来说就是我能够在几天之后查到这个告警，虽然这个用监控系统来体现会更好，不过这个也是不可或缺的。告警定制性正如可读性中提到的5xx告警，我们其实还有很多很多的告警，那么我们需要定制不同的模板，但是现实肯定是有个别差异的，所以定制的存在是必须的。举个例子，我们的A服务跑在aws上，B服务跑在gcp上，我们就需要定制了。第一aws和gcp的监控数据是不同的，所以我们得到的数据指标不一定完全吻合，所以需要定制化监控项。第二gcp存在墙的问题，那么我们如何拿到监控数据以及把告警发回来，又是需要单独定制的。告警收敛这个为什么我标志成尤为重要呢，是因为我现在每天收到的告警有几千条，其中包括邮件、短信、企业微信应用、企业微信机器人等等。那么会带来一个非常严重的问题，我不知道该去处理哪个告警，甚至会覆盖掉我需要优先处理的告警。 讲解我撸的告警通知系统注意：由于最近一些工作上的事情，其他方式我暂时没有太多时间撸，另外一个是短信和电话采用的三方应用不同，所实现的代码也不同，所以没办法做到统一，不过我后续会更新一版比较常用的短信和电话提供商代码。 项目介绍这个告警通知系统我自己取名潘多拉，代码我已经做了脱敏放到 github 上了，告警通知，有需要的通知可以去获取(有不会使用的通知可以加我微信，我可以为你单独讲解一下)下面是 tree ：通过 tree 可以很清晰明了的看到这个系统的简单性，不过对于重要公司其实已经够用了，或者你也可以定制自己的功能，尽管pr就好。 项目地址 GitHub1https://github.com/gsgs-libin/Pandora 如何运行目前有两中方式可以运行该项目： 直接运行依赖：需要你本地有python环境，装好gunicorn1234git clone https://github.com/gsgs-libin/Pandora.gitcd Pandorapip install -r requirements.txtgunicorn NoticeSystem:app -c gunicorn.conf 容器化运行依赖：本地需要装好docker或者k8s12构建镜像：docker build -t pandora:dev-1.1.1-1 .运行容器：docker run --name pandora -p 5001:5001 pandora:dev-1.1.1-1 ps:系统默认暴露5001端口，可以自行修改NoticeSystem.py中的port参数 企业微信应用模块说明要求： 有企业微信账号 创建一个企业微信应用 申请账号企业微信账号使用微信账号申请，这个就不细说了，非常简单，一步一步点就好了。 创建企业微信应用 获取秘钥，修改下载的代码修改代码文件中的./utils/secret.py: 123456789QYWXAPPKEY = &#123; \"WX_CORPID\":\"图1获取\", \"APPSecret\":&#123; \"SREAlert\":&#123; \"appsec\":\"图2获取\", \"agentid\":\"图2获取\" &#125; &#125;&#125; 图1：图2： 调用测试1curl 0.0.0.0:5001/send_qywx_app -H \"HTTP_VERIFICATION_KEY:5hbfJTAo9SBOb\" -X POST -d \"&#123;'AppName':'SREAlert','Content':'测试企业微信应用告警'&#125;\" 企业微信机器人模块说明需求： 企业微信客户端 任意一个群聊 创建一个机器人任意选择一个群聊，右键点击选择创建一个机器人，填写好机器人的名字就能创建一个测试机器人了。 获取机器人秘钥秘钥样例，我们只需要取地址对应key的值： 1https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=********** 修改代码，填写key修改文件utils/send_qywx_robot.py,填写上对应的key即可 1234RobotKeyList = &#123; \"lb6test1\":(('key','********************'),), \"lb6test2\":(('key','********************'),), &#125; 测试调用1curl 0.0.0.0:5001/send_qywx_robot -H \"HTTP_VERIFICATION_KEY:5hbfJTAo9SBOb\" -X POST -d \"&#123;'RobotKey':'lb6test1','Content':'测试','MesgType':'markdown'&#125;\" 邮件模块说明需求： 任意邮箱 得到smtp信息 注册邮箱任意邮箱即可，我这边直接选择网易163的。 获取smtp信息从这里可以获取到网易的smtp信息 修改代码，填写smtp信息修改文件utils/secret.py,替换如下内容： 123456EMAILSEC = &#123; \"EMAIL_SMTP_SERVER\": \"smtp.163.com:465\", \"EMAIL_USER\": \"xx@163.com\", \"EMAIL_PASSWORD\": \"******\", \"EMAIL_SENDER\": \"xx@163.com\"&#125; 运行测试1curl 0.0.0.0:5001/send_mail -H \"HTTP_VERIFICATION_KEY:5hbfJTAo9SBOb\" -X POST -d \"&#123;'ReceiverList':'a@gmail.com,b@163.com','Subject':'测试主题','Content':'测试邮件内容2'&#125;\" 关于邮件标题的修改有点可以自定义标题，我们直接修改utils/send_mail.py文件： 123456789101112def SendEmail(receiver_list,subject,content): sender = EMAILSEC[\"EMAIL_SENDER\"] subject = subject smtpserver = EMAILSEC[\"EMAIL_SMTP_SERVER\"] msg = MIMEText(content, 'html', 'utf-8') msg['From'] = formataddr((str(Header('【Alert】XX运维告警系统', 'utf-8')), sender)) msg['Subject'] = Header(subject, 'utf-8') msg['To'] = receiver_list # 字符串 smtp = smtplib.SMTP_SSL(smtpserver) smtp.login(EMAILSEC[\"EMAIL_USER\"], EMAILSEC[\"EMAIL_PASSWORD\"]) smtp.sendmail(sender,receiver_list.split(','), msg.as_string()) smtp.quit() 告警效果图邮件 企业微信应用 企业微信机器人","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://blog.itmonkey.icu/categories/DevOps/"}],"tags":[{"name":"python","slug":"python","permalink":"https://blog.itmonkey.icu/tags/python/"},{"name":"监控告警","slug":"监控告警","permalink":"https://blog.itmonkey.icu/tags/%E7%9B%91%E6%8E%A7%E5%91%8A%E8%AD%A6/"},{"name":"开源","slug":"开源","permalink":"https://blog.itmonkey.icu/tags/%E5%BC%80%E6%BA%90/"}]},{"title":"搭建企业私有yum仓库并打自己第一个rpm包","slug":"build-rpmrepo-and-first-rpm","date":"2020-06-17T11:27:21.000Z","updated":"2020-11-11T05:04:23.772Z","comments":true,"path":"2020/06/17/build-rpmrepo-and-first-rpm/","link":"","permalink":"https://blog.itmonkey.icu/2020/06/17/build-rpmrepo-and-first-rpm/","excerpt":"私有yum仓库的好处 减少rpm包下载网络开销 加速下载rpm包 便于管理自定义rpm包 具有企业特色","text":"私有yum仓库的好处 减少rpm包下载网络开销 加速下载rpm包 便于管理自定义rpm包 具有企业特色 如何搭建私有yum仓库创建自己yum仓库路径1mkdir /www/yum_store/centos/7/meitu/x86_64/ 现在仓库是空的，我们简单下载几个软件放到我们的私有仓库中： 1yum install --downloadonly --downloaddir=/www/yum_store/centos/7/meitu/x86_64/ nfs-utils --downloadonly (只下载不安装) --downloaddir (指定rpm包的下载路径) 安装createrepo工具1yum -y install createrepo 生成repodata索引1createrepo -pdo /www/yum_store/centos/7/meitu/x86_64/ /www/yum_store/centos/7/meitu/x86_64/ 为你的仓库提供web访问能力这个可以用很多方式，用nginx、apache或者其他，我这边做演示，简单启动一个python的web服务即可。 12cd /www/yum_store/centos/python -m SimpleHTTPServer 80 你可以在浏览器中访问你的web服务，可以得到如下的信息： 1http://localhost:80/7/meitu/x86_64/ 配置yum源，repo文件(暂无配置签名)1234567vim /etc/yum.repos.d/my-meitu.repo[meitu]name=my-meitu-repobaseurl=http://localhost:80/$releasever/meitu/$basearchenable=1gpgcheck=0 刷新yum源，查看私有仓库配置是否成功12345[root@node1 ~]# yum clean all[root@node1 ~]# yum repolistrepo id repo name statusmeitu/7/x86_64 my-meitu-repo 16repolist: 16 查看自己yum仓库中的rpm包有哪些 安装其中一个软件测试查看输出信息，你会发现该软件以及该软件的依赖都来自于你的私有仓库，另外你也可以看看你的web服务日志，会打印出一些日志。 1yum install nfs-utils.x86_64 如何打自己第一个rpm包目前有提供两种打包方式，第一rpmbuild，第二种使用FPM。相对来说rpmbuild会比较复杂，我是更喜欢fpm一点，所以今天简单讲解一下fpm，如果对rpmbuild有兴趣的同学，我后续也可以单独再写一篇讲解。 整体流程 把你的软件安装在一个临时目录里 修改一些自定义的配置 写两个脚本：安装后执行脚本，卸载后执行脚本 打包，生成rpm包 安装打包工具 fpm是ruby写的，所以要装一下ruby环境12yum -y groupinstall \"Development Tools\"yum -y install ruby ruby-devel rubygems gcc openssl-devel 配置ruby源加速123456gem sources --add http://gems.ruby-china.com/gem sources --remove https://rubygems.org/[root@node1 ~]# gem sources*** CURRENT SOURCES ***http://gems.ruby-china.com/ 安装fpm1gem install arr-pm fpm 制作helloworld软件我这边就不已繁琐的软件为例子了，我自己随便写了一个helloworld脚本：123vim helloworld#!/bin/bashecho \"hello world,this is my first rpm\" 整体的一个目录结构安装后脚本：创建一个目录卸载后脚本：把helloworld文件删除1234567891011121314151617181920212223[root@node1 helloworld]# tree.├── install_dir 打包的相对路径│ └── usr│ └── bin│ └── helloworld├── rpm 构建产物rpm存放的位置└── scripts ├── after_install.sh 安装后脚本 └── after_remove.sh 卸载后脚本5 directories, 3 files[root@node1 helloworld]# cat scripts/after_install.sh#!/bin/bashsource /etc/rc.d/init.d/functionsmkdir /wwwexit $?[root@node1 helloworld]# cat scripts/after_remove.sh#!/bin/bashsource /etc/rc.d/init.d/functionsrm /usr/bin/helloworld -rfexit $?[root@node1 helloworld]# 开始构建至于fpm的参数，直接看man文档就好了，非常简单，下面就是常用的几个。123[root@node1 helloworld]# fpm -f -s dir -t rpm -n meituhello -v 1.1.0 --iteration 1.el7 -C ./install_dir/ -p ./rpm/ --description 'meituhello-1.1.0.rpm' --url 'https://www.meitu.com' --after-install scripts/after_install.sh --after-remove scripts/after_remove.sh -m libin --vendor \"dgsfor@gmail.com\"Created package &#123;:path=&gt;\"./rpm/meituhello-1.1.0-1.el7.x86_64.rpm\"&#125; 查看rpm包信息1234567891011121314151617181920[root@node1 rpm]# rpm -qpi meituhello-1.1.0-1.el7.x86_64.rpmName : meituhelloVersion : 1.1.0Release : 1.el7Architecture: x86_64Install Date: (not installed)Group : defaultSize : 52License : unknownSignature : (none)Source RPM : meituhello-1.1.0-1.el7.src.rpmBuild Date : Fri 24 May 2019 02:20:19 PM EDTBuild Host : node1Relocations : /Packager : libinVendor : dgsfor@gmail.comURL : https://www.meitu.comSummary : meituhello-1.1.0.rpmDescription :meituhello-1.1.0.rpm 安装测试安装完之后直接执行meituhello命令，会输出你自定义的信息123456789[root@node1 rpm]# rpm -ivh meituhello-1.1.0-1.el7.x86_64.rpmPreparing... ################################# [100%]Updating / installing... 1:meituhello-1.1.0-1.el7 ################################# [100%] [root@node1 rpm]# ls /usr/bin/helloworld/usr/bin/helloworld[root@node1 rpm]# helloworldhello world,this is my first rpm 卸载理论上卸载会删除helloworld文件123[root@node1 ~]# rpm -e meituhello-1.1.0-1.el7.x86_64[root@node1 ~]# ls /usr/bin/helloworldls: cannot access /usr/bin/helloworld: No such file or directory 把这个包加到我们的私有yum仓库每新增一个rpm包，都要update一下仓库 123456789[root@node1 ~]# cp rpm/meituhello-1.1.0-1.el7.x86_64.rpm /www/yum_store/centos/7/meitu/x86_64/[root@node1 ~]# createrepo --update /www/yum_store/centos/7/meitu/x86_64/Spawning worker 0 with 1 pkgsWorkers FinishedSaving Primary metadataSaving file lists metadataSaving other metadataGenerating sqlite DBsSqlite DBs complete","categories":[{"name":"基础运维","slug":"基础运维","permalink":"https://blog.itmonkey.icu/categories/%E5%9F%BA%E7%A1%80%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"rpm","slug":"rpm","permalink":"https://blog.itmonkey.icu/tags/rpm/"},{"name":"yum仓库","slug":"yum仓库","permalink":"https://blog.itmonkey.icu/tags/yum%E4%BB%93%E5%BA%93/"}]},{"title":"美图实战分享：如何真实模拟生产流量进行服务性能压测","slug":"how-to-empress","date":"2020-06-17T08:38:09.000Z","updated":"2020-11-11T05:06:11.309Z","comments":true,"path":"2020/06/17/how-to-empress/","link":"","permalink":"https://blog.itmonkey.icu/2020/06/17/how-to-empress/","excerpt":"文章导读服务压力测试，是评估一个服务是否优秀的过程，他不仅能让你找到你的服务哪些地方存在性能瓶颈，而且还能让你准确的去做容量评估，防止容量不足，也规避了资源浪费。本文会带你了解以下几点内容： 压测的意义 压测注意点 压测准备 模型压测的自我理解 普通压测工具 goreplay压测工具介绍","text":"文章导读服务压力测试，是评估一个服务是否优秀的过程，他不仅能让你找到你的服务哪些地方存在性能瓶颈，而且还能让你准确的去做容量评估，防止容量不足，也规避了资源浪费。本文会带你了解以下几点内容： 压测的意义 压测注意点 压测准备 模型压测的自我理解 普通压测工具 goreplay压测工具介绍 为什么要压测 业务推广保障 准确评估容量 快速发现服务瓶颈 极限压力测试 服务迁移全链路测试压测需要注意哪些点(重点阅读) 环境统一(基础环境、参数配置、资源型号、请求链路) 压测机不能存在瓶颈 压测姿势一致 多次压测 注意写流量(如果你不想产生脏数据) 注意三方依赖服务(你压测A服务，要考虑是否会顺带压测到B服务) 极限压测过程中注意拐点(某些时候你qps上不去，响应时间大幅度上涨，那就说明是拐点了)环境统一这个目前我们有两种方式，第一种是新建测试环境压测，第二种是直接在生产环境压测。如果新建，那么我们需要考虑的点就是跟生产环境完全一致。 基础环境，包括系统版本，系统参数配置 软件环境，比如 Php 版本，Php 参数配置 资源型号，机器的型号配置，Redis 的规格，MySql 的规格等 请求链路，比如我请求的链路是经过两层 Nginx，不能在压测的时候只经过一层 Nginx。压测机器不能存在瓶颈我们之前在压测过程中，会发现当压测到一定量级后就上不去了，这时候我们一直在排查服务端是否有什么问题，包括部署资源是否足够，后端资源是否性能瓶颈，带宽是否打满等信息，绕了一圈都没有发现问题。最终我们发现是压测端机器性能较低，带宽打满了，最终换用了更高配的机器来进行压测。所以我们压测端一定不能成为压测计划的瓶颈。压测姿势一致 简单举个例子，用ab压测的时候，需要指定线程，压测时间等参数，假如第一次用10个线程，压测10分钟，第二次用12个线程，压测20分钟 那么这样得到的数据是完全无用的。 多次压测性能压测的数据是多次得来的，结论也是多次数据对比得出的。由于每次压测都会有无法预知的问题，所以每次压测结束后，都需要分析结果，看看是否符合预期，比如在第二次压测，某截网络断了，那么得到的数据可能会非常差，那么这次的压测数据就不能使用。 注意写流量这个也是跟我们的压测场景有关。 当我们在生产压测，那么我们肯定不能有写流量的压测，那么我们的生产数据就会脏掉，除非这些数据能够把控，在压测后能完全删除，不然是只能压测读流量。 当我们在测试环境压测，这种情况可以压测全链路，不过有一个点需要关注下，当我们是服务迁移的压测，这时候生产环境跟测试环境是数据实时同步的，测试环境的脏数据也会回写到生产环境。注意三方依赖服务 再举个例子，我美颜相机服务要准备压测，依赖登录、素材展示两个服务。 那么我们在压测的时候一定要去周知登录和素材的开发同学，以及让他们做好扩容准备和实时监控，实时反馈目前的一个服务状态，当有异常的时候，应立即停止压测。 极限压测过程中注意拐点在我们压测过程中，是需要记录每次压测的结果的，比如压测结果的qps、相应时间、服务SLA等信息。当某些值在一定持续增长的压力下增长缓慢、不增长甚至降低，那么这时候你就要考虑压测是否到达了一定的拐点，类似可以做一个如下的图来简单判断拐点： 压测前准备 明确整体系统链路情况 依赖资源的容量评估，确认压测的时候是否需要扩容 压测部分降级方案的确认 明确整体系统链路情况业务压测前，首先需要明确业务架构，开发需要配合运维梳理整个系统结构，开发补充业务层面，运维补充系统层面，整理出一个流量走势以及三方依赖图。明确是否跨机房、是否跨专线、是否存在网络隔离等问题。 依赖资源的容量评估 例子，A业务压测，三方依赖服务有B和C，首先要记录当前生产业务的qps，B和C业务的使用量级。假如是1000qps，B用量10台机器，C用量20台机器。那么我们压测需要压到2000qps，理论上B用20台机器，C用40台机器。 压测部分降级方案的确认做压测工作，我们需要明确几点： 我们是为了保障线上业务稳定才做的压测，所有的评估都要有预留。 压测遇到的问题也正是我们上生产或者迁移后遇到的问题，所以每一个问题都需要正视。 所以针对上面两点，我们需要明确业务容量评估一定要留cache，降级方案、回退方案一定要有，防止真的出现问题。下面是我们在压测过程中遇到的问题以及处理的过程： 模型压测(自我理解)这是我在压测过程中，总结的一个点，这个我可能没法用很好的词语来描述，我就用一个简单的例子来描述吧。 业务场景：简单的 Nginx + PHP 业务，容器化部署。 那么我们可以配置以下几个压测模型场景来进行压测，然后给出相应的结论： 单 Pod CPU 内存压测 单 Pod Nginx + PHP 空接口压测 单 Pod Nginx + PHP 业务单接口压测 关于单 Pod CPU 内存压测，我们可以在业务部署机器上启动一个空 Pod，然后进行压测，不涉及任何业务，这里得到的结论 — 排除底层平台带来的影响。 单 Pod Nginx + PHP 空接口压测，这里你可以随便写一个php接口，return 一个 OK 就行，然后进行压测，这里得到的结论 — 排除业务基础环境带来的影响比如 Nginx + PHP 等。 单 Pod Nginx + PHP 业务单接口压测，这里找一个具有代表性的接口，什么是具有代表性的接口，比如这个接口是涉及到读 Redis 或者 MySql 等后端资源的。这里得到的结论 — 业务层面的一个压测结果。 经过这一轮压测之后，你能非常明确你的压测结论是否存在异议，一步步排除环境因素。 普通压测工具介绍目前来说，我在运维过程中使用的压测工具也就是ab，wrk等，其实针对不同的场景，每一个工具都有自己的优点和缺点。 ab 支持多平台 默认短链接 只能单进程 无法控制压测时间，控制速率 wrk 类unix 支持多线程(更容易发挥多核的能力) 支持自定义脚本 默认长链接 goreploay介绍优点 可用于服务迁移前的全链路测试 支持http请求录制和重放，可以复制线上请求，在压测环境重放 支持http层的流量过滤，比如我只复制某一个接口的请求 支持请求放大，用于性能压测 ……文档地址12官方git地址：https://github.com/buger/goreplay官方文档：https://github.com/buger/goreplay/wiki 流量捕获 input-raw 捕获 HTTP 流量，需指定端口号 input-file 使用 –output-file 记录的文件作为输入 input-tcp 将多个 Gor 实例获取的流量聚集到一个 Gor 实例 流量重放 output-http 将流量重放到URL地址 output-file 将获取的流量记录如文件 output-tcp 将获取的流量转移至另外的 Gor 实例，与input-tcp组合使用 output-stdout debug 工具，将流量信息直接输出 请求过滤 http-allow-url 只发送正则匹配的url的请求 http-disallow-url 不发送正则匹配url的请求 http-allow-header 只发送指定head的请求 http-disallow-header 不发送指定head的请求 http-allow-method 允许的请求方法 http-set-header 增加http-header 流量限制 随机丢弃一部分请求 按照准确的qps限制 按照比例限制（这里的比例，可以实现大于100%，即倍量的压测） 根据Header 或者 URL 参数限制一部分请求 实战演练goreplay安装123依赖go环境wget https://github.com/buger/goreplay/releases/download/v1.0.0/gor_1.0.0_x64.tar.gztar xvf gor_1.0.0_x64.tar.gz 抓取流量在公司中，我们的流量入口一般是公共代理，比如nginx等。下面例子是我在公司其中一台公共代理上抓取的流量，有以下几个说明：(给的例子是我在使用过程中的，有特殊需求的可以参考官方文档进行相应的修改) 80端口 allow header：lb6test.meitu.com，这就是我想要压测的服务域名 method是get，因为post请求我们不压测，涉及到写数据 allowurl是我只想要抓取这个域名中a和b这两个接口的流量 disallow header，我要抓取token为空的，因为不为空的token涉及到登录，会连带压测我们登录的服务 outputfile：把抓取的流量输出为一个流量包，这样好携带 1./goreplay --input-raw :80 --http-allow-header 'Host: lb6test.meitu.com' -http-allow-method 'GET' -http-allow-url '/a/a.json' -http-allow-url '/b/b.json' -http-disallow-header 'Access-Token:' -output-file meitu.gor 普通输出流量这个操作一般我们会放到单独的压测机器上进行，如果你压测的是新部署的测试环境，注意这个域名一定要在这个机器上绑定一个host，防止你压测到生产环境去了。 loop，循环压测，因为流量包抓取的时间是固定的，如果不循环压测，流量重放结束就会终止压测 disallowurl，我突然不想压测b接口的流量，那么就关闭掉。 outputhttp，从该压测机发起对测试环境的请求流量1./goreplay --input-file-loop --input-file \"meitu.gor\" --http-disallow-url \"/b/b.json\" --output-http \"http://lb6test.meitu.com\" 压测输出流量当你按照普通输出流量走一遍的时候，并且不过滤任何接口，你就会知道你这个流量包到底有多少流量，是多少 qps，假如我的meitu.gor是100 qps。我想要50qps1./goreplay --input-file-loop --input-file \"meitu.gor\" --output-http \"http://lb6test.meitu.com|50\" 我想要200qps这里一个操作是把流量放到1000%，那么你就可以认为这个qps是无限大的，你可以任意指定你想要的qps了。1./goreplay --input-file-loop --input-file \"meitu.gor|1000%\" --output-http \"http://lb6test.meitu.com|200\" 一次压测太慢，我要快速压测如果你觉得一个进程太慢了，那么就多启动几个进程进行压测吧1nohup ./goreplay --input-file-loop --input-file \"meitu.gor\" --output-http \"http://lb6test.meitu.com\" &gt; /dev/null 2&gt;&amp;1 输出文档我们压测好之后，应该输出哪些东西呢 基础压测的数据 极限压测的数据 不同压力下的数据 qps，响应时间，成功率 下面是我当时压测公司某个服务记录的一些信息和描绘的图。","categories":[{"name":"运维进阶","slug":"运维进阶","permalink":"https://blog.itmonkey.icu/categories/%E8%BF%90%E7%BB%B4%E8%BF%9B%E9%98%B6/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://blog.itmonkey.icu/tags/linux/"},{"name":"服务压测","slug":"服务压测","permalink":"https://blog.itmonkey.icu/tags/%E6%9C%8D%E5%8A%A1%E5%8E%8B%E6%B5%8B/"}]},{"title":"美图运维之旅：在阿里云上的经验以及踩过的坑","slug":"meitu-aliyun-use-experience","date":"2020-06-17T07:40:51.000Z","updated":"2020-11-11T05:05:21.757Z","comments":true,"path":"2020/06/17/meitu-aliyun-use-experience/","link":"","permalink":"https://blog.itmonkey.icu/2020/06/17/meitu-aliyun-use-experience/","excerpt":"前言本篇chat可能更多的是文字表述，没啥高深的用词，都是大白话，需要各位朋友有耐性的阅读下去，也希望各位朋友能够提出宝贵建议。","text":"前言本篇chat可能更多的是文字表述，没啥高深的用词，都是大白话，需要各位朋友有耐性的阅读下去，也希望各位朋友能够提出宝贵建议。 美图公司是如何使用阿里云的关于使用方面，我想从以下几个点简单叙述一下， 权限规划 区域规划 网络规划 型号规划 统一登录规划 命名规划权限规划权限规划只要是在账号权限规划上，一般是主账号+子账号，然后通过ram来进行对应授权，为什么要做这个，因为主账号一般控制在运维手上，子账号可能会存在其他运维手上以及某些开发手上，那么就需要做严格的权限控制。目前阿里云ram有几个点：用户，群组，角色，策略等功能，目前角色和策略都支持自定义，然后可以应用到用户上，群组上。角色可以附加到ecs上，可以用高权限获取一些云上资源信息。区域规划做区域规划需要考虑几点： 第一、你的业务架构，比如我有idc机房在北京，那么我们上云的话一般会考虑北京region，又比如我的业务用户主要分布在江浙一带，那么我们一般就考虑上海或者杭州region。 第二、一定要找阿里云技术问到当前region的最新可用区是哪个，机器储备量级是多少，因为新可用区一般会有新机器，新型号，一般来说带来的好处就是性能高还便宜；另外老可用区机器型号非常少，经常会遇到售馨的问题。 第三、当我们定好了区域之后，后续再改变会非常麻烦，所以前期的调研准备一定要完善，考虑要全面。网络规划关于网络规划，主要有以下几点需要考虑下： 第一、vpc规划，可以按照你的业务架构划分vpc，不同vpc之间可以做一些安全组也隔离流量。例如我有大数据业务，普通业务，运维业务，那么我可以简单创建几个vpc：大数据vpc，生产业务vpc，测试业务vpc，运维vpc。 第二、子网划分，每一个vpc划分多个子网，比如我拿生产业务vpc来举例子，可以划分为：容器子网，mysql子网，redis子网等。(注意：子网在阿里云就是交换机的概念) 第三、子网的ip数量，这个的定义需要提前规划和统计业务量级，vpc本身是一个大的网段，单个子网的网段要合适，太大浪费，太小扩展麻烦。 我们内部的子网划分： 型号规划和命名规划把这两个放一起讲，一般来说公司会选择一款型号来作为自己的主力机型，这个配置比较贴近自己的业务形态，另外这个机型也要跟阿里云备案，让他们提供足够大的节点池。命名规划的话，我们可以按照业务提供能力来做不同的命名方式： 123k8s节点: aly-k8snode-10-10-10-1.meitu.commysql: aly-mysql-A-10-10-10-2.meitu.comredis: aly-redis-B-10-10-10-3.meitu.com 做好这个，第一能更好区分不同节点功能，第二能更好的去做自动化运维工作。 统一登录规划做统一登录就是要做跳板机、做权限管理。有以下几点要考虑： 第一、用户登录虚机需求，运维测需要做一个公共跳板机，所有的登录操作都需要通过该机器跳转。 第二、开发用户对于线上的机器不能有永久权限，开放的一般是临时权限，所以我们需要有一套授权，回收权限的平台。 第三、做好操作追踪，防止一些高危人员做一些非法操作。 我们内部的工单平台： 我们是如何做成本优化的关于成本优化，是一个非常重要并且也非常头疼的一件事情，当服务趋于稳定后，一定要慢慢去优化成本。那么关于成本优化的事项，我们做了这些事情： 容器化 弹性伸缩的深入使用 做好资源开通记录和监控 架构规划(内网和公网) 增加混布实例 定时巡检，删除无用资源 最低实例实行包年包月 容器化容器化趋势，他也是作为成本优化最好的方案之一，目前我们公司95%的业务已经容器化。我们在云上使用的是托管的k8s集群，另外我们自己有一套自研的k8s管理工具，来做日常的业务发布，迭代等操作。 弹性伸缩的深入应用我认为云服务和自建IDC最大的差距就是弹性伸缩的应用，我个人也是非常喜欢弹性伸缩这一个服务，因为他能很好的提升资源利用率，并且显著的降低成本。我们在以下几个场景用了弹性伸缩服务： 第一、托管k8s服务的容器节点。 第二、普通虚机业务开通弹性伸缩。 第三、容器业务服务开通HPA。 第四、共享带宽的弹性。 结合这些弹性伸缩的场景，我们做了一些工具，主要是观察全局的一个情况。阿里云弹性伸缩控制台总览：容器单个业务伸缩策略详情： 做好资源开通记录和监控这个非常重要，因为我们经常会遇到一个情况是：业务临时有个需求，需要开一台测试机器来做业务验证，我们开通完交付。他们测试完成之后并不会通知你是否需还需要这台机器，导致慢慢被遗忘，所以增加了额外的费用。另外一点我们也可以根据记录去做一些下线工作：当一个业务需要下线时候，我们可以根据他们的资源申请单去下线相关的服务，避免漏下或者错下。 架构规划(内网和公网)这个点涉及到的一般就是流量费用，公网带宽费用，我们遇到过一些情况是：业务调用能走内网的他不走内网，非要走公网，甚至他根本就不知道有内网地址。所以基于这个我们做了以下几点： 第一、服务上线评审，新服务上线一定要拉上运维，开发一起做一个评审，因为开发不懂线上网络结构，他们认为只要服务能够正常通就是没问题的，所以他们忽略了非常多的细节，比如内网调用等。 第二、服务提供内网域名，公司业务越多，那么存在互相依赖的地方就越多，这些内部依赖的动作能通过内网的一定要通过内网，不要从公网绕一圈。比如我在阿里云上下载oss图片，明显有内部的oss地址可用。 第三、服务部署规划，当有A的子服务B需要上线，且B是完全给A调用的，那么该服务上线的区域一定是A所在区域，不要存在跨可用区，跨region，跨专线等操作，这些都会带来额外的流量费用。增加混布实例混布一般是针对非容器化业务的，虽然混布不建议，但是也是不能缺少的。公司肯定会有类似的业务，并且年代久远，无人维护，像这类业务就可以统一丢到一两台机器上部署即可。定时巡检，删除无用资源这一步也是运维日常，我们会跑一些自动化的脚本去删除一些无用的服务，并且列出一些无主资源去让业务方确认是否释放。 镜像。阿里云的镜像可以只留最新的几个，删除老的镜像。 快照。跟镜像类似。 磁盘。删除无挂载，也无需保留的磁盘。 OSS。清理oss空间，业务方会保留非常多的静态资源，他们也不会删除，也不会去管，所以需要我们来推动他们来确认清理。 ECS。删除临时测试机器，删除业务已经下线的机器。 …… 虽然这些都是比较便宜，不过积少成多，成本优化本身就是一个漫长的过程，所以一点点的优化才是王道。 最低实例实行包年包月这个方案主要是针对弹性伸缩来说的，我们会观察每一个弹性伸缩组一天的监控数据，包括cpu利用率，实例数量。然后观察几周的数据，发现最低的一个实例数量，把这部分实例转成包年包月。 如下图，我们可以拿2-3台进行转包月操作： 我们在阿里云上遇到了哪些坑其实阿里云在国内市场已经做的非常好了，遇到的坑也大多比较贴近自身业务，有些坑他们也已经做了架构调整规避了。 区域资源配额问题 NAT 带宽问题 实例售馨问题 可用区交换机网段过大 其实坑还有很多，只不过有些都是小坑，有些可能我自己也有点忘记了。 下面简单解释下这些坑： 区域资源配额问题我们在重要节日推广，需要大量的机器储备，某年春节的时候遇到了区域资源配额瓶颈问题，即这个账号默认可以使用多少的vcpu(这个主要是按需的，包年包月的不会计算进来)。当时推广的时候，我们机器完全扩不上来了，最终排查到是资源配额上限了，也是紧急通知阿里云帮忙调整。 NAT 带宽问题最早阿里云的 NAT 还是有单独的带宽配置的，当时我们有一个服务推广效果比较好，直接把 NAT 打满了，导致我们整个vpc内的网络都非常慢，严重影响了我们服务质量。目前阿里云刚把 NAT 带宽调整成共享带宽包，带宽和 NAT 实现了分离，另外带宽也可以动态调整，非常的方便。 实例售馨问题如前面所说的，一个企业一般会选用一个主力型号，当时是弹性的机器过多，导致该型号实例售馨，新机器无法创建。针对这个问题，我们做了如下两个解决方式： 设置多实例弹性，弹性伸缩可以配置弹多个实例型号，并且也有优先级。 替换大规格机器，简单来说就是用一台大规格实例顶替多台小规格实例。可用区交换机网段过大遇到这个坑的背景也是我们想使用新可用区，因为我们IDC跟阿里云是默认专线打通的，所以会存在网段分配的问题，当初分配给阿里云的网段已经用完了，所以我想开新区，必须释放老的交换机或者拆分它。分配的网段用完了的根本原因还是：当初规划不合理，交换机设置过大，然后根本用不到这么多。我们如何做自动化运维的 弹性伸缩服务更新 统一登录权限申请 云上资源数据统计 自定义资源监控告警 定时弹性伸缩控制成本 重要业务推广通知 移动端运维 关于自动化运维这一块，其实需要紧密贴近公司的业务来做。只要你平时感觉哪个操作经常做，又非常繁琐，那么他就可以用工具替代，所以下面的东西都是我平时经常操作而抽象出来的。 弹性伸缩服务更新弹性伸缩服务，一个非常重要的操作就是更新，我们每个弹性伸缩组都有上百个实例，不可能去一个一个更新，肯定是创建一个镜像后，然后按照这个镜像进行滚动更新。我们有100个左右的弹性伸缩组，所以这个更新操作是经常需要的，所以我做了自动更新操作，并集成到了移动端和web端。 统一登录权限申请我们有自己的一套权限申请平台，相应的人员可以申请线上的机器，申请可以是临时的，也可以是拥有的。 云上资源数据统计这个统计信息会统计我们当前阿里云使用了多少机器，多少cpu，多少内存。这个信息会发给我们老大，同时开发老大也会关注这个，因为涉及到成本。 自定义资源监控告警其实阿里云集成的监控告警已经比较全面了，他可以通过邮件、短信、钉钉等方式发送出来。不过涉及到业务层面的，或者更贴合我们自己需求的，一般都会自定义通知，大家可以看我这篇chat，《美图分享：适用于中小型企业自研的监控告警通知系统（附源码）》，我平时自定义告警都是通过这个项目进行发送的，种类繁多。 定时弹性伸缩控制成本我们完全按照阿里云弹性伸缩设置默认的伸缩规则有时候可能还不能满足，借此我们可能还会设置一些定时伸缩，比如我晚上2点就让它维持两台机器。这个也是控制服务稳定性的一个操作，因为在某些时刻，服务是1台机器扛不住，两台机器会缩容的状态。我们开发了一个工具展示定时任务的列表(由于业务迁移了，所以这里没数据了，嘿嘿) 重要业务推广通知一般我们的业务在重要节日中，都会有推广。那么推广前，我们一定需要保障该服务的稳定性，该扩容的扩容，该加配置的加配置，所以我们做了重要推广通知的平台，来重点保障春节，五一，国庆等重要节日业务推广。 移动端运维移动端运维也是今年我们新引入的，因为我们已经厌烦了天天背着电脑了，能在手机上操作就在手机上操作了。其实上面的很多服务都是移动端的界面。由于自己开发能力有限，所以只开发了 IOS 客户端，后端就是python写的。(有兴趣的朋友，可以一起交流一下！)","categories":[{"name":"云运维","slug":"云运维","permalink":"https://blog.itmonkey.icu/categories/%E4%BA%91%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"运维","slug":"运维","permalink":"https://blog.itmonkey.icu/tags/%E8%BF%90%E7%BB%B4/"},{"name":"阿里云","slug":"阿里云","permalink":"https://blog.itmonkey.icu/tags/%E9%98%BF%E9%87%8C%E4%BA%91/"},{"name":"美图","slug":"美图","permalink":"https://blog.itmonkey.icu/tags/%E7%BE%8E%E5%9B%BE/"}]}],"categories":[{"name":"sre-report","slug":"sre-report","permalink":"https://blog.itmonkey.icu/categories/sre-report/"},{"name":"k8s","slug":"k8s","permalink":"https://blog.itmonkey.icu/categories/k8s/"},{"name":"云运维","slug":"云运维","permalink":"https://blog.itmonkey.icu/categories/%E4%BA%91%E8%BF%90%E7%BB%B4/"},{"name":"DevOps","slug":"DevOps","permalink":"https://blog.itmonkey.icu/categories/DevOps/"},{"name":"Report","slug":"Report","permalink":"https://blog.itmonkey.icu/categories/Report/"},{"name":"anthos","slug":"anthos","permalink":"https://blog.itmonkey.icu/categories/anthos/"},{"name":"运维进阶","slug":"运维进阶","permalink":"https://blog.itmonkey.icu/categories/%E8%BF%90%E7%BB%B4%E8%BF%9B%E9%98%B6/"},{"name":"基础运维","slug":"基础运维","permalink":"https://blog.itmonkey.icu/categories/%E5%9F%BA%E7%A1%80%E8%BF%90%E7%BB%B4/"},{"name":"go","slug":"go","permalink":"https://blog.itmonkey.icu/categories/go/"},{"name":"多云专辑","slug":"多云专辑","permalink":"https://blog.itmonkey.icu/categories/%E5%A4%9A%E4%BA%91%E4%B8%93%E8%BE%91/"}],"tags":[{"name":"sre","slug":"sre","permalink":"https://blog.itmonkey.icu/tags/sre/"},{"name":"devops","slug":"devops","permalink":"https://blog.itmonkey.icu/tags/devops/"},{"name":"prometheus","slug":"prometheus","permalink":"https://blog.itmonkey.icu/tags/prometheus/"},{"name":"gpu","slug":"gpu","permalink":"https://blog.itmonkey.icu/tags/gpu/"},{"name":"grafana","slug":"grafana","permalink":"https://blog.itmonkey.icu/tags/grafana/"},{"name":"k8s","slug":"k8s","permalink":"https://blog.itmonkey.icu/tags/k8s/"},{"name":"华为云","slug":"华为云","permalink":"https://blog.itmonkey.icu/tags/%E5%8D%8E%E4%B8%BA%E4%BA%91/"},{"name":"GPU","slug":"GPU","permalink":"https://blog.itmonkey.icu/tags/GPU/"},{"name":"gcp","slug":"gcp","permalink":"https://blog.itmonkey.icu/tags/gcp/"},{"name":"firewalls","slug":"firewalls","permalink":"https://blog.itmonkey.icu/tags/firewalls/"},{"name":"time","slug":"time","permalink":"https://blog.itmonkey.icu/tags/time/"},{"name":"运营报告","slug":"运营报告","permalink":"https://blog.itmonkey.icu/tags/%E8%BF%90%E8%90%A5%E6%8A%A5%E5%91%8A/"},{"name":"巡检","slug":"巡检","permalink":"https://blog.itmonkey.icu/tags/%E5%B7%A1%E6%A3%80/"},{"name":"gke","slug":"gke","permalink":"https://blog.itmonkey.icu/tags/gke/"},{"name":"anthos","slug":"anthos","permalink":"https://blog.itmonkey.icu/tags/anthos/"},{"name":"运维门户","slug":"运维门户","permalink":"https://blog.itmonkey.icu/tags/%E8%BF%90%E7%BB%B4%E9%97%A8%E6%88%B7/"},{"name":"vue","slug":"vue","permalink":"https://blog.itmonkey.icu/tags/vue/"},{"name":"运维","slug":"运维","permalink":"https://blog.itmonkey.icu/tags/%E8%BF%90%E7%BB%B4/"},{"name":"梳理","slug":"梳理","permalink":"https://blog.itmonkey.icu/tags/%E6%A2%B3%E7%90%86/"},{"name":"cdn","slug":"cdn","permalink":"https://blog.itmonkey.icu/tags/cdn/"},{"name":"ingress","slug":"ingress","permalink":"https://blog.itmonkey.icu/tags/ingress/"},{"name":"短连接","slug":"短连接","permalink":"https://blog.itmonkey.icu/tags/%E7%9F%AD%E8%BF%9E%E6%8E%A5/"},{"name":"内核参数","slug":"内核参数","permalink":"https://blog.itmonkey.icu/tags/%E5%86%85%E6%A0%B8%E5%8F%82%E6%95%B0/"},{"name":"operator","slug":"operator","permalink":"https://blog.itmonkey.icu/tags/operator/"},{"name":"go","slug":"go","permalink":"https://blog.itmonkey.icu/tags/go/"},{"name":"ingress-nginx","slug":"ingress-nginx","permalink":"https://blog.itmonkey.icu/tags/ingress-nginx/"},{"name":"https","slug":"https","permalink":"https://blog.itmonkey.icu/tags/https/"},{"name":"helm","slug":"helm","permalink":"https://blog.itmonkey.icu/tags/helm/"},{"name":"prometheus-operator","slug":"prometheus-operator","permalink":"https://blog.itmonkey.icu/tags/prometheus-operator/"},{"name":"prometheusrules","slug":"prometheusrules","permalink":"https://blog.itmonkey.icu/tags/prometheusrules/"},{"name":"serviceMonitor","slug":"serviceMonitor","permalink":"https://blog.itmonkey.icu/tags/serviceMonitor/"},{"name":"chartmuseum","slug":"chartmuseum","permalink":"https://blog.itmonkey.icu/tags/chartmuseum/"},{"name":"vpn-tunnel","slug":"vpn-tunnel","permalink":"https://blog.itmonkey.icu/tags/vpn-tunnel/"},{"name":"java","slug":"java","permalink":"https://blog.itmonkey.icu/tags/java/"},{"name":"cloudnative","slug":"cloudnative","permalink":"https://blog.itmonkey.icu/tags/cloudnative/"},{"name":"nginx","slug":"nginx","permalink":"https://blog.itmonkey.icu/tags/nginx/"},{"name":"云运维","slug":"云运维","permalink":"https://blog.itmonkey.icu/tags/%E4%BA%91%E8%BF%90%E7%BB%B4/"},{"name":"rsync","slug":"rsync","permalink":"https://blog.itmonkey.icu/tags/rsync/"},{"name":"inotify-tools","slug":"inotify-tools","permalink":"https://blog.itmonkey.icu/tags/inotify-tools/"},{"name":"文件同步","slug":"文件同步","permalink":"https://blog.itmonkey.icu/tags/%E6%96%87%E4%BB%B6%E5%90%8C%E6%AD%A5/"},{"name":"python","slug":"python","permalink":"https://blog.itmonkey.icu/tags/python/"},{"name":"监控告警","slug":"监控告警","permalink":"https://blog.itmonkey.icu/tags/%E7%9B%91%E6%8E%A7%E5%91%8A%E8%AD%A6/"},{"name":"开源","slug":"开源","permalink":"https://blog.itmonkey.icu/tags/%E5%BC%80%E6%BA%90/"},{"name":"rpm","slug":"rpm","permalink":"https://blog.itmonkey.icu/tags/rpm/"},{"name":"yum仓库","slug":"yum仓库","permalink":"https://blog.itmonkey.icu/tags/yum%E4%BB%93%E5%BA%93/"},{"name":"linux","slug":"linux","permalink":"https://blog.itmonkey.icu/tags/linux/"},{"name":"服务压测","slug":"服务压测","permalink":"https://blog.itmonkey.icu/tags/%E6%9C%8D%E5%8A%A1%E5%8E%8B%E6%B5%8B/"},{"name":"阿里云","slug":"阿里云","permalink":"https://blog.itmonkey.icu/tags/%E9%98%BF%E9%87%8C%E4%BA%91/"},{"name":"美图","slug":"美图","permalink":"https://blog.itmonkey.icu/tags/%E7%BE%8E%E5%9B%BE/"}]}